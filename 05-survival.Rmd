# Survival Analysis

```{r include=FALSE}
library(knitr)
library(gridExtra)
library(glmnet)
library(e1071)
library(MASS)
library(party)
library(pec)
library(tidyverse)
```

We turn our attention to survival analysis which deals with so-called time-to-event endpoints. We will use the *lymphoma* data set to set the scene and explain the basics. In particular, we will discuss elastic net regularization in the context of cox regression, introduce the time-dependent Brier score as a measure of prediction accuracy, and we give an example on how to use the `pec` package to benchmark prediction algorithms.

## Survival Endpoints and Cox Regression


We start by reading the lymphoma data which consists of gene expression data for $p=7399$ genes measured on $n=240$ patients, as well as survival data, for these patients. 

```{r, include=FALSE}
# data source: http://web.stanford.edu/~hastie/StatLearnSparsity/data.html
# load packages for survival analysis
library(survival)
library(survminer)
set.seed(1)
```

```{r}
# read gene expression matrix
x <- read.table("data/lymphx.txt")%>%
  as.matrix

# read survival data
y <- read.table("data/lymphtime.txt",header = TRUE)%>%
  as.matrix
```

The survival data consists of two variables `time` (the survival time) and `status` (event status, 1 in case of death, 0 in case of censoring). 

```{r echo=FALSE}
kable(head(y),digits=3)
```

The next plots shows the distribution of the survival times on a linear and log-scale. 

```{r echo=FALSE,fig.height=3, warning=FALSE, message=FALSE}
dd <- data.frame(y)
p1 <- dd%>%
  ggplot(.,aes(x=time))+
  geom_histogram(alpha=0.8)+
  theme_bw()
p2 <- p1+
  scale_x_continuous(trans='log2')
ggarrange(p1,p2)
```

The distribution on the left is right skewed. However, after a log transformation the distribution is near-to-symmetric. What makes this endpoint so special? Why can't we just use (regularized) linear regression to predict the (log) survival time based on the gene expression features? Such an approach would be shortsighted the reason being that we so far did not take into account the event status. The following graph shows survival times along side with the event status for a few patients. For patients with an event (blue triangles) the survival time corresponds with the time-to-event. However, for censored patients (red dots) the actual time-to-event is not observed and will be larger than the survival time.

```{r echo=FALSE}
dd <- data.frame(y)%>%
  dplyr::slice(1:20)
dd$subject <- paste0("S",seq(nrow(dd)))
dd$status <- ifelse(dd$status==1,"Event","Censor")

ggplot(dd, aes(subject, time)) + 
  geom_bar(stat = "identity", width = 0.5) + 
  geom_point(data = dd, 
             aes(subject, time, color = status, shape = status), 
             size = 4) +
  coord_flip() +
  theme_bw() + 
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        axis.text.y = element_blank())
```

In survival analysis we denote the time-to-event with $T$. As illustrated above we typically only partially observe $T$ as some subjects may be censored due to:

* Loss to follow-up
* Withdrawal from study
* No event by end of fixed study period.

Therefore we observe the survival time $Y$ (which equals the event time or the censoring time whichever occurs earlier) and the event status $D$ ($D=1$ in case of event, $D=0$ in case of censoring). 

A fundamental quantity in survival analysis is the survival function

\[S(t)=P(T>t)=1-F(t)\]

which can be estimated using the Kaplan-Meier method. In R we use `survfit` to invoke Kaplan-Meier and `ggsurvplot` to plot the estimated curve.

```{r}
dat <- data.frame(y)
fit.surv <- survfit(Surv(time, status) ~ 1, 
                    data = dat)
ggsurvplot(fit.surv,conf.int=FALSE)
```

More specific information on the estimated survival probabilities can be obtained using the `summary` function. 

```{r, eval=FALSE}
# estimated probability of surviving beyond 10 years
summary(survfit(Surv(time, status) ~ 1, data = dat), times = 10)
```

Now, how do we study the relationship between covariates and survival time? The solution is Cox regression! We introduce the hazard function defined as

\begin{eqnarray*}
h(t)&=&\lim_{dt\rightarrow 0}\frac{P(t\leq T < t+dt|T\geq t)}{dt}\\
&=&-S'(t)/S(t).
\end{eqnarray*}

The Cox proportional hazards model then assumes a semi-parametric form for the hazard

\[h(t|X)=h_0(t)\exp(X^T\beta),\]

where $h_0(t)$ is the baseline hazard and $\beta$ are the regression coefficients. Cox regression estimates the regression coefficients by maximizing the so-called partial likelihood function (surprisingly this works without specifying the baseline hazard function). For illustration we fit a Cox regression model using the first 3 genes as predictors.

<!-- $\ell(\beta|\textbf{y},\textbf{d},\textbf{X})$  -->

```{r}
dat <- data.frame(cbind(y,x[,1:3]))
fit <- coxph(Surv(time,status)~.,data=dat)
summary(fit)
```

The (exponentiated) regression coefficients are interpreted as hazard-ratios. For example a unit change in the 3rd covariate accounts for a risk reduction of $\exp(\beta_3)$=`r round(exp(coef(fit)[3]),2)` or $14\%$. The results of Cox regression are often visualized using a forest plot.

```{r warning=FALSE, message=FALSE}
ggforest(fit)
```

## Regularized Cox Regression

The lymphoma data consists of $p=$ `r ncol(x)` predictors. A truly high-dimensional example! Similar as for linear regression and logistic regression we can build upon the Cox regression model and use subset selection or regularization. The R package `glmnet` implements elastic net penalized cox regression. For illustration we restrict ourselves to the top genes (highest variance) and as a preprocessing step we scale the feature matrix. 

```{r, include=FALSE}
# read gene expression matrix
x <- read.table("data/lymphx.txt")%>%
  as.matrix

# read survival data
y <- read.table("data/lymphtime.txt",header = TRUE)%>%
  as.matrix
```

```{r}
# filter for top genes (highest variance) and scale the input matrix
topvar.genes <- order(apply(x,2,var),decreasing=TRUE)[1:50]
x <- scale(x[,topvar.genes])
```

We split the data set into training and test data.

```{r}
set.seed(1234)
train_ind <- sample(1:nrow(x),size=nrow(x)/2)
xtrain <- x[train_ind,]
ytrain <- y[train_ind,]
xtest <- x[-train_ind,]
ytest <- y[-train_ind,]
```

```{r, include=FALSE, eval=FALSE}
library(pheatmap)
ind <- sample(1:nrow(x),size=25)
xhp <- t(x[ind,])
colnames(xhp) <- paste0("P",seq(ncol(xhp)))
rownames(xhp) <- NULL
annotation_col <- data.frame(
  response=y[ind,1]
)
rownames(annotation_col) <- colnames(xhp)
hp <- pheatmap(xhp,cluster_rows = FALSE,cluster_cols = FALSE,
               annotation_col = annotation_col,
               annotation_names_col = FALSE,fontsize=15)
```

We invoke `glmnet` with argument `family="cox"` and set the mixing parameter to $\alpha=0.95$.

```{r}
set.seed(1)
fit.coxnet <- glmnet(xtrain, ytrain, family = "cox",alpha=0.95)
plot(fit.coxnet,xvar="lambda")
```

We tune the amount of penalization by using cross-validation and Harrel's concordance index as a goodness of fit measure.

```{r}
cv.coxnet <- cv.glmnet(xtrain,ytrain,
                       family="cox",
                       type.measure="C",
                       nfolds = 5,
                       alpha=0.95)
plot(cv.coxnet)
```

The C-index ranges from 0.5 to 1. A value of 0.5 indicates that the model is no better at predicting an outcome than random chance. The tuning parameter which maximizes the C-index is $\lambda_{\rm{opt}}=$ `r round(cv.coxnet$lambda.min,3)`. The next graphic shows the magnitude of the non-zero coefficients (note that we standardized the input covariates).

```{r, echo=FALSE}
dbeta <- data.frame(NULL)
dbeta <- data.frame(betahat=as.numeric(coef(fit.coxnet,s=cv.coxnet$lambda.min)))
dbeta$betaname <- colnames(x)
dbeta%>%
  dplyr::filter(betahat!=0)%>%
  ggplot(.,aes(x=reorder(betaname,abs(betahat)),y=abs(betahat)))+
  geom_bar(stat="identity",width=0.5)+
  xlab("gene")+ylab("abs(betahat)")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5,size=7),
        text = element_text(size = 15))
```

We use the obtained model to make predictions on the test data. In particular we compute the linear predictor 

$$\hat{f}(X_{\textrm{new}})=X_{\textrm{new}}^T\hat{\beta}_{\lambda_{\rm min}}.$$
We can now classify patients into good and poor prognosis based on thresholding the linear predictor at zero.

```{r}
# linear predictor
lp <- predict(fit.coxnet,
              newx=xtest,
              s=cv.coxnet$lambda.min,
              type="link")
dat.test <- data.frame(ytest)
dat.test$prognosis <- ifelse(lp>0,"poor","good")
fit.surv <- survfit(Surv(time, status) ~ prognosis, 
                    data = dat.test)
ggsurvplot(fit.surv,conf.int = TRUE)
```

The survival curves are reasonably well separated, which suggests we have derived a gene signature which at least deserves further investigation.

## Brier Score

We have seen how to evaluate the generalization error in the linear regression and classification context. For time-to-event data this is slightly more involved. A popular way to quantify the prediction accuracy is the time-dependent Brier score

\[{\rm BS}(t,\hat{S})={\bf E}[(\Delta_{\rm{new}}(t)-\hat{S}(t|X_{\rm new}))^2] \]

where $\Delta_{\textrm{new}}(t)={\bf 1}(T_{\textrm{new}}\geq t)$ is the true status of a new test subject and
$\hat{S}(t|X_{\rm new})$ is the predicted survival probability. Calculation of the Brier score is complicated by the fact that we do not always observe the event time $T$ due to censoring. The R package `pec` estimates the Brier score using a technique called *Inverse Probability of Censoring Weighting (IPCW)*. 

For illustration we use forward selection on the training data to obtain a prediction model (we choose `steps=5` to reduce CPU time).

```{r}
dtrain <- data.frame(cbind(ytrain,xtrain))
dtest <- data.frame(cbind(ytest,xtest))
fit.lo <- coxph(Surv(time,status)~1,data=dtrain,
              x=TRUE,y=TRUE)
up <- as.formula(paste("~", 
                       paste(colnames(xtrain), 
                             collapse="+")))
fit.fw <- stepAIC(fit.lo,
                  scope=list(lower=fit.lo,
                             upper=up),
                  direction="forward",
                  steps=5,
                  trace=FALSE)

```

The following table summarizes the variables added in each step of the forward selection approach.
```{r}
kable(as.data.frame(fit.fw$anova),digits=3,booktabs=TRUE)
```

The next figure shows the estimated hazard ratios and confidence intervals. This type of plot is often referred to as forest plot.

```{r, warning=FALSE}
survminer::ggforest(fit.fw)
```

Finally we use the `pec` package to calculate the Brier score on the training and test data.

```{r message=FALSE}
library(pec)
fit.pec.train <- pec::pec(
  object=list("cox.fw"=fit.fw), 
  data = dtrain, 
  formula = Surv(time, status) ~ 1, 
  splitMethod = "none")


fit.pec.test <- pec::pec(
  object=list("cox.fw"=fit.fw), 
  data = dtest, 
  formula = Surv(time, status) ~ 1, 
  splitMethod = "none")

```
The following figure shows the Brier scores evaluated on training and test data.
```{r}
par(mfrow=c(1,2))
plot(fit.pec.train,main="training data")
plot(fit.pec.test,main="test data")
```

The plot on the right shows the Brier score on the test data and indicates that the selected model performs slightly better than the reference model (no covariates).

The `pec` package can also be used to benchmark different prediction models. We illustrate this based on random forest and forward selection. In this illustration we do not split the data into training and test. Instead we use cross-validation to compare the two prediction approaches. 

We start by writing a small wrapper function to use forward selection in `pec`. (A detailed description on the `pec` package and on how to set up wrapper functions is provided [here](https://pubmed.ncbi.nlm.nih.gov/25317082/).)

```{r}
selectCoxfw <- function(formula,data,steps=5,direction="both")
{
  require(prodlim)
  fmlo <- reformulate("1",formula[[2]])
  fitlo <- coxph(fmlo,data=data,x=TRUE,y=TRUE)
  fwfit <- stepAIC(fitlo,
                   scope=list(lower=fitlo,
                              upper=formula),
                   direction=direction,
                   steps=steps,
                   trace=FALSE)
  if (fwfit$formula[[3]]==1){
    newform <- reformulate("1",formula[[2]])
    newfit <- prodlim(newform,
                      data=data)
  }else{
    newform <-fwfit$formula
    newfit <- coxph(newform,data=data,x=TRUE,y=TRUE)
  }
  out <- list(fit=newfit,
              In=attr(terms(newfit$formula),which = "term.labels"))
  out$call <-match.call()
  class(out) <- "selectCoxfw"

  out
}

predictSurvProb.selectCoxfw <- function(object,newdata,times,...){
  predictSurvProb(object[[1]],newdata=newdata,times=times,...)
}
```

We run forward selection with maximum 5 steps.

```{r}
dat <- data.frame(cbind(y,x))
fm <- as.formula(paste("Surv(time, status) ~ ", 
                       paste(colnames(dat[,-(1:2)]), 
                             collapse="+")))
fit.coxfw <- selectCoxfw(fm,data=dat,
                         direction="forward",steps=5)
```

We fit a random forest using `cforest` from the `party` package.

```{r}
fit.cforest <- pec::pecCforest(fm, data =dat, 
                               control = party::cforest_classical(ntree = 100))
```

We can obtain a measure of variable importance using the function `varimp`.

```{r echo=FALSE, warnings=FALSE, message=FALSE, fig.height=8}
vp <- party::varimp(fit.cforest$forest)
dvp <- data.frame(VARNAME=names(vp),VARIMP=vp)
vplot <- dvp%>%
  arrange(desc(VARIMP))%>%
  ggplot()+
  geom_point(aes(x = reorder(VARNAME, VARIMP), y = VARIMP), size = 3, shape = 18)+
  labs(title = "Variable Importance Random Survival Forest")+
  xlab("Variable")+
  ylab("Variable importance")+
  coord_flip()+
  theme_bw()
print(vplot)
```


Finally we compare the two approaches using the cross-validated Brier score. 

```{r message=FALSE, warning=FALSE}
pec.cv <- pec::pec(
  object=list("cox.fw"=fit.coxfw,"cforest"=fit.cforest), 
  data = dat, 
  formula = Surv(time, status) ~ 1, 
  splitMethod = "cv5")
plot(pec.cv)
```

We conclude that forward selection and random forest slightly outperform the reference model (no covariates included).
