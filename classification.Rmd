---
title: "Classification and Survival Analysis"
author: "Nicolas StÃ¤dler"
date: "3/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warnings=FALSE)
```


```{r,include=FALSE}
library(knitr)
library(caret)
library(mlbench)
library(glmnet)
library(tidyverse)
library(e1071)
library(MASS)
```

We so far focused on the linear regression model. We will now extend to the situation where the response is binary or time-to-event. The former is also referred to as classification.

## Classification

We focus on the binary situation where response $G$ takes values $0$ and $1$, and, the aim is to do prediction based based on covariates $X=(X_1,\ldots,X_p)$. We model the probability of success $p(x;\beta)=P(G=1|X=x;\beta)$ using the logistic regression approach with logit link function

\[\rm{logit}(x;\beta)=log \Big(\frac{p(x;\beta)}{1-p(x;\beta)}\Big)=X^T\beta.\]

In logistic regression we estimate the regression parameter $\beta$ by maximizing the log-likelihood

\begin{align*}
l(\beta|\bf G,\bf X)&=\sum_{i=1}^{n}(1-g_i)\log(1-p(x_i;\beta))+g_i\log p(x_i;\beta)\\
&=\sum_{i=1}^{n}g_i x_i^T\beta - \log(1+\exp(x_i^T\beta)).
\end{align*}

Given an estimate $\hat \beta$ (from training data), prediction based on new input data $X_{\rm new}$ can be obtained via the predicted probablity of success $p(X_{\rm new};\hat \beta)$, e.g. 

\begin{align*}
\hat{G}(X_{\rm new})&=1\quad \rm{if}\quad p(X_{\rm new};\hat\beta)>0.5. 
\end{align*}

The quality of the prediction is judged based on the missclassification error on test data.

Similar as in linear regression, in the high-dimensional context where $n$ is small compared to $p$, the maximum likelihood estimator does often lead to overfitting and a poor generalisation error. We will in the following generalize the previously introduced idea of regularization to classification. In the context of linear regression we penalized the residual sum of squares (RSS). We extend this and penalize the negative log-likelihood with an elastic net penalty


\begin{align*}
\hat{\beta}^{\rm Lasso}_{\lambda}&=\rm{argmin}-l(\beta|\bf G,\bf X)+\lambda\big((1-\alpha)\|\beta\|_2^2/2+\alpha\|\beta\|_1\big).
\end{align*}

### South African Heart Disease Data
A retrospective sample of males in a heart-disease high-risk region
of the Western Cape, South Africa. There are roughly two controls per
case of CHD. Many of the CHD positive men have undergone blood
pressure reduction treatment and other programs to reduce their risk
factors after their CHD event. In some cases the measurements were
made after these treatments. These data are taken from a larger
dataset, described in  Rousseauw et al, 1983, South African Medical
Journal. 

sbp		systolic blood pressure
tobacco		cumulative tobacco (kg)
ldl		low densiity lipoprotein cholesterol
adiposity
famhist		family history of heart disease (Present, Absent)
typea		type-A behavior
obesity
alcohol		current alcohol consumption
age		age at onset
chd		response, coronary heart disease

```{r}
# data source: https://web.stanford.edu/~hastie/ElemStatLearn/
dat <-  read.table("http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data",sep=",",head=T,row.names=1)
vars <- c("chd","sbp","tobacco","ldl","famhist","obesity","alcohol","age")
dat <- dat[,vars]

pairs(data.matrix(dat[,-1]),col=ifelse(dat$chd==1,"red","blue"))
```
```{r}
fit.logistic <- glm(chd~sbp+tobacco+ldl+famhist+obesity+alcohol+age,data=dat,family="binomial")
broom::tidy(fit.logistic)
```

We proceed with backward selection.

```{r}
fit.bw <- stepAIC(fit.logistic,direction = "backward")
```

```{r}
kable(as.data.frame(fit.bw$anova),digits=2)
```

And the final regression coefficients are

```{r}
kable(broom::tidy(fit.bw),digits=2)
```


Finally we run the Lasso approach

```{r}
x <- scale(data.matrix(dat[,-1]))
y <- dat$chd
fit.lasso <- glmnet(x=x,y=y,family="binomial")
plot(fit.lasso,xvar = "lambda",label=TRUE)
```
This example was only for illustration. We now turn to a truly high-dimensional example.

### Leukemia Dataset

<!--  Example from https://cran.r-project.org/web/packages/varbvs/vignettes/leukemia.html -->

Expression levels recorded for 3,571 genes in 72 patients with leukemia (Golub et al, 1999). The binary outcome encodes the disease subtype: acute lymphobastic leukemia (ALL) or acute myeloid leukemia (AML). Data are represented as a 72 x 3,571 matrix x of gene expression values, and a vector y of 72 binary disease outcomes.


```{r}
set.seed(1)

# get leukemia data
library(varbvs) 
data(leukemia)
x <- leukemia$x
y <- leukemia$y

# set settings for glmnet 
nfolds <- 10                  # number of cross-validation folds.
alpha  <- 0.95                  # elastic net mixing parameter.
lambda <- 10^(seq(0,-2,-0.05))  # lambda sequence.

# run glmnet
fit.glmnet <-glmnet(x,y,family = "binomial",lambda=lambda,alpha=alpha)

# run cv.glmnet
cv.glmnet <- cv.glmnet(x,y,family = "binomial",type.measure = "class",
                       alpha = alpha,nfolds = nfolds,lambda = lambda)
```


The following Figure shows the Elastic net solution for the grid of lambda values.

```{r}
plot(fit.glmnet,xvar="lambda",label=TRUE)
```

We run 10-fold cross-validation and take $\lambda_{\rm opt}$ as the largets $\lambda$ within 1 standard error of the minimum classification error.

```{r}
plot(cv.glmnet)
lambda.opt <- cv.glmnet$lambda.1se
```

The optimal coefficients can be optained as

```{r}
beta.glmnet <- coef(fit.glmnet, s = cv.glmnet$lambda.1se)
barplot(as.numeric(beta.glmnet)[-1],xlab="gene",ylab="beta glment")
```

Finally we predict the disease outcome using the fitted model and compare against the observed values.
```{r}
pred <- c(predict(fit.glmnet,x,s = lambda.opt,type = "class"))
print(table(true = factor(y),pred = factor(pred)))
```


### Sonar Dataset

To illustrate the approaches we use the Sonar dataset from the mlbench package. The goal is to train a model to discriminate between rock "R" and metal "M" based on sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock. The $p=60$ covariates represent energies within particular frequency bands.

```{r}
data(Sonar)

set.seed(107)
inTrain <- createDataPartition(
  y = Sonar$Class,
  ## the outcome data are needed
  p = .5,
  ## The percentage of data in the
  ## training set
  list = FALSE
)
dtrain <- Sonar[ inTrain,]
xtrain <- as.matrix(dtrain[,-61])
ytrain <- dtrain$Class
dtest  <- Sonar[-inTrain,]
xtest <- as.matrix(dtest[,-61])
ytest <- dtest$Class
```


We first run logistic regression once with only an intercept and once with all the covariates. We
note that glm reports convergence problems when using all variables. This indicates that the model could be to complex to fit.

In order to compare the performance of the different methods we setup trainControl. We use 10 fold cross-validation

```{r}
tc <- trainControl(method = "cv", number = 10) # 10-fold CV, performance=accuracy

# tc <- trainControl(method = "cv", number = 10, # 10-fold CV, performance=ROC
#                    summaryFunction = twoClassSummary,
#                    classProbs = TRUE)
```


```{r}
fit.0 <-  glm(Class~1,family=binomial,data=dtrain)
fit.full <-  glm(Class~.,family=binomial,data=dtrain)
fit.full.caret <- train(Class ~., 
                        data = dtrain, 
                        method = "glm", 
                        family = "binomial",
                        trControl = tc)
```

Next we use forward selection.

```{r}
upper.model <- paste("~", paste(colnames(dtrain), collapse=" + ")) 
fit.fw <- stepAIC(fit.0,
                  scope=upper.model,
                  direction = "forward", 
                  steps=10,
                  trace = FALSE)
summary(fit.fw)
broom::tidy(fit.fw)
```


```{r}
fit.fw.caret <- train(x=xtrain,
                      y=ytrain, 
                      method = "glmStepAIC", 
                      family = "binomial",
                      direction ="forward",
                      steps=10,
                      trControl = tc,
                      trace = FALSE
)
# accuracy
fit.fw.caret$results
# summary of the model
summary(fit.fw.caret$finalModel)
```

Next, we use L1-penalized regression.


```{r}
fit.l1 <-glmnet(x=xtrain,y=ytrain,alpha=1,family="binomial") 
plot(fit.l1,xvar="lambda",label=TRUE)
cvfit <- cv.glmnet(x=xtrain, y=ytrain, family = "binomial", type.measure = "class",alpha=1)
cvfit$lambda.min
plot(cvfit)
```

```{r}
lambda.grid <- fit.l1$lambda
fit.l1.caret<-train(x=xtrain,
                 y=ytrain,
                 method = "glmnet",
                 family="binomial",
                 preProc = c("center","scale"),
                 tuneGrid = expand.grid(alpha = 1,lambda=lambda.grid),
                 trControl = tc
) 

#
ggplot(fit.l1.caret)
# Best lambda
fit.l1.caret$bestTune$lambda
# Model coefficients
coef(fit.l1.caret$finalModel,fit.l1.caret$bestTune$lambda)
# Make predictions
confusionMatrix(data =fit.l1.caret %>% predict(xtest), ytest)
confusionMatrix(data =fit.fw.caret %>% predict(xtest), ytest)
confusionMatrix(data =fit.full.caret %>% predict(xtest), ytest)
```

Finally we compare all models
```{r}
models <- list(full=fit.full.caret,
               fw=fit.fw.caret,
               l1=fit.l1.caret)
summary(resamples(models),metric = "Accuracy")
```

```{r}
coef(fit.l1,s=cvfit$lambda.min)
coef(fit.fw)
```


## Survival analysis

Another important endpoint in many areas is time-to-event. We will give a very brief introduction into survival analysis and show the cox proportional hazards model can be used to address high-dimensional problems. For subject $i$ we denote the event time with $T_i$. Unfortunately, typically we do for each subject observe the event time as a subject may be censored due to:

* Loss to follow-up
* Withdrawal from study
* No event by end of fixed study period.

Therefore in practise we observe the survival time $Y_i$ (which e) and the event indicator $\delta_i$ ($\delta_i=1$ in case of event $=0$ in case of censoring).

A fundamental quantity in survival analysis is the survival probability

\[S(t)=P(T>t)=1-F(t).\]

The Kaplan-Meier method is the most common way to estimate survival times and probabilities.
The survival probability can be estimated using the Kaplan-Meier method.


The Cox proportional hazards model is commonly used for the study of the relationship beteween predictor variables and survival time. The Cox model assumes a semi-parametric form for the hazard, or the instantaneous rate at which events occur

\[h(t|X)=h_0(t)exp(X^T\beta).\]

$\lambda_0(t)$ is the baseline hazard and $\beta$ are the regression coefficients. In the classic setting $n>p$, inference is made via the partial likelihood. In the high-dimensional context we can penalize the negative log of the partial likelihood.

If you are interested to learn more about survival analysis I recommend you to have a look at this tutorial https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html.


### Example Dataset
Gene expression data for 7399 genes measured on 240 patients, as well as censored survival times, for these patients. We start by reading the data and plotting the estimated surival curve.

```{r}
# data source: http://web.stanford.edu/~hastie/StatLearnSparsity/data.html
# load packages for survival analysis
library(survival)
library(survminer)

# read data
x <- read.table("data/lymphx.txt")%>%# gene expression (covariates)
  as.matrix
y <- read.table("data/lymphtime.txt",header = TRUE)%>%# survival time and event variable
  as.matrix
dat <- data.frame(cbind(y,x))

# kaplan meier estimate
fit.surv <- survfit(Surv(time, status) ~ 1, data = dat)
ggsurvplot(fit.surv)
```

For example the estimated probability of surviving beyond 10 years is obtained as

```{r}
summary(survfit(Surv(time, status) ~ 1, data = dat), times = 10)
```

We now perform L1-penalized cox regression using glmnet.

```{r}
set.seed(1)
fit.coxnet <- glmnet(x, y, family = "cox",alpha=1)
plot(fit.coxnet,xvar="lambda")

cv.coxnet <- cv.glmnet(x,y,family="cox")
plot(cv.coxnet)
#plot(cv.coxnet, xlim = c(log(0.05), log(0.5)), ylim = c(11.3,12.4))

cv.coxnet$lambda.min

betahat <- coef(fit.coxnet,s=cv.coxnet$lambda.min)
Shat <- rownames(betahat)[which(betahat != 0)]

fm <- as.formula(paste0("Surv(time, status) ~ 0+",paste(Shat,collapse = "+")))
fit.shat <- coxph(fm,data=dat,x=TRUE,y=TRUE)
```



```{r, eval=FALSE}
library(party)
library(randomForestSRC)
# Condicional Inferences Trees
fm <- Surv(time, status) ~ .
fitcforest <- pec::pecCforest(fm, data =dat[,1:20], 
                              control = party::cforest_classical(ntree = 1000))

# Survival Random Forests
fitrsf <- randomForestSRC::rfsrc(fm, data = dat[,1:20], forest = TRUE, 
                                 ntree = 1000, splitrule = "logrank", 
                                 na.action = "na.impute")

# Variable Importance
varimp <- randomForestSRC::vimp(fitrsf, importance = "permute")$importance
data.frame(varimp)%>%
  tibble::rownames_to_column("NAME")%>%
  ggplot()+
  geom_point(aes(x = reorder(NAME, varimp), y = varimp), size = 3, shape = 18)+
  labs(title = "Variable Importance Random Survival Forest")+
  xlab("Variable")+
  ylab("Variable importance")+
  coord_flip()
```

```{r, eval=FALSE}
library(pec)
models <- list("cox.shat"=fit.shat)
fitpec <- pec::pec(
  object=models, 
  data = dat, 
  formula = Surv(time, status) ~ 1, 
  splitMethod = "cv10", B = 1, keep.index = TRUE, keep.matrix = TRUE)
plot(fitpec)
```


## Exercises

1. Take covariates from dataset XYZ and simulate data with 5 active covariates. Run forward regression, 
ridge regression and lasso regression

2. Calculate the Ridge and the Lasso solution for orthogonal covariates.

3. 
