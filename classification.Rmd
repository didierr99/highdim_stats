---
title: "Classification and Survival Analysis"
author: "Nicolas StÃ¤dler"
date: "3/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hide")
```


```{r}
library(caret)
library(mlbench)
library(glmnet)
library(tidyverse)
library(e1071)
```

We so far focused on linear regression. We will now extend to situation where response is binary (classification) and where response is time-to-event. 

## Classification

We focus on the binary situation where response $G$ takes values $0$ and $1$, and, the aim is to do prediction based based on covariates $X=(X_1,\ldots,X_p)$. We model the success probability $p(x)=P(G=1|X=x;\beta)$ using the logistic regression approach

\[\rm{logit}(x;\beta)=log \Big(\frac{p(x;\beta)}{1-p(x;\beta)}\Big)=X^T\beta.\]

In logistic regression we estimate the regression parameter by maximizing the log-likelihood

\begin{align*}
l(\beta|\bf G,\bf X)&=\sum_{i=1}^{n}(1-g_i)\log(1-p(x_i;\beta))+g_i\log p(x_i;\beta)\\
&=\sum_{i=1}^{n}g_i x_i^T\beta - \log(1+\exp(x_i^T\beta)).
\end{align*}

Prediction based on new input data $X_{\rm new}$ is obtained as

\begin{align*}
\hat{G}(X_{\rm new})&=1\quad \rm{if}\quad P(G=1|X_{\rm new};\beta)>0.5. 
\end{align*}

In the following we use the 0-1 loss and use 10-fold cross-validation to approximate the prediction error.

Similar as in linear regression in the high-dimensional context where $n$ small compared to $p$ the maximum likelihood estimater (i.e. binomial regression). We will discuss in the following forward regression, L1-penalized likelihood and support vector machines as alternative methods.

### Example Dataset

To illustrate the approaches we use the Sonar dataset from the mlbench package. The goal is to train a model to discriminate between rock "R" and metal "M" based on sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock. The $p=60$ covariates represent energies within particular frequency bands.

```{r}
data(Sonar)

set.seed(107)
inTrain <- createDataPartition(
  y = Sonar$Class,
  ## the outcome data are needed
  p = .5,
  ## The percentage of data in the
  ## training set
  list = FALSE
)
dtrain <- Sonar[ inTrain,]
dtest  <- Sonar[-inTrain,]

```

### Logistic Regression and Forward selection
```{r}
tc <- trainControl(method = "cv", number = 10)
fit.forward <- train(Class ~1, 
                     data = dtrain,
                     method = "glmStepAIC", 
                     family = "binomial",
                     direction ="forward",
                     trControl = tc,
                     trace = FALSE
                    )
# nvmax
fit.forward$bestTune
# accuracy
fit.forward$results
# summary of the model
summary(fit.forward$finalModel)
# final model coefficients
coef(fit.forward$finalModel,2)

```

ctrl <- trainControl(method = "repeatedcv", repeats = 3)

plsFit <- train(
  Class ~ .,
  data = training,
  method = "pls",
  preProc = c("center", "scale"),
  tuneLength = 15,
  ## added:
  trControl = ctrl
)

## Survival Analysis



## Exercises

1. Take covariates from dataset XYZ and simulate data with 5 active covariates. Run forward regression, 
ridge regression and lasso regression

2. Calculate the Ridge and the Lasso solution for orthogonal covariates.

3. 
