# Regularization

```{r, include=FALSE}
library(knitr)
library(caret)
library(glmnet)
library(tidyverse)
library(MASS)
library(lars)
library(gridExtra)
library(stats)
library(splines)
```

We have seen that multiple regression falls short in the high-dimensional context. It leads to overfitting and as a result in large estimates of regression coefficients. Augmentation of the least-squares optimization with constraints on the regression coefficients can decrease the risk of overfitting. In the following we will discuss methods which minimize the residual sum of squares, $\rm{RSS}(\beta)$, under some constraints on the parameter $\beta$.


## Model Selection

We will shortly see that the approaches which we introduce do not only fit one single model but they explore a whole series of models (indexed as $m=1,\ldots,M$). Model selection refers to the choice of an optimal model achieving a low generalization error. A plausible approach would be to fit the different models to  the training data and then select the model with smallest error on the test data. However, this is an illegitimate approach as the test data has to be kept untouched for the final evaluation of the selected model. Therefore we guide model selection by approximating the generalization error using training data only. We review now two such approximations, namely, cross-validation and the Akaike information criterion (AIC). 

K-fold cross-validation approximates the prediction error by splitting the training data into K chunks as illustrated below (here $K=5$).

```{r crossvalidation,echo=FALSE,out.width="80%"}
knitr::include_graphics("crossvalidation.JPG")
```

Each chunk is then used as "hold-out" validation data to estimate the error of $m$th model trained on the other $K-1$ data chunks. In that way we obtain $K$ error estimates and we typically take the average as the cross-validation error of model $m$ (denoted by ${\rm CV}_m$). The next plot shows a typical cross-validation error plot. This curve attains its minimum at a model with  $p_m=4$ ($p_m$ is the number of included predictors in model $m$).

```{r message=FALSE,warning=FALSE,echo=FALSE}
set.seed(1)
library(glmnet)
library(caret)
x <-  matrix(rnorm(20 * 20), 20, 15)
y <- x[,1:4]%*%c(2,-2,2,-2)+rnorm(20)
data <- data.frame(x,y)
train.control <- trainControl(method = "cv", number = 5)
step.model <- train(y ~., data = data,
                    method = "leapForward", 
                    tuneGrid = data.frame(nvmax = 1:15),
                    trControl = train.control
                    )
ggplot(step.model)+
  theme_bw()+
  theme(text =element_text(size=15))+
  ylab("CV")+xlab("Number of Predictors")
```

The AIC approach is founded in information theory and selects the model with smallest AIC

$$
{\rm AIC}_m=-2\;{\rm loglik}+2\;p_{m}.
$$
Thus, AIC rewards goodness of fit (as assessed by the likelihood function *loglik*) and penalizes model complexity (by the term $2 p_m$). The figure below shows for the same example the AIC curve. Also the AIC approaches suggests to use a model with $p_m=4$ predictors.

```{r message=FALSE,warning=FALSE,echo=FALSE}
models <- leaps::regsubsets(y~., data = data, 
                            nvmax = 15,
                            method = "forward")

smodels <-  summary(models)
nvar <- apply(smodels$which,1,sum)-1
k <- nvar+2
n <- nrow(data)
aic <- smodels$bic-log(n)*k+k*(2+(2*k+2)/(n-k-1))
plot(nvar, aic, xlab = "Number of Predictors", ylab = "AIC",type="b")
```


## Subset- and Stepwise Regression

The most common approach to impose constraints is subset selection. In this approach we retain only a subset of the variables, and eliminate the rest from the model. OLS is used to estimate the coefficients of the inputs that are retained. More formally, given a subset $S\subset\{1,\ldots,p\}$ we solve the optimization problem


\[
\hat{\beta}_{S}=\text{arg}\!\!\!\!\!\min\limits_{\beta_j=0\;\forall j\notin S}\!\!\!\textrm{RSS}(\beta).
\]

It is easy to show that this is equivalent to OLS regression based on subset $S$ covariates, i.e.

\[
\hat{\beta}_{S}=(\textbf{X}_S^T \textbf{X}_S)^{-1}\textbf{X}_S^T \textbf{y}.
\]

In practice we need to explore a sequence of subsets $S_1,\ldots,S_M$ and choose an optimal subset by either a re-sampling approach or by using an information criterion (see Section \@ref(model-selection)). There are a number of different strategies available. *Best subsets regression* consists of looking at all possible combinations of covariates. Rather than search though all possible subsets, we can seek a good path through them. Two popular approaches are *backward stepwise* regression which starts with the full model and sequentially deletes covariates, whereas *forward stepwise* regression starts with the intercept, and then sequentially adds into the model the covariate that most improves the fit. 

In `R` we can use `regsubsets` from the `leaps` package or `stepAIC` from the `MASS` package to perform subset- and stepwise regression. For example to perform forward stepwise regression based on AIC we proceed as follows.

```{r include=FALSE}
set.seed(1)
n <- 10
p <- 9
beta <- c(2,rep(0,p-1))

# simulate covariates
xtrain <- matrix(rnorm(n*p),n,p)
ytrain <- xtrain%*%beta+rnorm(n,sd=0.5)
dtrain <- data.frame(xtrain)
dtrain$y <- ytrain
```

```{r}
# Forward regression
fit0 <- lm(y~1,data=dtrain)
up.model <- paste("~", paste(colnames(dtrain[,-(p+1)]), collapse=" + "))
fit.fw <- stepAIC(fit0,
                  direction="forward",
                  scope=
                    list(lower=fit0,
                         upper=up.model)
                    ,
                  trace = FALSE
)
```

We can summarize the stepwise process.

```{r}
kable(as.data.frame(fit.fw$anova),digits=3,booktabs=TRUE
      ,caption="Inclusion of covariates in forward stepwise regression.")
```

Finally we can retrieve the regression coefficients of the optimal model.

```{r}
kable(broom::tidy(fit.fw),digits=3,booktabs=TRUE,
      caption="Regression coefficients of the optimal model.")
```

## Ridge Regression

Subset selection as outlined above works by either including or
excluding covariates, i.e. constrain specific regression coefficients to be zero.

An alternative is *Ridge regression*, which regularizes the optimization problem by shrinking regression coefficients towards zero. This discourages complex models because
models that overfit tend to have larger coefficients. Ridge regression can be formulated as a constrained optimization problem

\[
\hat{\beta}^{\rm Ridge}_{c}=\text{arg}\min\limits_{\|\beta\|_2^2\leq c}\textrm{RSS}(\beta).
\]

The geometry of the optimization problem is illustrated in Figure \@ref(fig:ridgegeom). It shows the levels sets of ${\rm RSS}(\beta)$, ellipsoids centered around the OLS estimate, and the circular ridge
parameter constraint, centered around zero with radius $c > 0$. The Ridge estimator
is the point where the smallest level set hits the constraint. Exactly at that point the $\rm{RSS}(\beta)$ is
minimized over those $\beta$â€™s that "live" inside the constraint.

```{r ridgegeom,echo=FALSE,out.width="70%",fig.cap="Geometry of Ridge regression."}
knitr::include_graphics("ridge_geometry.JPG")
```

Alternatively, Ridge regression can be cast as the optimization of the penalised residual sum of squares with a *penalty* on the magnitude of the coefficients, i.e. 

\[\hat{\beta}^{\rm Ridge}_{\lambda}=\textrm{arg}\min\limits_{\beta}\textrm{RSS}(\beta)+\lambda\|\beta\|^2_2.\]

Both formulations are equivalent in the sense that there is a one-to-one relationship between the tuning parameters $c$ and $\lambda$. We will use more often the latter "penalisation" formulation. The parameter $\lambda$ is the amount of penalisation. Note that with no penalization, $\lambda=0$, Ridge regression coincides with OLS. Increasing $\lambda$ has the effect of shrinking the regression coefficients to zero. 

The Ridge optimization problem has the closed form solution (see exercises)

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=(\textbf{X}^T \textbf{X}+\lambda \textbf{I})^{-1}\textbf{X}^T \textbf{y}.
\end{align*}

Note that for $\lambda>0$ the matrix $\textbf{X}^T \textbf{X}+\lambda \textbf{I}$ has always full rank and therefore Ridge regression is well defined even in the high-dimensional context (in contrast to OLS).

Ridge regression is implemented in the package `glmnet`. We use `alpha=0` and can call 

```{r}
fit.ridge.glmnet <-glmnet(x=xtrain,y=ytrain,alpha=0) 
plot(fit.ridge.glmnet,xvar="lambda",label=TRUE)
```


### Choice of penalty parameter

In subset- and stepwise regression we had to identify the optimal subset. Similarly, for Ridge regression model selection consists of selecting the tuning parameter $\lambda$. We proceed by choosing a grid of values $0<\lambda_1<\lambda_2<\ldots<\lambda_M<\infty$ and proceed as explained in Section \@ref(model-selection), that is we choose the optimal $\lambda_{\rm opt}$ by either re-sampling or information criteria. In `glmnet` we use cross-validation using the command `cv.glmnet`.

```{r warning=FALSE}
cv.ridge.glmnet <-cv.glmnet(x=xtrain,y=ytrain,alpha=0) 
```

The next plot shows the cross-validation error with upper and lower standard deviations as a function of the lambda values (note the log scale for the lambdas).  

```{r}
plot(cv.ridge.glmnet)
```

The tuning parameter with the smallest cross-validation error is stored in the argument `lambda.min`.
```{r}
cv.ridge.glmnet$lambda.min
```

Another choice is `lambda.1se` which denotes the largest $\lambda$ within 1 standard error of the smallest cross-validation error.

```{r}
cv.ridge.glmnet$lambda.1se
```

### Shrinkage property

The OLS estimator becomes unstable (high variance) in presence of collinearity. A nice property of Ridge regression is that it counteracts this by shrinking low-variance components more than high-variance components. 

This can be best understood by rotating the data using a principle component analysis (see Figure \@ref(fig:principlecomponent)). In particular, we consider the singular value decomposition

$$\textbf{X}=\textbf{U}\textbf{D}\textbf{V}^T,$$

where the columns of $\textbf{U}$ form an orthonormal basis of the column space of $\textbf{X}$, $\textbf{D}$ is a diagonal matrix with entries $d_1\geq d_2\geq\ldots\geq d_p \geq 0$ called the singular values, and the columns of $\textbf{V}$ represent the principle component directions. For OLS the vector of fitted values ${\bf \hat y}^{\rm OLS}$ is the orthogonal projection of ${\bf y}$ onto the column space of $\bf X$. Therefore, in terms of rotated data we have

\[\hat{\textbf{y}}^{\rm OLS}=\sum_{j=1}^{p}\textbf{u}_j \textbf{u}_j^T \textbf{y}.\]

Similarly, we can represent the fitted values from Ridge regression as

\[\hat{\textbf{y}}^{\rm Ridge}=\sum_{j=1}^{p}\textbf{u}_j \frac{d_j^2}{d_j^2+\lambda}\textbf{u}_j^T\textbf{y}.\]

This shows that the level of shrinkage $\frac{d_j^2}{d_j^2+\lambda}$ is largest in the direction of the last principle component, which in return is the direction where the data exhibits smallest variance.


```{r principlecomponent, echo=FALSE,fig.height=5, fig.width=10, fig.cap="Left plot: 2-dimensional input data. Right plot: input data rotated using principle component analysis."}
library(MASS)
set.seed(1315)
n <- 50
x <- mvrnorm(n=50,mu=c(0,0),Sigma=cbind(c(1,0.8),c(0.8,1)))
colnames(x) <- c("X1","X2")
fit.pc <- prcomp(x)
par(mfrow=c(1,2))
plot(x,xlim=c(-2,2),ylim=c(-2,2))
plot(fit.pc$x,xlim=c(-2,2),ylim=c(-2,2))
```

```{r include=FALSE, eval=FALSE}
fn <- function(x){x/(1+x))}
curve(fn,xlim=c(0,20),ylim=c(0,1),xlab="d",ylab="d^2/(1+d^2)",lwd=2,cex.axis=1.5)
```

### Effective degrees of freedom

Although Ridge regression involves all $p$ covariates the *effective degrees of freedom* are smaller than $p$ as we have imposed constraints through the penalty. In the book @elements it is shown that the effective degrees of freedom for Ridge regression, $\nu^{\rm ridge}_{\lambda}$, are given by

\[\nu^{\rm ridge}_{\lambda}=\sum_{j=1}^{p}\frac{d_j^2}{d_j^2+\lambda},\]
where $d_1,\ldots,d_p$ are the singular values of $\bf X$.

```{r}
# get singular values
fit.svd <- svd(xtrain) #fit.svd$d

# ridge degree of freedom for lambdaopt
df_lambdaopt <- sum(fit.svd$d^2/(fit.svd$d^2+cv.ridge.glmnet$lambda.min))
df_lambdaopt
```

### Bayesian interpretation


We have introduced regularization by least-squares optimization with additional constraints on $\beta$. An alternative approach to regularization is based on Bayesian statistics. In a Bayesian setting the parameter $\beta=(\beta_1,\ldots,\beta_p)$ is itself a random variable with *prior* distribution $p(\beta)$. Bayesian inference is based on the *posterior* distribution $$p(\beta|D)=\frac{p(D|\beta)p(\beta)}{p(D)},$$
where $D$ denotes the data and $p(D|\beta)$ is the likelihood function. In the exercises we will show that the Ridge solution can be viewed as the maximum a posteriori (MAP) estimate of a hierarchical Bayesian model where the data follows a multivariate regression model
\[Y_i|X_i,\beta\sim N(X_i^T\beta,\sigma^2),\; i=1,\ldots,n\]
and the regression coefficients are equipped with prior
\[\beta_j \sim N(0,\tau^2),\; j=1,\ldots,p.\]

### Splines

Ridge regression and high-dimensionality play a role in many subfields of statistics. We illustrate this with the example of smoothing splines for univariate non-parametric regression.

Sometimes it is extremely unlikely that the true function $f(X)$ is actually linear in $X$. Consider the following example.

```{r, echo=FALSE, fig.cap="Non-linear (sinusoidal) relationship between Y and X."}
# define a non-linear function
n <- 101
x <- seq(0, 1, length.out = n)
fx <- sin(2 * pi * x)

# generate noisy data
set.seed(1)
y <- fx + rnorm(n, sd = 0.5)

plot(x, y)# plot of the data
```

How can we approximate the relationship between Y and X?
The most simple approximation is a straight horizontal line (dashed blue line; the true sinusoidal function is depicted in black). 

```{r echo=FALSE,fig.cap="Approximation by a constant."}
const <- mean(y)
plot(x, y)
abline(h=const, lty = 2, col = "blue",lwd=3)
lines(x, fx, lwd = 2)
legend("topright", legend = "f(x)", lty = 1, lwd = 2, bty = "n")
```




Clearly this approximation is too rigid. Next, we try a piecewise constant approximation with two inner "knots".

```{r echo=FALSE, fig.cap="Piecewise constant approximation."}
# see https://bobby.gramacy.com/surrogates/splines.html
xi <- c(1/3,2/3)
hc1 <- function(x) { as.numeric(x < xi[1]) }
hc2 <- function(x) { as.numeric(x >= xi[1] & x < xi[2]) }
hc3 <- function(x) { as.numeric(x >= xi[2]) }
Hc <- data.frame(cbind(hc1(x), hc2(x), hc3(x)))
names(Hc) <- paste("hc", 1:3, sep="")
fit.pwc <- lm(y~.,data=Hc)
plot(x, y)
lines(x, fx, lwd = 2)
lines(x[x<xi[1]], predict(fit.pwc)[x<xi[1]], lty = 2, col = "blue",lwd=3)
lines(x[x>xi[1]&x<xi[2]], predict(fit.pwc)[x>xi[1]&x<xi[2]], lty = 2, col = "blue",lwd=3)
lines(x[x>xi[2]], predict(fit.pwc)[x>xi[2]], lty = 2, col = "blue",lwd=3)
#abline(v=1/3,col="white",lwd=6);abline(v=2/3,col="white",lwd=6)
```

Finally, we use a piecewise linear function.

```{r echo=FALSE, fig.cap="Piecewise linear approximation."}
Hl <- cbind(Hc, hc1(x)*x, hc2(x)*x, hc3(x)*x)
names(Hl)[4:6] <- paste("hl", 1:3, sep="")
fit.pwl <- lm(y~., data=Hl)
plot(x, y)
lines(x, fx, lwd = 2)
lines(x[x<xi[1]], predict(fit.pwl)[x<xi[1]], lty = 2, col = "blue",lwd=3)
lines(x[x>xi[1]&x<xi[2]], predict(fit.pwl)[x>xi[1]&x<xi[2]], lty = 2, col = "blue",lwd=3)
lines(x[x>xi[2]], predict(fit.pwl)[x>xi[2]], lty = 2, col = "blue",lwd=3)
```


The approximation improves. Nevertheless it would be nice if the different line segments would line up. What we need are piecewise polynomials which are "smooth" at the knots. Such functions are called "splines". We assume that $f$ can be expressed by a set of basis functions 

$$ f(X)=\sum_{j=1}^{p}\beta_j B_j(X).$$
For example for a cubic spline with $K$ fixed knots and fixed polynomial degree $d=3$ ("cubic") we have $p=K+d+1$ and the $B_j(x)$'s form a B-spline basis (one could also use the truncated-power basis). The coefficients $\beta_m$ are estimated using OLS. Although we have only one single variable $X$, the design matrix consists of $p=K+d+1$ features and we quickly run into issues due to overfitting. In `R` we obtain a B-spline basis with `bs` and we can plot the basis functions $B_j(x)$ as follows.

```{r}
spl <- bs(x,df=10) # cubic spline with p=10 degrees of freedom
plot(spl[,1]~x, ylim=c(0,max(spl)), type='l', lwd=2, col=1, 
     xlab="Cubic B-spline basis", ylab="")
for (j in 2:ncol(spl)) lines(spl[,j]~x, lwd=2, col=j)
```

The estimated coefficients $\hat \beta_j$ are obtain using `lm`.
```{r}
fit.csp <- lm(y~spl)
#fit.csp <- lm(y~bs(x,df=10))
coef(fit.csp)
```
The cubic spline with $p=10$ degrees of freedom fits the data well as shown in the next plot (in dashed violet). 

```{r}
plot(x, y)
lines(x, fx, lwd = 2)
lines(x, predict(fit.csp), lty = 2, col = "violet",lwd=3)
```

An alternative approach are so-called *smoothing splines*, where we take $p=n$ and the $B_j(x)$'s are an n-dimensional set of basis functions representing the family of natural cubic splines with knots at the unique values of $x_i$, $i=1,\ldots,n$. The coefficients $\beta_j$ cannot be estimated using OLS as the number $p$ of basis functions (columns of the design matrix) equals the number of observations $n$. Smoothing splines overcome this hurdle by imposing a generalized ridge penalty on the spline coefficients $\beta_j$, i.e.

\[\hat{\beta}_{\lambda}=\textrm{arg}\min\limits_{\beta}\;\|\textbf{y}- \textbf{B} \beta\|^2+\lambda \beta^T\Omega\beta,\]

where $\bf B$ is the design matrix with $jth$ column $(B_j(x_1),\ldots,B_j(x_n))^T$. In practice we can fit 
smoothing splines using the function `smooth.spline`. The penalty term is specified by setting the effective degrees of freedom $\nu$ or by selecting $\lambda$ using cross-validation (see Section \@ref(choice-of-penalty-parameter)).


```{r, eval=FALSE,include=FALSE}
#https://www.hse.ru/data/2018/03/15/1164357459/5._Splines.html
#https://cswr.nrhstat.org/3-5-splines.html
# https://lbelzile.github.io/lineaRmodels/splines.html
#https://bobby.gramacy.com/surrogates/splines.html


my.knots <- sort(unique(x))[-c(1,101)]
bs_mat <- ns(x,knots=my.knots)
bmat2 <- splineDesign(knots=my.knots,x=x,outer.ok = TRUE)
fit.smsp <- smooth.spline(x, y, all.knots = TRUE,keep.stuff = TRUE)
fit.smsp$auxM

x <- seq(0, 1, by=0.001)
spl <- ns(x,df=6)
plot(spl[,1]~x, ylim=c(min(spl),max(spl)), type='l', lwd=2, col=1, 
     xlab="Cubic B-spline basis", ylab="")
for (j in 2:ncol(spl)) lines(spl[,j]~x, lwd=2, col=j)
```

We fit smoothing splines to our simulation example.

```{r}
# smoothing spline with 10 effective degrees of freedom
fit.smsp.df10 <- smooth.spline(x, y, df = 10) 

# smoothing spline with 30 effective degrees of freedom
fit.smsp.df30 <- smooth.spline(x, y, df = 30) 

# smoothing spline with effective degrees of freedom estimated by cv
fit.smsp.cv <- smooth.spline(x, y) 

plot(x, y)
lines(x, fx, lwd = 2)
lines(x, fit.smsp.df10$y, lty = 2, col = "blue",lwd=3)
lines(x, fit.smsp.df30$y, lty = 3, col = "green",lwd=3)
lines(x, fit.smsp.cv$y, lty = 4, col="red",lwd=3)
legend(0.7,1.5,
       lty=1:4,
       lwd=3,
       col=c("black","blue","green","red"),
       legend=c("truth","cubic p=10","cubic p=30","smoothing"))
```

The smoothing spline with $\nu=30$ (in green) leads to overfitting. The smoothing splines obtained by cross-validation (in red) or by fixing $\nu=10$ (in blue) are both good approximation of the truth. The corresponding effective degrees of freedom of the cross-validation solution can be retrieved from the model fit.

```{r}
fit.smsp.cv$df
```

## Lasso Regression

We have discussed Ridge regression and discussed its properties. Although Ridge regression can deal with high-dimensional data a disadvantage compared to subset- and stepwise regression is that it does not perform variable selection and therefore the interpretation of the final model is more challenging. 

In Ridge regression we minimize $\rm RSS(\beta)$ given constraints on the so-called *L2-norm* of the regression coefficients

\[\|\beta\|^2_2=\sum_{j=1}^p \beta^2_j \leq c.\] 

Another very popular approach in high-dimensional statistics is *Lasso regression* (Lasso=least absolute shrinkage and selection operator). The Lasso works very similarly. The only difference is that constraints are imposed on the *L1-norm* of the coefficients

\[\|\beta\|_1=\sum_{j=1}^p |\beta_j| \leq c.\]

Therefore the Lasso is referred to as L1 regularization. The change in the form of the constraints (L2 vs L1) has important implications. Figure \@ref(fig:lassogeom) illustrates the geometry of the Lasso optimization. Geometrically the Lasso constraint is a diamond with "corners" (the Ridge constraint is a circle). If the sum of squares "hits" one of these corners then the coefficient corresponding to the axis is shrunk to zero. As $p$ increases, the multidimensional diamond has an increasing number of corners, and so it is highly likely that some coefficients will be set to zero. Hence, the Lasso performs not only shrinkage but it also sets some coefficients to zero, in other words the Lasso simultaneously performs variable selection. A disadvantage of the "diamond" geometry is that in general there is no closed form solution for the Lasso (the Lasso optimisation problem is not differentiable at the corners of the diamond).

```{r lassogeom,echo=FALSE,out.width="80%",fig.cap="Geometry of Lasso regression."}
knitr::include_graphics("lasso_geometry.JPG")
```

Similar to Ridge regression the Lasso can be formulated as a penalisation problem

\[
\hat{\beta}^{\rm Lasso}_{\lambda}=\text{arg}\min\limits_{\beta}\;\textrm{RSS}(\beta)+\lambda\|\beta\|_1.
\]

To fit the Lasso we use `glmnet` (with $\alpha=1$).

```{r}
fit.lasso.glmnet <-glmnet(x=xtrain,y=ytrain,alpha=1) 
```

The following figure shows the Lasso solution for a grid of $\lambda$ values. We note that the Lasso shrinks some coefficients to exactly zero.

```{r}
plot(fit.lasso.glmnet,xvar="lambda",label=TRUE)
```

We choose the optimal tuning parameter $\lambda_{\rm opt}$ by cross-validation.

```{r warning=FALSE}
cv.lasso.glmnet <-cv.glmnet(x=xtrain,y=ytrain,alpha=1) 
plot(cv.lasso.glmnet)
cv.lasso.glmnet$lambda.min
```

The coefficient for the optimal model can be extracted using the `coef` function.

```{r}
beta.lasso <- coef(fit.lasso.glmnet, s = cv.lasso.glmnet$lambda.min)
names(beta.lasso) <- colnames(xtrain)
beta.lasso
```

We now discuss some properties of the Lasso.

### Numerical optimization and soft thresholding

In general there is no closed-form solution for the Lasso. The optimization has to be performed numerically. An efficient algorithm is implemented in `glmnet` and is referred to as "Pathwise Coordinate Optimization". The algorithm updates one regression coefficient at a time using the so-called soft-thresholding function. This is done iteratively until some convergence criterion is met.

An exception is the case with an orthonormal design matrix $\bf X$, i.e. $\bf X^T\bf X=\bf I$. Under this assumption we have

\begin{align*}
\textrm{RSS}(\beta)&=(\textbf{y}-\textbf{X}\beta)^T(\textbf{y}-\textbf{X}\beta)\\
&=\textbf{y}^T\textbf{y}-2\beta^T\hat\beta^{\rm OLS}+\beta^T\hat\beta
\end{align*}

and therefore the Lasso optimization reduces to $j=1,\ldots,p$ univariate problems

\[\textrm{minimize}\; -\hat\beta_j^{\rm OLS}\beta_j+0.5\beta_j^2+0.5\lambda |\beta_j|.\]

In the exercises we will show that the solution is

\begin{align*}
\hat{\beta}_{\lambda,j}^{\textrm{Lasso}}&=\textrm{sign}(\hat{\beta}_j^{\rm OLS})\left(|\hat{\beta}_j^{\rm OLS}|-0.5\lambda\right)_{+}\\
&=\left\{\begin{array}{ll}
      \hat\beta^{\rm OLS}_j-0.5\lambda & {\rm if}\;\hat\beta^{\rm OLS}_j>0.5\lambda\\
      0 & {\rm if}\;|\hat\beta^{\rm OLS}_j|\leq 0.5\lambda\\
 \hat\beta^{\rm OLS}_j+0.5\lambda & {\rm if}\;\hat\beta^{\rm OLS}_j<-0.5\lambda
    \end{array}
  \right.
\end{align*}


That is, in the orthonormal case, the Lasso is a function of the OLS estimator. This function, depicted in the next figure, is referred to as *soft-thresholding*.

```{r, echo=FALSE, fig.cap="Soft-thresholding function."}
softthreshold <- function(x,lambda=1){
  sign(x)*pmax(abs(x)-0.5*lambda,0)
}
curve(softthreshold,xlim=c(-2,2),xlab="beta ols",ylab="beta lasso")
```

The soft-thresholding function is not only used for numerical optimization of the Lasso but also plays a role in wavelet thresholding used for signal and image denoising.

### Variable selection

We have seen that the Lasso simultaneously shrinks coefficients and sets some of them to zero. Therefore the Lasso performs variable selection which leads to more interpretabel models (compared to Ridge regression). For the Lasso we can define the set of selected variables

$$\hat S^{\rm Lasso}_{\lambda}=\{j\in (1,\ldots,p); \hat\beta^{\rm Lasso}_{\lambda,j}\neq 0\}$$

In our example this set can be obtained as follows.

```{r}
Shat <- rownames(beta.lasso)[which(beta.lasso != 0)]
Shat
```

An interesting question is whether the Lasso does a good or bad job in variable selection. That is, does $\hat S^{\rm Lasso}_{\lambda}$ tend to agree with the true set of active variables $S_0$? Or, does the Lasso typically under- or over-select covariates? These questions are an active field of statistical research. 

### Elasticnet Regression

We have encountered the L1 and L2 penalty. The Lasso (L1) penalty
has the nice property that it leads to sparse solutions, i.e. it simultaneously performs variable selection. A disadvantage is that the Lasso penalty is somewhat indifferent to the choice among a set of strong but correlated variables. The Ridge (L2) penalty, on the other hand, tends
to shrink the coefficients of correlated variables toward each other. An attempt to take the best of both worlds is the *elastic net* penalty which has the form 

\[\lambda \Big(\alpha \|\beta\|_1+(1-\alpha)\|\beta\|_2^2\Big).\]

The second term encourages highly correlated features to be averaged, while
the first term encourages a sparse solution in the coefficients of these averaged features.

In `glmnet` the elastic net regression is implemented using the mixing parameter $\alpha$. The default is $\alpha=1$, i.e. the Lasso.

## Diabetes example

We now review what we have learned with an example. The data that we consider consist of
observations on 442 patients, with the response of interest being a quantitative
measure of disease progression one year after baseline. There are ten baseline
variables â€” age, sex, body-mass index, average blood pressure, and six blood
serum measurements â€” plus quadratic terms, giving a total of $p=64$ features. The task for a statistician is to construct a model that predicts the response $Y$ from the covariates. The two hopes are, that the model would produce accurate baseline
predictions of response for future patients, and also that the form of the model would suggest
which covariates were important factors in disease progression.

We start by splitting the data into training and test data.

```{r}
diabetes <- readRDS(file="data/diabetes.rds")
data <- as.data.frame(cbind(y=diabetes$y,diabetes$x2))
colnames(data) <- gsub(":",".",colnames(data))
train_ind <- sample(seq(nrow(data)),size=nrow(data)/2)
data_train <- data[train_ind,]
xtrain <- as.matrix(data_train[,-1])
ytrain <- data_train[,1]
data_test <- data[-train_ind,]
xtest <- as.matrix(data_test[,-1])
ytest <- data_test[,1]
```


```{r eval=FALSE, include=FALSE}
fit <- lm(y~age+sex+bmi+map,data=data)
summary(fit)
```

We perform forward stepwise regression.


```{r}
# Forward regression
fit0 <- lm(y~1,data=data_train)
up.model <- paste("~", 
                  paste(
                    colnames(data_train[,-1]),collapse=" + ")
                  )
fit.fw <- stepAIC(fit0,direction="forward",
                  scope=list(lower=fit0,
                             upper=up.model
                             
                  ),
                  trace = FALSE
)
#summary(fit.fw)
```

The selection process is depicted in the following table.

```{r}
kable(as.data.frame(fit.fw$anova),digits=2,
      booktabs=TRUE)
```

The regression coefficients and the corresponding statistics of the AIC-optimal model are shown next.

```{r}
kable(broom::tidy(fit.fw),digits=2,
      booktabs=TRUE)
```

We continue by fitting Ridge regression. We show the trace plot and the cross-validation plot.

```{r}
# Ridge
set.seed(1515)
fit.ridge <- glmnet(xtrain,ytrain,alpha=0)
fit.ridge.cv <- cv.glmnet(xtrain,ytrain,alpha=0)
plot(fit.ridge,xvar="lambda")
plot(fit.ridge.cv)
```

Finally, we run the Lasso approach and show the trace and the cross-validation plots.

```{r, fit.cap="Trace and cross-validation lot."}
# Lasso
set.seed(1515)
fit.lasso <- glmnet(xtrain,ytrain,alpha=1)
fit.lasso.cv <- cv.glmnet(xtrain,ytrain,alpha=1)
plot(fit.lasso,xvar="lambda")
plot(fit.lasso.cv)#fit.lasso.cv$lambda.1se
```

We calculate the root-mean-square errors (RMSE) on the test data and compare with the full model.

```{r}
# Full model
fit.full <- lm(y~.,data=data_train)
# RMSE
pred.full <- predict(fit.full,newdata=data_test)
pred.fw <- predict(fit.fw,newdata=data_test)
pred.ridge <- as.vector(predict(fit.ridge,newx=xtest,s=fit.ridge.cv$lambda.1se))
pred.lasso <- as.vector(predict(fit.lasso,newx=xtest,s=fit.lasso.cv$lambda.1se))
res.rmse <- data.frame(
  method=c("full","forward","ridge","lasso"),
  rmse=c(RMSE(pred.full,ytest),RMSE(pred.fw,ytest),RMSE(pred.ridge,ytest),RMSE(pred.lasso,ytest)))
kable(res.rmse,digits = 2,
      booktabs=TRUE)
```

The Lasso has the lowest generalization error (RMSE). We plot the regression coefficients for all 3 methods.

```{r, echo=FALSE}
beta.fw <- coef(fit.fw)
beta.ridge <- coef(fit.ridge,s=fit.ridge.cv$lambda.1se)
beta.lasso <- coef(fit.lasso,s=fit.lasso.cv$lambda.1se)
res.coef <- data.frame(forward=0,ridge=as.numeric(beta.ridge),lasso=as.numeric(beta.lasso))
rownames(res.coef) <- rownames(beta.ridge)
res.coef[names(beta.fw),"forward"] <- beta.fw
res.coef$coef <- rownames(res.coef)
res.coef.l <- pivot_longer(res.coef,cols=c("forward","ridge","lasso"),names_to="method")

res.coef.l%>%
  dplyr::mutate(coef=factor(coef,levels = unique(coef)))%>%
  ggplot(.,aes(x=coef,y=value,fill=method))+
  geom_bar(width=0.5,position = position_dodge(width = 0.8),stat="identity")+
  theme_bw()+
  theme(legend.position = "top",axis.text.x = element_text(angle = 90,vjust = 0.5, hjust=1))+
  scale_fill_grey(aesthetics = c("fill","color"))+
  xlab("")+ylab("beta")
```



```{r, eval=FALSE,echo=FALSE}
library(lars)
data("diabetes")
x <- diabetes$x #mean=0, L2 norm=1
xs <- scale(diabetes$x) #mean=0, var=1
x2 <- diabetes$x2
x2s <- scale(x2)
y <- diabetes$y-mean(diabetes$y)
ys <- scale(y)

# Lasso
fit <- glmnet(xs,ys,alpha=1,standardize = FALSE,intercept=FALSE)
plot(fit,xvar="norm",label=TRUE) # Fig 6.2 in SLS
fit.cv <- cv.glmnet(xs,ys,alpha=1,standardize = FALSE,intercept=FALSE)
plot(fit.cv) # Fig 6.5
```

```{r, eval=FALSE,echo=FALSE}
library(lars)
data("diabetes")
dat <- read.csv("data/diabetes_raw.csv")
colnames(dat) <- c("age","sex","bmi","map","tc","ldl","hdl","tch","ltg","glu","y")
# x <- apply(dat[,-11],2,FUN=function(x){
#   x <- x-mean(x)
#   x/sqrt(sum(x^2))
# })
x <- scale(dat[,-11])/(sqrt(nrow(dat)-1))
head(x[,"age"])
head(diabetes$x2[,"age"])
head(diabetes$x2[,"age^2"])
age2 <- dat$age^2
age2c <- age2-mean(age2)
age2sc <- age2c/sqrt(sum(age2c^2))
xage2 <- (x[,"age"])^2
xage2c <- xage2-mean(xage2)
xage2sc <- xage2c/sqrt(sum(xage2c^2))
head(xage2sc)

x2 <- scale(x^2)/sqrt((nrow(x)-1))
colnames(x2) <- paste0(colnames(x2),"^2")
xint <- scale(model.matrix(~0+.^2,data=data.frame(x)))/sqrt((nrow(x)-1))
head(xint[,"age:sex"])
head(diabetes$x2[,"age:sex"])

xfull <- cbind(x,x2,xint)[,colnames(diabetes$x2)]
```

```{r, eval=FALSE,echo=FALSE}
library(lars)
data("diabetes")
x2s <- scale(diabetes$x2)
colnames(x2s) <- gsub(":",".",colnames(x2s))
y <- diabetes$y-mean(diabetes$y)
dat <- as.data.frame(cbind(y,x2s))

# Forward regression
fit0 <- lm(y~1,data=dat)
fit.fw <- stepAIC(fit0,direction="forward",
                  scope=list(lower=fit0,upper=paste("~", paste(colnames(x2s), collapse=" + "))
                             ),
                  trace = FALSE
                  )
kable(as.data.frame(fit.fw$anova),digits=2,booktabs=TRUE)

#summary(fit.fw)
beta.fw <- coef(fit.fw)
kable(broom::tidy(fit.fw),digits=2,booktabs=TRUE)

# Ridge
set.seed(1515)
fit.ridge <- glmnet(x2s,y,alpha=0,intercept = FALSE,standardize = FALSE)
fit.ridge.cv <- cv.glmnet(x2s,y,alpha=0,intercept = FALSE,standardize = FALSE)
plot(fit.ridge.cv)
fit.ridge.cv$lambda.1se
beta.ridge <- coef(fit.ridge,s=fit.ridge.cv$lambda.1se)

# Lasso
set.seed(1515)
fit.lasso <- glmnet(x2s,y,alpha=1,intercept = FALSE,standardize = FALSE)
fit.lasso.cv <- cv.glmnet(x2s,y,alpha=1,intercept = FALSE,standardize = FALSE)
plot(fit.lasso.cv)
fit.lasso.cv$lambda.1se
beta.lasso <- coef(fit.lasso,s=fit.lasso.cv$lambda.1se)


res.coef.l%>%
  dplyr::filter(coef!="(Intercept)")%>%
  dplyr::mutate(coef=factor(coef,levels = unique(coef)))%>%
  ggplot(.,aes(x=coef,y=value,fill=method))+
  geom_bar(stat="identity",position = position_dodge(width = .7),width=0.8)+
  theme_bw()+
  theme(legend.position = "top",axis.text.x = element_text(angle = 90,vjust = 0.5, hjust=1))+
  xlab("")+ylab("beta")
```

We point out that the same analysis can be conducted with the `caret` package. The code to do so is provided next.

```{r eval=FALSE}
tc <- trainControl(method = "cv", number = 10)

## Ridge
lambda.grid <- fit.ridge.cv$lambda
fit.ridge.caret<-train(x=xtrain,
                       y=ytrain, 
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 0,
                                              lambda=lambda.grid),
                       trControl = tc
) 

# CV curve
plot(fit.ridge.caret)
# Best lambda
fit.ridge.caret$bestTune$lambda
# Model coefficients
coef(fit.ridge.caret$finalModel,fit.ridge.cv$lambda.1se)%>%head
# Make predictions
fit.ridge.caret %>% predict(xtest,s=fit.ridge.cv$lambda.1se)%>%head

## Lasso
lambda.grid <- fit.lasso.cv$lambda
fit.lasso.caret<-train(x=xtrain,
                       y=ytrain, 
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 1,
                                              lambda=lambda.grid),
                       trControl = tc
) 

# CV curve
plot(fit.lasso.caret)
# Best lambda
fit.lasso.caret$bestTune$lambda
# Model coefficients
coef(fit.lasso.caret$finalModel,
     fit.lasso.caret$bestTune$lambda)%>%head
# Make predictions
fit.lasso.caret%>%predict(xtest,
                          s=fit.ridge.cv$lambda.1se)%>%head

## Compare Ridge and Lasso
models <- list(ridge= fit.ridge.caret,ridge = fit.lasso.caret)
resamples(models) %>% summary( metric = "RMSE")
```

