---
title: "Multivariate Regression"
author: "Nicolas Städler"
date: "3/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hide",warning=FALSE)
```


```{r, echo=FALSE}
library(caret)
library(glmnet)
library(tidyverse)
```

## General Notation
We will typicall denote an input variable by the symbol $X$. If $X$ is a vector, its components can be accessed
by subscripts $X_j$. Quantative outputs will be denoted by $Y$, and qualitative outputs by $G$ (for group). We use uppercase letters such as $X$, $Y$ or $G$ when referring to the generic aspects of a variable. Observed values are written in lowercase; hence $i$th observed value of $X$ is written as $x_i$ (where $x_i$ is again a scalar or vector). Matrices are represented by bold uppercase letters; for example, a set of $N$ input p-vectors $x_i$, $i=1,\ldots,N$ would be represented by the $N\times p$ matrix $\bf{X}$. All vectors are assumed to be column vectors, the $i$th row of $\bf{X}$ is $x_i^T$.

For the moment we can loosely state the learning task as follows: given the value of an input vector $X$, make a good prediction of the output $Y$, denoted by $\hat Y$ (pronounced "y-hat"). $\hat Y$ is the outcome of a learning rule $\hat{f}(X)$.

We need data to construct learning rules. We thus suppose we have available a set of measurments $(x_i,y_i)$, $i=1,\ldots,N$, known as the training data, with which to construct $\hat{f}(X)$. 

## Statistical Learning

A key goal of a statistical learning is prediction. Therefore the assessment of how the method generalizes beyond 
the observed data is extremely important in practice. 

When buliding a good learning rule $\hat{f}(X)$ we need to keep in mind the following 2 fundamental steps in statistical learning:

1. Training step: In this step we explore based on training data a series of learning rules $\hat{f}_k(X)$, $k=1,...,K$. We compare the prediction performance of each rule. We choose the rule with the least prediction error.

2. Testing step: In this step we test the performance of the best learning rule $\hat{f}(X)$ identified in the Trainig step based on independent test data.

To perform the Training step we need to estimate the prediction error for the different learning methods. There are several approaches to do so. Examples include the use of information criteria (AIC, BIC or Mallows' Cp) or the use of re-sampling approaches (cross-validation and the bootstrap).


## Multivariate Regression and Least Squares

We will discuss in this course several learning rules. In this section we consider Multivariate Regression. Given a vector of inputs $X=(X_1,X_2,\ldots,X_p)$, we predict the output $Y$ via the model:

\[ \hat{Y}=\sum_{j=1}^{p}X_j\hat\beta_j.\]

The term $\beta_0$ is the intercept. If we include the constant variable 1 in $X$, include $\hat\beta_0$ in the vector of coefficients $\hat\beta$, then we can write 

\[\hat{Y}=X^T\hat\beta.\]

How do we fit the linear model to a set of training data? In Multivariate Regression we typically use least squares. In this approach, we pick the coefficient $\beta$ to minimize the residual sum of squares

\begin{align*}
\rm{RSS}(\beta)&=\sum_{i=1}^{N}(y_i-x_i^T\beta)^2\\
&=(\bf y - \bf X \beta)^T (\bf y - \bf X \beta)\\
&=\|\bf y - \bf X \beta\|^2.
\end{align*}

If the matrix $\bf X^T \bf X$ is nonsingular, then the solution is given by 

\[\hat\beta=(\bf X^T \bf X)^{-1}\bf X^T \bf y.\]

Thus, the learning rule at the new input point $X_0$ is

\begin{align*}
\hat{Y}&=\hat{f}(X_0)\\
&=X_0^T\hat\beta\\
&=X_0^T(\bf X^T \bf X)^{-1}\bf X^T \bf y.\\
\end{align*}

## Overfitting and High-dimensionality

Overfitting refers to the phenomenon of modelling the noise rather than the signal. In case the true model is parsimonious (few covariates driving the response) and data on many covariates are available, it is likely that a linear combination of all covariates yields a higher likelihood than a combination of the few that are actually related to the response. As only the few covariates related to the response contain the signal, the model involving all covariates then cannot
but explain more than the signal alone: it also models the error. Hence, it overfits the data. In high-dimensional
settings overfitting is a real threat. The number of explanatory variables exceeds the number of observations. It
is thus possible to form a linear combination of the covariates that perfectly explains the response, including the
noise.
Large estimates of regression coefficients are often an indication of overfitting. Augmentation of the estimation
procedure with a constraint on the regression coefficients is a simple remedy to large parameter estimates. As a
consequence it decreases the probability of overfitting. Overfitting is illustrated in the next example.

```{r}
set.seed(1)
n <- 10
p <- 9
beta <- c(1,rep(0,p-1))

# simulate covariates
x <- matrix(rnorm(n*p),n,p)
y <- x%*%beta+rnorm(n,sd=0.5)

# lm
fit <- lm(y~0+x)
summary(fit)
coef(fit)
```
As $\beta = (1, 0, \ldots, 0)$, many regression coefficient are clearly over-estimated.

The fitted values are plotted against the values of the first covariates in the right bottom panel of
As a reference the line x = y is added, which represents the ‘true’ model. The fitted model follows
the ‘true’ relationship. But it also captures the deviations from this line that represent the errors.

```{r}
t.d <-data.frame(x=x[,1],y=y,yhat=predict(fit)) 
t.d%>%
  ggplot(.,aes(x=x))+
  geom_point(aes(y=y),size=2)+
  geom_line(aes(y=yhat),col="red")+
  geom_abline(yintercept=0,xintercept=1,col="blue",lty=2)
```


```{r}
set.seed(1)
n <- 20
p <- 50
# simulate covariates
xtrain <- matrix(rnorm(n*p),n,p)
xtest <- matrix(rnorm(n*p),n,p)
# simulate outcome
beta0 <- 0
beta <- c(2,rep(0,p-1))
ytrain <- beta0+xtrain%*%beta+rnorm(n,sd=1)
ytest <- beta0+xtest%*%beta+rnorm(n,sd=1)
# train and test data as data.frames
dtrain <- data.frame(cbind(ytrain,xtrain))
colnames(dtrain) <- c("y",paste0("x",1:ncol(xtrain)))
dtest <- data.frame(cbind(ytest,xtest))
colnames(dtest) <- c("y",paste0("x",1:ncol(xtrain)))
```


```{r}
fit <- lm(y~x1,data=dtrain)
dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
```
```{r}
fm <- as.formula(paste0("y~",paste0("x",1:9,collapse="+")))
fit <- lm(fm,data=dtrain)
summary(fit)
dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
```


```{r}
fm <- as.formula(paste0("y~",paste0("x",1:10,collapse="+")))
fit <- lm(fm,data=dtrain)
summary(fit)
dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
```

```{r}
fm <- as.formula(paste0("y~",paste0("x",1:19,collapse="+")))
fit <- lm(fm,data=dtrain)
summary(fit)
dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
```

We note by including more noise variables into the model we improve the fit to the training data. However, on the performance on the test data continuously gets worse. Ultimately we get a perfect fit if $p=n-1$. In other words there are no residual degrees of freedom. Also for $p\leq n-1$ the matrix $\bf X^T \bf X$ becomes singular. A rule of thumb is that residual degrees of freedom should be at least 10.

## Collinearity and High-dimensionality

Recall collinearity in regression analysis refers to the event of two (or multiple) covariates being strongly linearly related. The case of two (or multiple) covariates being perfectly linearly dependent is referred as super-collinearity.

```{r}
set.seed(1)
n <- 20
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- x1+rnorm(n,sd=0.25)
x4 <- x1
y <- 2*x1+rnorm(n)
dat <- data.frame(y,x1,x2,x3,x4)
```

```{r}
dat%>%
  ggplot(.,aes(x=x1,y=x2))+
  geom_point()+
  ggtitle("no collinearity")

dat%>%
  ggplot(.,aes(x=x1,y=x3))+
  geom_point()+
  ggtitle("collinearity")

dat%>%
  ggplot(.,aes(x=x1,y=x4))+
  geom_point()+
  ggtitle("super collinearity")
```

In the presence of collinearity, the estimate of one variable's impact on the dependent variable Y while controlling for the others tends to be less precise than if predictors were uncorrelated with one another. 

Add Figure with "shaky" plain.

```{r}
set.seed(1)
summary(lm(y~x1+x2,data=dat))
summary(lm(y~x1+x3,data=dat))
summary(lm(y~x1+x4,data=dat))
```
In the case of super-collinearity the OLS is not defined as rank of the design matrix is 1 and therefore the matrix $\bf X^T \bf X$ is singular and therefore not invertable. 

```{r}
x <- as.matrix(dat[,-1])
det(t(x)%*%x)
```

High-dimensional problems with $p>n$ always suffer from super-collinearity: the rank of the design matrix is maximally equal to $n$ which implies that the columns of $\bf X$ are linearly dependent. The OLD estimator cannot be calculated in this situation.

```{r}
xtx <- t(xtest)%*%xtest
det(xtx)
#solve(txt)
```

## Contrainted Least Squares

We have seen that OLS falls short in the high-dimensional context. It leads to overfitting and as a result in large estimates of regression coefficients. Augmentation of the estimation
procedure with a constraint on the regression coefficients is a simple remedy to large parameter estimates. As a consequence it decreases the probability of overfitting. In the following we will discuss methods which minimize $\rm{RSS}(\beta)$ under some contstraints on the parameter $\beta$.


## Subset selection in regression

We illustrate the Training and Testing steps based on linear regression.

First we simulate training and test data. We generate $p=15$ covariates $X_{i1},\ldots,X_{ip}$ as i.i.d $N(0,1)$. We assume the outcome $Y$ depends only on the first covariate and follows $Y_i\sim \beta_1 X_{i1}+\epsilon_i$, $\beta_1=2$, $\epsilon_i\sim N(0,1)$. We assume the size of the training and test data is $n:=n_{\rm train}=n_{\rm test}=20$.

```{r}
set.seed(1)
n <- 20
p <- 15
# simulate covariates
x_train <- matrix(rnorm(n*p),n,p)
x_test <- matrix(rnorm(n*p),n,p)

# simulate outcome
beta0 <- 0
beta <- c(2,0,0,0,2,rep(0,p-5))
y_train <- beta0+x_train%*%beta+rnorm(n,sd=1)
y_test <- beta0+x_test%*%beta+rnorm(n,sd=1)

# train and test data as data.frames
data_train <- data.frame(cbind(y_train,x_train))
colnames(data_train) <- c("y",paste0("x",1:ncol(x_train)))
data_test <- data.frame(cbind(y_test,x_test))
colnames(data_test) <- c("y",paste0("x",1:ncol(x_train)))
```

In this example we build a series of $K=4$ candidate learning rules.  

1. Linear regression including all $p=15$ covariates
2. Linear regression including covariates $X_{10},\ldots,X_{15}$ 
3. Linear regression including covariates $X_{1},\ldots,X_{5}$ covariates
4. Linear regression including only $X_1$.

If we introduce the so-called set of active variables $S_k=\{j\in (1,\ldots,p); \beta_{j}\neq 0\}$ the 4 learning rules are given by the equations 

\begin{align*}
\hat{f}_k(X_0)&=X_0^T\hat\beta_{S_k}\\
&=X_0^T(\bf X_{S_k}^T \bf X_{S_k})^{-1}\bf X_{S_k}^T \bf y,
\end{align*}
where $S_{1}=\{1,\ldots,p\}$, $S_{2}=\{10, 11, 12, 13, 14, 15\}$, $S_{3}=\{1, 2, 3, 4, 5\}$ and $S_{4}=\{1\}$.

```{r, show=TRUE}
fit1 <- lm(y~.,data=data_train)
fit2 <- lm(y~x10+x11+x12+x13+x14+x15,data=data_train)
fit3 <- lm(y~x1+x2+x3+x4+x5,data=data_train)
fit4 <- lm(y~x1,data=data_train)
```

We now compare the different learning rules based on AIC, BIC and leave-one-out cross-validation (LOOCV).

```{r, echo=FALSE}
res <- data.frame(
  LearningRule=c("LR1","LR2","LR3","LR4")
  )
res$AIC <- res$BIC <- res$LOOCV <- res$RSStrain <- res$RSStest <- NA
res$AIC <- c(AIC(fit1),AIC(fit2),AIC(fit3),AIC(fit4))
res$BIC <- c(BIC(fit1),BIC(fit2),BIC(fit3),BIC(fit4))

library(caret)
res$LOOCV[1] <- train(formula(fit1), data = data_train, 
             method = "lm",
             trControl = trainControl(method = "LOOCV"))$results$RMSE
res$LOOCV[2] <- train(formula(fit2), data = data_train, 
             method = "lm",
             trControl = trainControl(method = "LOOCV"))$results$RMSE
res$LOOCV[3] <- train(formula(fit3), data = data_train, 
             method = "lm",
             trControl = trainControl(method = "LOOCV"))$results$RMSE
res$LOOCV[4] <- train(formula(fit4), data = data_train, 
             method = "lm",
             trControl = trainControl(method = "LOOCV"))$results$RMSE

res$RSStrain[1] <- mean((data_train$y-predict(fit1))^2)
res$RSStrain[2] <- mean((data_train$y-predict(fit2))^2)
res$RSStrain[3] <- mean((data_train$y-predict(fit3))^2)
res$RSStrain[4] <- mean((data_train$y-predict(fit4))^2)

res$RSStest[1] <- mean((data_test$y-predict(fit1,newdata=data_test))^2)
res$RSStest[2] <- mean((data_test$y-predict(fit2,newdata=data_test))^2)
res$RSStest[3] <- mean((data_test$y-predict(fit3,newdata=data_test))^2)
res$RSStest[4] <- mean((data_test$y-predict(fit4,newdata=data_test))^2)


knitr::kable(res,digits = 2)
```

We note that learning rule 4 is achieves the smallest prediction error on test data which coincides with 
the outcome from LOOCV. Note the 

## Forward stepwise selection 

In the previous section we compared learning methods based on 4 different subsets. For $p$ potential covariates there are $2^p$ possible subsets. Hence the number of potential learning methods grows exponentially with $p$. When an exhaustive search is not possible, we can seek a good path through them. One of the most common search methods is forward stepwise selection. This starts with the intercept, and then sequentially adds into the model the covariate that most improves the fit. In that way we create a series of learning methods and we can choose the best one as described above.

```{r, echo=FALSE}
tc <- trainControl(method = "cv", number = 10)
fit.forward <- train(y ~., data = data_train,
                     method = "leapForward", 
                     tuneGrid=data.frame(nvmax=1:15),
                     trControl = tc,
                     trace = FALSE
                    )
# nvmax
fit.forward$bestTune
# accuracy
fit.forward$results
# summary of the model
summary(fit.forward$finalModel)
# final model coefficients
coef(fit.forward$finalModel,2)
```

## Ridge Regression

Subset selection and forward regression outlined above all work by either including or
excluding covariates, i.e. contrain specific regression coefficients to be zero.
An alternative is Ridge Regression, which regularises the problem by shrinking
regression coefficients towards zero. This discourages complex models because
models that overfit tend to have larger coefficients. We can formulate Ridge Regression as the constrained optimization problem

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=\rm{argmin}_{\|\beta\|^2\leq c}\frac{1}{n}\rm{RSS}(\beta).
\end{align*}

The constrained estimation interpretation of the ridge regression estimator is illustrated in the figure. It shows the levels sets of the $\rm{RSS}(\beta)$ which are ellipsoids and centered around zero the circular ridge
parameter constraint parametrized by some $c > 0$. The ridge regression estimator
is then the point where the smallest level set hits the constraint. Exactly at that point the $\rm{RSS}(\beta)$ is
minimized over those $\beta$’s that live inside the constraint.

![](ridge_geometry.JPG)

Alternatively Ridge Regression can be cast as 
minimisation of the penalised residual sum of squares, with
a penalty on the magnitude of the coefficients

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=\rm{argmin}\frac{1}{n}\rm{RSS}(\beta)+\lambda\|\beta\|^2.
\end{align*}

Both formulation are equivalent. However we will use more often the penalisation definition.
$\lambda$ the amount of penalisation. Note that for "no penalization", $\lambda=0$, The Ridge Regression coincides with OLS. Increasing $\lambda$ has the effect of shinking the regression coefficients to zero.

```{r}
fit.ridge.glmnet <-glmnet(x=x_train,y=y_train,alpha=0) 
plot(fit.ridge.glmnet,xvar="lambda",label=TRUE)

```

The Ridge optimization problem has the closed form solution

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=(\bf X^T \bf X+\lambda \bf I)^{-1}\bf X^T \bf y,
\end{align*}

which can be easily verified by taking the derivative of the loss function $l_{\rm Ridge}(\beta|\bf Y,\bf X)=\rm{RSS}(\beta)+\lambda\|\beta\|^2$ and equating it to zero. Note that for $\lambda>0$ the matrix $\bf X^T \bf X+\lambda \bf I$ has always full rank and therefore the Ridge Regression is well defined in the high-dimensional context (in contrast to OLS).


### Choice of penalty parameter

We proceed as explained in the section on statistical learning. We define a grid 
of penalty parameters $0<\lambda_1<\lambda_2<\ldots\lambda_K$ and choose the optimal $\lambda_{\rm opt}$ by either a re-sampling approach or by using an information criteria. In glmnet we can use cross-validation

```{r}
cv.ridge.glmnet <-cv.glmnet(x=x_train,y=y_train,alpha=0) 
plot(cv.ridge.glmnet)
cv.ridge.glmnet$lambda.min
```

Although Ridge Regression involves all $p$ covariate the degrees of freedom $df$ is smaller then $p$ as we imposed constraints. One can show that the degrees of freedom for Ridge Regression equals 

\[\rm DF^{\rm Ridge}(\lambda)=\sum_{j=1}^{p}\frac{d_j^2}{d_j^2+\lambda},\]
where $d_1,\ldots,d_p$ are the singular values of $\bf X$.

```{r}
fit.svd <- svd(x_train)
fit.svd$d
df_lambdaopt <- sum(fit.svd$d^2/(fit.svd$d^2+cv.ridge.glmnet$lambda.1se))
```

In our example the residual degrees of freedom is $n-DF^{\rm Ridge}(\lambda_{\rm opt})=$`r round(nrow(x_train)-df_lambdaopt,2)` which lies within our "rule of thumb" of minimum of $5-10$ residual degrees of freedom to avoid overfitting.

### Shrinkage property

We noted that another challenge in multivariate regression is collinearity. In particular, we have seen that OLS becomes unstable with highly correlated covariates. Ridge regression counter acts this by shrinking low-variance components more than high-variance components. 

We note that $\bf \hat Y^{OLS}$ can be viewed has $\bf Y$ orthogonally projected onto the hyperplane spanned the column space of $\bf X$. In term of principle components we get

\[\bf \hat Y^{OLS}=\sum_{j=1}^{p}u_j u_j^T y.\]

Similar we can represent the Ridge projection as

\[\bf \hat Y^{Ridge}=\sum_{j=1}^{p}u_j \frac{d_j^2}{d_j^2+\lambda}u_j^T y.\]

This shows that the largest shrinkage is in the direction of the last principle component which in return is the component with largest variance. This is illustrated in Figure XXX.

Although Ridge Regression works in high-dimenions, $p>>n$, a disadvantage is that it does not perform variable selection and therefore interpretation of the model is more challenging. 

### Bayesian interpretation

Finally Ridge Regression beviewed as the maximum a posetrior (MAP) estimate of hierarchical bayesian model: 
\[Y_i|X_i,\beta\sim N(X_i^T\beta,\sigma^2),\; i=1,\ldots,n\]
with prior distribution on the regression coefficients
\[\beta_j \sim N(0,\tau^2),\; j=1,\ldots,p\]

### Towards non-parametric regression

Some times it is extremely unlikely that the true function $f(X)$ is actually linear in $X$. Consider the following example
```{r}
# define function
n <- 101
x <- seq(0, 1, length.out = n)
fx <- sin(2 * pi * x)

# generate noisy data
set.seed(1)
y <- fx + rnorm(n, sd = 0.5)

plot(x, y)             # data
lines(x, fx, lwd = 2)  # f(x)
legend("topright", legend = "f(x)", lty = 1, lwd = 2, bty = "n")
```

One approach to find a non-parametric approximation are so-called smoothing splines. They write 
\[\hat f(x)=\sum_{j=1}^{N}N_j(x)\hat\beta_j,\]
where $N_j(x)$'s are the natural cubic spline basis. The coefficients $\beta$ are the solution to the Ridge Regression 

\[\|\bf Y- \bf N \beta\|^2+\lambda \beta^T\Omega\beta,\]
with design matrix $\bf N$ has $jth$ column $(N_j(x_1),\ldots,N_j(x_n))^T$. In practice we can fit 
smoothing spline using the function smooth.spline. The penalty term is specified by setting the degree of freedom or by cross-validation.

```{r}
fit.smsp <- smooth.spline(x, y, df = 10)
fit.smsp.cv <- smooth.spline(x, y)
plot(x, y)
lines(x, fx, lwd = 2)
lines(x, fit.smsp$y, lty = 2, col = 2,lwd=3)
lines(x, fit.smsp.cv$y, lty = 3, col=3,lwd=3)
```


## Caret package
In the context of statistical learning we would proceed as follows

1. Choose a grid $0<\lambda_1<\lambda_2<\ldots\lambda_K$
2. Define a series of learning rule

\begin{align*}
Y_{\rm new}&=\hat{f}_k(X_{\rm new})\\
&=X_{\rm new}\hat \hat{\beta}^{\rm Ridge}_{\lambda_k}
\end{align*}

3. Use Cross-validation to find the optimal learning rule

```{r}
lambda.max <- 10
lambda.min <- 10^{-6}*lambda.max
lambda.grid <- 10^seq(log10(lambda.min),log10(lambda.max),length=100)
fit.ridge<-train(y ~., 
                 data = data_train,
                 method = "glmnet",
                 preProcess = c("center", "scale"),
                 tuneGrid = expand.grid(alpha = 0,lambda=lambda.grid),
                 trControl = tc
) 

#
plot(fit.ridge)
# Best lambda
fit.ridge$bestTune$lambda
# Model coefficients
coef(fit.ridge$finalModel,fit.ridge$bestTune$lambda)
# Make predictions
yhat <- fit.ridge %>% predict(data_test)
```

```{r}
models <- list(forward= fit.forward,ridge = fit.ridge)
resamples(models) %>% summary( metric = "RMSE")
```


## The Lasso

A ridge solution can be hard to interpret because it is not sparse (none of the $\beta_j$'s is set exactly to $0$). 

The Lasso works similar to Ridge but shrinkage is achieved by penalizing the L1 norm of the regression coefficients instead of the Euclidean (L2) norm

\begin{align*}
\hat{\beta}^{\rm Lasso}_{\lambda}&=\rm{argmin}-\frac{1}{n}\rm{RSS}(\beta)+\lambda\|\beta\|.
\end{align*}

The following Figure shows the Lasso solution for a grid of lambda values. Interestingly, we note that the Lasso
shrinks some coefficients to exactly zero. This is an important feature of the L1 penalty.

```{r}
fit.lasso.glmnet <-glmnet(x=x_train,y=y_train,alpha=1) 
plot(fit.lasso.glmnet,xvar="lambda",label=TRUE)
```

To identify the optimal model we proceed as in Ridge Regression.

```{r eval=FALSE}
mysd <- function(y) sqrt(sum((y-mean(y))^2)/length(y))
sx <- scale(x,scale=apply(x, 2, mysd))
sx <- as.matrix(sx, ncol=20, nrow=100)
sy <- as.vector(scale(y, scale=mysd(y)))
max(abs(colSums(sx*sy)))/100
## [1] 0.1758808
fitGLM <- glmnet(sx,sy)
max(fitGLM$lambda)
## [1] 0.1758808

fit.lasso.glmnet <-glmnet(x=x_train,y=y_train,
                          alpha=1,
                          standardize=FALSE,
                          intercept=FALSE) 
max(fit.lasso.glmnet$lambda)
max(abs(t(x_train)%*%y_train))/nrow(x_train)
```


```{r}
lambda.max <- max(abs(t(x_train)%*%y_train))/nrow(x_train)
lambda.min <- 10^{-6}*lambda.max
lambda.grid <- 10^seq(log10(lambda.min),log10(lambda.max),length=100)
fit.lasso<-train(y ~., 
                 data = data_train,
                 method = "glmnet",
                 tuneGrid = expand.grid(alpha = 1,lambda=lambda.grid),
                 trControl = tc
) 

#
plot(fit.lasso)
# Best lambda
fit.lasso$bestTune$lambda
# Model coefficients
coef(fit.lasso$finalModel,fit.lasso$bestTune$lambda)
# Make predictions
yhat <- fit.lasso %>% predict(data_train)
```

We discuss now some important properties of the Lasso.

### Variable Selection
The Lasso does not only shrink coefficients to zero but also performs variable selection and therefore 
leads to more interpretabel models. To understand the difference between L1 and L2 penalization the following Geometric interpretation is helpful:

The lasso performs L1 shrinkage, so that there are "corners'' in the constraint, which in two dimensions corresponds to a diamond. If the sum of squares "hits'' one of these corners, then the coefficient corresponding to the axis is shrunk to zero.

As p increases, the multidimensional diamond has an increasing number of corners, and so it is highly likely that some coefficients will be set equal to zero. Hence, the lasso performs shrinkage and (effectively) subset selection.

For the Lasso we can define the selecte variables

$$S^{\rm Lasso}_{\lambda}=\{j\in (1,\ldots,p); \hat\beta^{\rm Lasso}_{\lambda,j}\neq 0\}$$

An interesting question is whether the Lasso does a good job in variable selection. In particular, does $S^{\rm Lasso}_{\lambda}$ agree with the true set of active variables $S_0=\{j\in (1,\ldots,p); \beta_{j}\neq 0\}$, does Lasso typically under or over select variables, i.e. $S^{\rm Lasso}_{\lambda}\subset S_0$ or $S_0 \subset S^{\rm Lasso}_{\lambda}$? These questions were an active field of mathematical statistics. We briefly discuss two results of this research.


### Numerical Optimization and Soft Thresholding

In contrast to Ridge Regression, there is no closed form solution to the Lasso optimization problem. In the case of orthonormal design matrix $\bf X$, ie. $\bf X^T\bf X=\bf I$, we have

\[\rm{RSS}(\beta)=(Y-X\beta)^T(Y-X\beta)=\|\hat\beta-\beta\|^2\]

and therefore 

\[\hat\beta^{\rm Lasso}=\rm{argmin} (\hat\beta_j^{\rm OLS}-\beta_j)^2+\lambda |\beta_j|\]

Soft Thresholding also plays an important role in the efficient optimization of the Lasso problem for non orthonormal design matrix. Essentially the algorithm iterates in a coordinate-wise fashion and in each step solves soft thresholing problem. The process is iterated until convergence.

## Elastic Net

## Caret Package

The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

- data splitting
- pre-processing
- feature selection
- model tuning using resampling
- variable importance estimation

as well as other functionality.

There are many different modeling functions in R. Some have different syntax for model training and/or prediction. The package started off as a way to provide a uniform interface the functions themselves, as well as a way to standardize common tasks (such parameter tuning and variable importance).


Read https://cran.r-project.org/web/packages/caret/vignettes/caret.html for a short intro and 
https://topepo.github.io/caret/ for a long introduction.

In this section we explore the caret package to compare performance of the different regression approaches based on a real data example.


## Details on the Prediction Error

\[Y=f(X)+\epsilon\]

where $E[\epsilon]=0$ and $Var(\epsilon)=\sigma_{\epsilon}^2$, then prediction error of a learning method $\hat{f}(X)$ at a new input point $X=x_0$ is given by:

\begin{align*}
\rm{Err}(x_0)&=E[(Y-\hat{f}(x_0))^2|X=x_0]\\
&=\sigma^2_{\epsilon}+(E[\hat{f}(x_0)]-f(x_0))^2+ E(\hat{f}(x_0)-E\hat{f}(x_0))^2\\
&=\rm{Irreducible Error}+\rm{Bias}^2+Variance.
\end{align*}


## Exercises

1. Take covariates from dataset XYZ and simulate data with 5 active covariates. Run forward regression, 
ridge regression and lasso regression

2. Calculate the Ridge and the Lasso solution for orthogonal covariates.

3. 
