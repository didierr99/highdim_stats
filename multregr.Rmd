---
title: "Multivariate Regression"
author: "Nicolas Städler"
date: "3/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warnings=FALSE)
```


```{r, include=FALSE}
library(knitr)
library(caret)
library(glmnet)
library(tidyverse)
library(MASS)
library(lars)
library(gridExtra)
library(stats)
library(splines)
```

## General Notation
We will typicall denote an input variable by the symbol $X$. If $X$ is a vector, its components can be accessed
by subscripts $X_j$. Quantative outputs will be denoted by $Y$, and qualitative outputs by $G$ (for group). We use uppercase letters such as $X$, $Y$ or $G$ when referring to the generic aspects of a variable. Observed values are written in lowercase; hence $i$th observed value of $X$ is written as $x_i$ (where $x_i$ is again a scalar or vector). Matrices are represented by bold uppercase letters; for example, a set of $N$ input p-vectors $x_i$, $i=1,\ldots,N$ would be represented by the $N\times p$ matrix $\bf{X}$. All vectors are assumed to be column vectors, the $i$th row of $\bf{X}$ is $x_i^T$.

For the moment we can loosely state the learning task as follows: given the value of an input vector $X$, make a good prediction of the output $Y$, denoted by $\hat Y$ (pronounced "y-hat"). $\hat Y$ is the outcome of a learning rule $\hat{f}(X)$.

We need data to construct learning rules. We thus suppose we have available a set of measurments $(x_i,y_i)$, $i=1,\ldots,N$, known as the training data, with which to construct $\hat{f}(X)$. 

## Statistical Learning

A key goal of a statistical learning is prediction. Therefore the assessment of how the method generalizes beyond 
the observed data is extremely important in practice. 

When buliding a good learning rule $\hat{f}(X)$ we need to keep in mind the following 2 fundamental steps:

1. Training step: In this step we explore based on training data a series of learning rules $\hat{f}_k(X)$, $k=1,...,K$. We compare the prediction performance of each rule. We choose the rule with the least prediction error.

2. Testing step: In this step we test the performance of the best learning rule $\hat{f}(X)$ based on independent test data.

To perform the Training step we need to estimate the prediction error for the different learning methods. There are several approaches to do so. Examples include the use of information criteria (AIC, BIC or Mallows' Cp) or the use of re-sampling approaches (cross-validation and the bootstrap).


## Multivariate Regression and Least Squares

We will discuss in this course several learning rules. In this section we consider Multivariate Regression. Given a vector of inputs $X=(X_1,X_2,\ldots,X_p)$, we predict the output $Y$ via the model:

\[ \hat{Y}=\sum_{j=1}^{p}X_j\hat\beta_j.\]

The term $\beta_0$ is the intercept. If we include the constant variable 1 in $X$, include $\hat\beta_0$ in the vector of coefficients $\hat\beta$, then we can write 

\[\hat{Y}=X^T\hat\beta.\]

How do we fit the linear model to a set of training data? In Multivariate Regression we typically use least squares. In this approach, we pick the coefficient $\beta$ to minimize the residual sum of squares

\begin{align*}
\rm{RSS}(\beta)&=\sum_{i=1}^{N}(y_i-x_i^T\beta)^2\\
&=(\bf y - \bf X \beta)^T (\bf y - \bf X \beta)\\
&=\|\bf y - \bf X \beta\|^2.
\end{align*}

If the matrix $\bf X^T \bf X$ is nonsingular, then the solution is given by 

\[\hat\beta=(\bf X^T \bf X)^{-1}\bf X^T \bf y.\]

Thus, the learning rule at the new input point $X_{\rm new}$ is

\begin{align*}
\hat{Y}&=\hat{f}(X_{\rm new})\\
&=X_{\rm new}^T\hat\beta\\
&=X_{\rm new}^T(\bf X^T \bf X)^{-1}\bf X^T \bf y.\\
\end{align*}

## Overfitting and High-dimensionality

Overfitting refers to the phenomenon of modelling the noise rather than the signal. In case the true model is parsimonious (few covariates driving the response) and data on many covariates are available, it is likely that a linear combination of all covariates yields a higher likelihood than a combination of the few that are actually related to the response. As only the few covariates related to the response contain the signal, the model involving all covariates then cannot
but explain more than the signal alone: it also models the error. Hence, it overfits the data. In high-dimensional
settings overfitting is a real threat. The number of explanatory variables exceeds the number of observations. It
is thus possible to form a linear combination of the covariates that perfectly explains the response, including the
noise.
Large estimates of regression coefficients are often an indication of overfitting. Overfitting is illustrated in the next example.

We simulate training data with $p=9$ covariates $X_{i1},\ldots,X_{ip}$ as i.i.d $N(0,1)$. The outcome $Y$ depends only on the first covariate according to $Y_i\sim \beta_1 X_{i1}+\epsilon_i$, where $\beta_1=2$ and $\epsilon_i\sim N(0,0.5)$. The size of the training data is $n=10$.

```{r}
set.seed(1)
n <- 10
p <- 9
beta <- c(2,rep(0,p-1))

# simulate covariates
xtrain <- matrix(rnorm(n*p),n,p)
ytrain <- xtrain%*%beta+rnorm(n,sd=0.5)
dtrain <- data.frame(xtrain)
dtrain$y <- ytrain

# lm
fit <- lm(y~-1+.,data=dtrain)
```
The estimated regression coefficients are

```{r}
coef(fit)
```

As the true coefficients are $\beta = (1, 0, \ldots, 0),$ many regression coefficient are clearly over-estimated.

In the next plot the fitted values are plotted against the values of the first covariate.
As a reference the line $x = y$ is added, which represents the ‘true’ model. The fitted model follows
the ‘true’ relationship. But it also captures the deviations from this line that represent the errors.

```{r, echo=FALSE}
t.d <-data.frame(x=xtrain[,1],y=ytrain,yhat=predict(fit)) 
t.d%>%
  ggplot(.,aes(x=x))+
  geom_point(aes(y=y),size=2)+
  geom_line(aes(y=yhat),col="red")+
  geom_abline(intercept=0,slope=1,col="blue",lty=2)
```

In this example the model fits the data perfectly. The residual degrees of freedom, $n-p$, to estimate the error is exactly `r n-p`. A rule of thumb to avoid overfitting is that the residual degrees of freedom should be at least $10$.

## Collinearity and High-dimensionality

Recall that collinearity in regression analysis refers to the event of two (or multiple) covariates being strongly linearly related. The case of two (or multiple) covariates being perfectly linearly dependent is referred as super-collinearity.

The following figures show examples of two covariates with no-, high- and super-collinearity.

```{r, include=FALSE}
set.seed(1)
n <- 20
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- x1+rnorm(n,sd=0.25)
x4 <- x1
dat <- data.frame(x1,x2,x3,x4)
dat$y <- 2*dat$x1+rnorm(n)
```

```{r, echo=FALSE}
gp1 <- dat%>%
  ggplot(.,aes(x=x1,y=x2))+
  geom_point()+
  ggtitle("no collinearity")

gp2 <- dat%>%
  ggplot(.,aes(x=x1,y=x3))+
  geom_point()+
  ggtitle("collinearity")

gp3 <- dat%>%
  ggplot(.,aes(x=x1,y=x4))+
  geom_point()+
  ggtitle("super collinearity")

grid.arrange(gp1,gp2,gp3,ncol=3)
```

In the presence of collinearity, the estimate of one variable's impact on the dependent variable $Y$ while controlling for the others tends to be less precise than if predictors were uncorrelated with one another. Intuitively this can be explained based on the following figure which shows the response Y and two highly correlated covariates x1 and x2: we note that 2-dimensional OLS hyperplain deviates from the true model; further we see that the exact position of OLS hyperplain is very uncertain.

![](collinearity.JPG)


We illustrate this by fitting the following the two models 

```{r}
modela <- lm(y~x1+x2,data=dat)
modelb <- lm(y~x1+x3,data=dat)
```

We see that variance of the estimated regression coefficients is larger in model 2 where x1 and x3 exhibit high collinearity.

```{r}
# variance of model a
diag(vcov(modela))

#variance of model b
diag(vcov(modelb))
```

In the case of super-collinearity the OLS estimate is not defined as the rank of the design matrix is 0 and therefore the matrix $\bf X^T \bf X$ is singular and not invertible. In our example, if we include x4 into the design matrix, the design matrix has not full rank

```{r}
x <- as.matrix(dat[,-5])
det(t(x)%*%x)
```

High-dimensional problems with $p>n$ always suffer from super-collinearity: the rank of the design matrix is maximally equal to $n$ which implies that the columns of $\bf X$ are linearly dependent. The OLS estimator cannot be calculated in this situation.

## Contrainted Least Squares

We have seen that OLS falls short in the high-dimensional context. It leads to overfitting and as a result in large estimates of regression coefficients. Augmentation of the estimation
procedure with a constraint on the regression coefficients is a simple remedy to large parameter estimates. As a consequence it decreases the probability of overfitting. In the following we will discuss methods which minimize $\rm{RSS}(\beta)$ under some contstraints on the parameter $\beta$.


## Subset selection

The most traditional approach to impose constraints is subset selection. In this approach we retain only a subset of the variables, and eliminate the rest from the model. OLS is used to estimate the coefficients of the inputs that are retained. More formally, given a subset $S\subset\{1,\ldots,p\}$ we solve the optimization problem


\begin{align*}
\hat{\beta}_{S}&=\rm{argmin}_{\beta_j=0\;\rm{for\; all}\;j\notin S}\rm{RSS}(\beta)\\
&=(\bf{X}_S^T \bf{X}_S)^{-1}\bf{X}_S^T \bf{y}.
\end{align*}

In practice we need to explore a sequence of subsets $S_1,\ldots,S_K$ and choose the optimal subset by either a re-sampling approach or by using an information criteria. There are a number of different strategies for choosing the sequence of subsets. Best subsets regression consists of looking at possible combinations of covariates. Rather than search though all possible subsets, we can seek a good path through them. Two popular approaches are backward stepwise selection which starts with the full model and sequentially deletes covariates, whereas forward stepwise selection starts with the intercept, and then sequentially adds into the model the covariate that most improves the fit. 

In R we can use stepAIC for subset selection. It uses AIC to select the optimal subset. For example to perform forward stepwise selection we proceed

```{r}
# Forward regression
fit0 <- lm(y~1,data=dtrain)
fit.fw <- stepAIC(fit0,direction="forward",
                  scope=list(lower=fit0,upper=paste("~", paste(colnames(dtrain[,-10]), collapse=" + "))
                             ),
                  trace = FALSE
                  )
```

The selection process can be summarized

```{r}
kable(as.data.frame(fit.fw$anova),digits=3)
```

The regression coefficients of the optimal model are

```{r}
kable(broom::tidy(fit.fw),digits=3)
```

## Ridge Regression

Subset selection and forward regression outlined above all work by either including or
excluding covariates, i.e. contrain specific regression coefficients to be zero.
An alternative is Ridge Regression, which regularises the problem by shrinking
regression coefficients towards zero. This discourages complex models because
models that overfit tend to have larger coefficients. We can formulate Ridge Regression as the constrained optimization problem

\begin{align*}
\hat{\beta}^{\rm Ridge}_{s}&=\rm{argmin}_{\|\beta\|^2\leq s}\rm{RSS}(\beta).
\end{align*}

The constrained estimation interpretation of the ridge regression estimator is illustrated in the figure. It shows the levels sets of the $\rm{RSS}(\beta)$ which are ellipsoids and centered around zero the circular ridge
parameter constraint parametrized by some $s > 0$. The ridge regression estimator
is then the point where the smallest level set hits the constraint. Exactly at that point the $\rm{RSS}(\beta)$ is
minimized over those $\beta$’s that live inside the constraint.

![](ridge_geometry.JPG)

Alternatively Ridge Regression can be cast as 
minimisation of the penalised residual sum of squares, with
a penalty on the magnitude of the coefficients

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=\rm{argmin}\rm{RSS}(\beta)+\lambda\|\beta\|^2.
\end{align*}

Both formulation are equivalent (one-to-one relationship between $s$ and $\lambda$). We will use more often the second "penalisation" formulation. The parameter $\lambda$ is the amount of penalisation. Note that for "no penalization", $\lambda=0$, the Ridge Regression coincides with OLS. Increasing $\lambda$ has the effect of shinking the regression coefficients to zero. 

```{r}
fit.ridge.glmnet <-glmnet(x=xtrain,y=ytrain,alpha=0) 
plot(fit.ridge.glmnet,xvar="lambda",label=TRUE)
```

The Ridge optimization problem has the closed form solution

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=(\bf X^T \bf X+\lambda \bf I)^{-1}\bf X^T \bf y,
\end{align*}

which can be easily verified by taking the derivative of the loss function $\ell_{\rm Ridge}(\beta|\bf Y,\bf X)=\rm{RSS}(\beta)+\lambda\|\beta\|^2$ and equating it to zero. Note that for $\lambda>0$ the matrix $\bf X^T \bf X+\lambda \bf I$ has always full rank and therefore the Ridge Regression is well defined in the high-dimensional context (in contrast to OLS).


### Choice of penalty parameter

We proceed as explained in the section on statistical learning. We define a grid 
of penalty parameters $0<\lambda_1<\lambda_2<\ldots<\lambda_K<\infty$ and choose the optimal $\lambda_{\rm opt}$ by either a re-sampling approach or by using an information criteria. In glmnet we can use cross-validation

```{r}
cv.ridge.glmnet <-cv.glmnet(x=xtrain,y=ytrain,alpha=0) 
plot(cv.ridge.glmnet)
cv.ridge.glmnet$lambda.min
```

Although Ridge Regression involves all $p$ covariate the degrees of freedom $\rm{DF}$ is smaller then $p$ as we imposed constraints. One can show that the degrees of freedom for Ridge Regression equals 

\[\rm DF^{\rm Ridge}(\lambda)=\sum_{j=1}^{p}\frac{d_j^2}{d_j^2+\lambda},\]
where $d_1,\ldots,d_p$ are the singular values of $\bf X$.

```{r, echo=FALSE}
# get singular values
fit.svd <- svd(xtrain) #fit.svd$d

# ridge degree of freedom for lambdaopt
df_lambdaopt <- sum(fit.svd$d^2/(fit.svd$d^2+cv.ridge.glmnet$lambda.1se))
```

In our example the residual degrees of freedom is $n-\rm{DF}^{\rm Ridge}(\lambda_{\rm opt})$=`r round(nrow(xtrain)-df_lambdaopt,2)` which lies within the "rule of thumb" of minimum of $5-10$ residual degrees of freedom to avoid overfitting.

### Shrinkage property

We noted that another challenge in multivariate regression is collinearity. In particular, we have seen that OLS becomes unstable with highly correlated covariates. Ridge regression counter acts this by shrinking low-variance components more than high-variance components. 

We note that ${\bf \hat y}^{OLS}$ can be viewed has $\bf y$ orthogonally projected onto the hyperplane spanned the column space of $\bf X$. In term of principle components we get

\[{\bf\hat y}^{OLS}=\sum_{j=1}^{p}u_j u_j^T y.\]

Similar we can represent the Ridge projection as

\[{\bf\hat y}^{Ridge}=\sum_{j=1}^{p}u_j \frac{d_j^2}{d_j^2+\lambda}u_j^T y.\]

This shows that the largest shrinkage is in the direction of the last principle component which in return is the component with largest variance. This is illustrated in Figure XXX.

Although Ridge Regression works in high-dimensions, $p>>n$, a disadvantage is that it does not perform variable selection and therefore interpretation of the model is more challenging. 

### Bayesian interpretation

Ridge Regression beviewed as the maximum a posetrior (MAP) estimate of hierarchical bayesian model: 
\[Y_i|X_i,\beta\sim N(X_i^T\beta,\sigma^2),\; i=1,\ldots,n\]
with prior distribution on the regression coefficients
\[\beta_j \sim N(0,\tau^2),\; j=1,\ldots,p\]

### Towards non-parametric regression

Some times it is extremely unlikely that the true function $f(X)$ is actually linear in $X$. Consider the following example

```{r}
# define function
n <- 101
x <- seq(0, 1, length.out = n)
fx <- sin(2 * pi * x)

# generate noisy data
set.seed(1)
y <- fx + rnorm(n, sd = 0.5)

plot(x, y)             # data
lines(x, fx, lwd = 2)  # f(x)
legend("topright", legend = "f(x)", lty = 1, lwd = 2, bty = "n")
```

One approach to find a non-parametric approximation are so-called smoothing splines. The approach starts by approximating $f(x)$ as natural cubic spline 

\[f(x)\approx\sum_{j=1}^{N}N_j(x)\beta_j,\]
where $N_j(x)$'s are an n-dimensional set of basis functions for representing this family of natural splines. For the coefficients $\beta_j$ we take the solution to the generalized ridge regression problem

\[\|\bf Y- \bf N \beta\|^2+\lambda \beta^T\Omega\beta,\]

with design matrix $\bf N$ has $jth$ column $(N_j(x_1),\ldots,N_j(x_n))^T$. In practice we can fit 
smoothing spline using the function smooth.spline. The penalty term is specified by setting the degree of freedom or by cross-validation.

```{r, eval=FALSE,include=FALSE}
#https://www.hse.ru/data/2018/03/15/1164357459/5._Splines.html
#https://cswr.nrhstat.org/3-5-splines.html
# https://lbelzile.github.io/lineaRmodels/splines.html
#https://bobby.gramacy.com/surrogates/splines.html
my.knots <- sort(unique(x))[-c(1,101)]
bs_mat <- ns(x,knots=my.knots)
bmat2 <- splineDesign(knots=my.knots,x=x,outer.ok = TRUE)
fit.smsp <- smooth.spline(x, y, all.knots = TRUE,keep.stuff = TRUE)
fit.smsp$auxM

x <- seq(0, 1, by=0.001)
spl <- ns(x,df=6)
plot(spl[,1]~x, ylim=c(min(spl),max(spl)), type='l', lwd=2, col=1, 
     xlab="Cubic B-spline basis", ylab="")
for (j in 2:ncol(spl)) lines(spl[,j]~x, lwd=2, col=j)
```


```{r}
vv <- read.csv("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/phoneme.data")
```


```{r}
fit.smsp <- smooth.spline(x, y, df = 10)
fit.smsp.cv <- smooth.spline(x, y)
plot(x, y)
lines(x, fx, lwd = 2)
lines(x, fit.smsp$y, lty = 2, col = 2,lwd=3)
lines(x, fit.smsp.cv$y, lty = 3, col=3,lwd=3)
```

## The Lasso

In Ridge Regression we minimize the $\rm RSS(\beta)$ given constrains on the L2-norm of the regression coefficients

\[\|\beta\|^2_2=\sum_{j=1}^p \beta^2_j \leq c.\] 

An other very popular approach in high-dimensional statistics is the Lasso. The Lasso works very similar. The only difference is that constraints are imposed on the so-called L1-norm of the coefficients

\[\|\beta\|_1=\sum_{j=1}^p |\beta_j| \leq c.\]

The change in the form of the constraints has important implications. The following figure illustrates the geometry of the Lasso optimization.

![](lasso_geometry.JPG)

Geometrically the Lasso contraints is a diamond with "corners" (Ridge contraint is a circle). If the sum of squares "hits'' one of these corners, then the coefficient corresponding to the axis is shrunk to zero. As $p$ increases, the multidimensional diamond has an increasing number of corners, and so it is highly likely that some coefficients will be set equal to zero. Hence, the Lasso performs not only shrinkage but it also  sets some coefficients to zero and therefore simultaneously also performs variable selection. A disadvantage of the "diamond" geometry is that there is no closed form solution (Lasso optimisation problem is not differentiable at the corners of the diamond).

Similar to Ridge the Lasso can be formulated as a penalisation problem

\begin{align*}
\hat{\beta}^{\rm Lasso}_{\lambda}&=\rm{argmin}\rm{RSS}(\beta)+\lambda\|\beta\|_1.
\end{align*}

The following Figure shows the Lasso solution for a grid of lambda values. We see that the Lasso shrinks some coefficients to exactly zero.

```{r}
fit.lasso.glmnet <-glmnet(x=xtrain,y=ytrain,alpha=1) 
plot(fit.lasso.glmnet,xvar="lambda",label=TRUE)
```

To identify an optimal model we define a grid 
of penalty parameters $0<\lambda_1<\lambda_2<\ldots\lambda_K$ and choose the optimal $\lambda_{\rm opt}$ by cross-validation.

```{r}
cv.lasso.glmnet <-cv.glmnet(x=xtrain,y=ytrain,alpha=1) 
plot(cv.lasso.glmnet)
cv.lasso.glmnet$lambda.min
```

The optimal Lasso regression coefficient can be optained as

```{r}
beta.lasso <- coef(fit.lasso.glmnet, s = cv.lasso.glmnet$lambda.min)
names(beta.lasso) <- colnames(xtrain)
beta.lasso
```

and we can identify the selected variables

$$S^{\rm Lasso}_{\lambda}=\{j\in (1,\ldots,p); \hat\beta^{\rm Lasso}_{\lambda,j}\neq 0\}$$

as

```{r}
Shat <- rownames(beta.lasso)[which(beta.lasso != 0)]
Shat
```

We discuss now some important properties of the Lasso.

### Numerical Optimization and Soft Thresholding

We mentioned in the beginning that in general the Lasso optimization problem has not a closed from solution. The reason is that the absolute value funtion $|\beta_j|$ is not differential at zero.

In the case of orthonormal design matrix $\bf X$, ie. $\bf X^T\bf X=\bf I$, we have

\begin{align*}
\rm{RSS}(\beta)&=(Y-X\beta)^T(Y-X\beta)\\
&=\sum_{j=1}^p(\beta_j-\hat\beta_j^{\rm OLS})^2
\end{align*}

and therefore the Lasso optimization reduces to $j=1,\ldots,p$ univariate problems

\[\rm{minimize}\quad \left(\hat\beta_j^{\rm OLS}-\beta_j\right)^2+\lambda |\beta_j|.\]

In the exercises we will show that the solution is

\[\hat \beta_{\lambda,j}^{\rm Lasso}=\rm{sign}(\hat\beta_j^{\rm OLS})\left(|\hat\beta_j^{\rm OLS}|-0.5\lambda\right)_{+}.\]

This function is depicted in the next figure. It is also referred to as the soft-thresholing function.

```{r, echo=FALSE}
softthreshold <- function(x,lambda=1){
  sign(x)*pmax(abs(x)-0.5*lambda,0)
}
curve(softthreshold,xlim=c(-2,2),xlab="beta ols",ylab="beta lasso")
```


In general there is no closed-form solution for the Lasso. The optimization has to be performed numerically. An very efficient algorithm is implemented in glmnet. It is based on coordinate-wise optimization, where one coefficient is updated at a time using the soft-thresholding function. This is done iteratively until some convergence criteria is met.

### Variable Selection
The Lasso does not only shrink coefficients to zero but also performs variable selection and therefore 
leads to more interpretabel models. To understand the difference between L1 and L2 penalization the following Geometric interpretation is helpful:


For the Lasso we can define the selecte variables

$$S^{\rm Lasso}_{\lambda}=\{j\in (1,\ldots,p); \hat\beta^{\rm Lasso}_{\lambda,j}\neq 0\}$$

An interesting question is whether the Lasso does a good job in variable selection. In particular, does $S^{\rm Lasso}_{\lambda}$ agree with the true set of active variables $S_0=\{j\in (1,\ldots,p); \beta_{j}\neq 0\}$, does Lasso typically under or over select variables, i.e. $S^{\rm Lasso}_{\lambda}\subset S_0$ or $S_0 \subset S^{\rm Lasso}_{\lambda}$? These questions were an active field of mathematical statistics. We briefly discuss two results of this research.

### Elastic Net

The elastic-net selects variables like
the lasso, and shrinks together the coefficients of correlated predictors like
ridge.

## Diabetes Example

These data consist of
observations on 442 patients, with the response of interest being a quantitative
measure of disease progression one year after baseline. There are ten baseline
variables—age, sex, body-mass index, average blood pressure, and six blood
serum measurements—plus quadratic terms, giving a total of 64 features.The statisticians were asked to construct a model that predicted response y from covariates
x1, x2,... ,x10. Two hopes were evident here, that the model would produce accurate baseline
predictions of response for future patients, and also that the form of the model would suggest
which covariates were important factors in disease progression.

We with splitting into train - and test data.

```{r}
library(lars)
data("diabetes")
data <- as.data.frame(cbind(y=diabetes$y,diabetes$x2))
colnames(data) <- gsub(":",".",colnames(data))
train_ind <- sample(seq(nrow(data)),size=nrow(data)/2)
data_train <- data[train_ind,]
xtrain <- as.matrix(data_train[,-1])
ytrain <- data_train[,1]
data_test <- data[-train_ind,]
xtest <- as.matrix(data_test[,-1])
ytest <- data_test[,1]
```


We perform Forward Regression.
```{r}
# Forward regression
fit0 <- lm(y~1,data=data_train)
fit.fw <- stepAIC(fit0,direction="forward",
                  scope=list(lower=fit0,upper=paste("~", paste(colnames(data_train[,-1]), collapse=" + "))
                             ),
                  trace = FALSE
                  )
#summary(fit.fw)
```

The variable selection process is as follows

```{r}
kable(as.data.frame(fit.fw$anova),digits=2)
```

And the final regression coefficients are

```{r}
kable(broom::tidy(fit.fw),digits=2)
```

We continue with Ridge Regression

```{r}
# Ridge
set.seed(1515)
fit.ridge <- glmnet(xtrain,ytrain,alpha=0)
fit.ridge.cv <- cv.glmnet(xtrain,ytrain,alpha=0)
plot(fit.ridge,xvar="lambda")
plot(fit.ridge.cv)
#fit.ridge.cv$lambda.1se
```

Finally we run the Lasso

```{r}
# Lasso
set.seed(1515)
fit.lasso <- glmnet(xtrain,ytrain,alpha=1)
fit.lasso.cv <- cv.glmnet(xtrain,ytrain,alpha=1)
plot(fit.lasso,xvar="lambda")
plot(fit.lasso.cv)
#fit.lasso.cv$lambda.1se
```

We compare the 3 approaches based on their root-mean-square error (RMSE)

```{r}
# RMSE
pred.fw <- predict(fit.fw,newdata=data_test)
pred.ridge <- as.vector(predict(fit.ridge,newx=xtest,s=fit.ridge.cv$lambda.1se))
pred.lasso <- as.vector(predict(fit.lasso,newx=xtest,s=fit.lasso.cv$lambda.1se))
res.rmse <- data.frame(rmse=c(RMSE(pred.fw,ytest),RMSE(pred.ridge,ytest),RMSE(pred.lasso,ytest)),
                       method=c("fw","ridge","lasso"))
kable(res.rmse,digits = 2)
```

And we plot the final regression coefficients

```{r, echo=FALSE}
beta.fw <- coef(fit.fw)
beta.ridge <- coef(fit.ridge,s=fit.ridge.cv$lambda.1se)
beta.lasso <- coef(fit.lasso,s=fit.lasso.cv$lambda.1se)
res.coef <- data.frame(fw=0,ridge=as.numeric(beta.ridge),lasso=as.numeric(beta.lasso))
rownames(res.coef) <- rownames(beta.ridge)
res.coef[names(beta.fw),"fw"] <- beta.fw
res.coef$coef <- rownames(res.coef)
res.coef.l <- pivot_longer(res.coef,cols=c("fw","ridge","lasso"),names_to="method")

res.coef.l%>%
  dplyr::mutate(coef=factor(coef,levels = unique(coef)))%>%
  ggplot(.,aes(x=coef,y=value,fill=method))+
  geom_bar(stat="identity",position = position_dodge(width = .7),width=0.8)+
  theme(legend.position = "top",axis.text.x = element_text(angle = 90,vjust = 0.5, hjust=1))+
  xlab("")+ylab("beta")
```



```{r, eval=FALSE,echo=FALSE}
data("diabetes")
x <- diabetes$x #mean=0, L2 norm=1
xs <- scale(diabetes$x) #mean=0, var=1
x2 <- diabetes$x2
x2s <- scale(x2)
y <- diabetes$y-mean(diabetes$y)
ys <- scale(y)

# Lasso
fit <- glmnet(xs,ys,alpha=1,standardize = FALSE,intercept=FALSE)
plot(fit,xvar="norm",label=TRUE) # Fig 6.2 in SLS
fit.cv <- cv.glmnet(xs,ys,alpha=1,standardize = FALSE,intercept=FALSE)
plot(fit.cv) # Fig 6.5

```

```{r, eval=FALSE,echo=FALSE}
data("diabetes")
dat <- read.csv("data/diabetes_raw.csv")
colnames(dat) <- c("age","sex","bmi","map","tc","ldl","hdl","tch","ltg","glu","y")
# x <- apply(dat[,-11],2,FUN=function(x){
#   x <- x-mean(x)
#   x/sqrt(sum(x^2))
# })
x <- scale(dat[,-11])/(sqrt(nrow(dat)-1))
head(x[,"age"])
head(diabetes$x2[,"age"])
head(diabetes$x2[,"age^2"])
age2 <- dat$age^2
age2c <- age2-mean(age2)
age2sc <- age2c/sqrt(sum(age2c^2))
xage2 <- (x[,"age"])^2
xage2c <- xage2-mean(xage2)
xage2sc <- xage2c/sqrt(sum(xage2c^2))
head(xage2sc)

x2 <- scale(x^2)/sqrt((nrow(x)-1))
colnames(x2) <- paste0(colnames(x2),"^2")
xint <- scale(model.matrix(~0+.^2,data=data.frame(x)))/sqrt((nrow(x)-1))
head(xint[,"age:sex"])
head(diabetes$x2[,"age:sex"])

xfull <- cbind(x,x2,xint)[,colnames(diabetes$x2)]
```

```{r, eval=FALSE,echo=FALSE}
library(lars)
data("diabetes")
x2s <- scale(diabetes$x2)
colnames(x2s) <- gsub(":",".",colnames(x2s))
y <- diabetes$y-mean(diabetes$y)
dat <- as.data.frame(cbind(y,x2s))

# Forward regression
fit0 <- lm(y~1,data=dat)
fit.fw <- stepAIC(fit0,direction="forward",
                  scope=list(lower=fit0,upper=paste("~", paste(colnames(x2s), collapse=" + "))
                             ),
                  trace = FALSE
                  )
kable(as.data.frame(fit.fw$anova),digits=2)

#summary(fit.fw)
beta.fw <- coef(fit.fw)
kable(broom::tidy(fit.fw),digits=2)

# Ridge
set.seed(1515)
fit.ridge <- glmnet(x2s,y,alpha=0,intercept = FALSE,standardize = FALSE)
fit.ridge.cv <- cv.glmnet(x2s,y,alpha=0,intercept = FALSE,standardize = FALSE)
plot(fit.ridge.cv)
fit.ridge.cv$lambda.1se
beta.ridge <- coef(fit.ridge,s=fit.ridge.cv$lambda.1se)

# Lasso
set.seed(1515)
fit.lasso <- glmnet(x2s,y,alpha=1,intercept = FALSE,standardize = FALSE)
fit.lasso.cv <- cv.glmnet(x2s,y,alpha=1,intercept = FALSE,standardize = FALSE)
plot(fit.lasso.cv)
fit.lasso.cv$lambda.1se
beta.lasso <- coef(fit.lasso,s=fit.lasso.cv$lambda.1se)


res.coef.l%>%
  dplyr::filter(coef!="(Intercept)")%>%
  dplyr::mutate(coef=factor(coef,levels = unique(coef)))%>%
  ggplot(.,aes(x=coef,y=value,fill=method))+
  geom_bar(stat="identity",position = position_dodge(width = .7),width=0.8)+
  theme(legend.position = "top",axis.text.x = element_text(angle = 90,vjust = 0.5, hjust=1))+
  xlab("")+ylab("beta")
```

We finally show how we can use the caret package to perform Ridge and Lasso regression. 
With the caret package we can also easily compare the 3 methods.

```{r}
tc <- trainControl(method = "cv", number = 10)

## Ridge
lambda.grid <- fit.ridge.cv$lambda
fit.ridge.caret<-train(x=xtrain,
                       y=ytrain, 
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 0,lambda=lambda.grid),
                       trControl = tc
) 

# CV curve
plot(fit.ridge.caret)
# Best lambda
fit.ridge.caret$bestTune$lambda
# Model coefficients
coef(fit.ridge.caret$finalModel,fit.ridge.cv$lambda.1se)%>%head
# Make predictions
fit.ridge.caret %>% predict(xtest,s=fit.ridge.cv$lambda.1se)%>%head

## Lasso
lambda.grid <- fit.lasso.cv$lambda
fit.lasso.caret<-train(x=xtrain,
                       y=ytrain, 
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 1,lambda=lambda.grid),
                       trControl = tc
) 

# CV curve
plot(fit.lasso.caret)
# Best lambda
fit.lasso.caret$bestTune$lambda
# Model coefficients
coef(fit.lasso.caret$finalModel,fit.lasso.caret$bestTune$lambda)%>%head
# Make predictions
fit.lasso.caret %>% predict(xtest,s=fit.ridge.cv$lambda.1se)%>%head

## Compare Ridge and Lasso
models <- list(ridge= fit.ridge.caret,ridge = fit.lasso.caret)
resamples(models) %>% summary( metric = "RMSE")
```

## Details on the Prediction Error

\[Y=f(X)+\epsilon\]

where $E[\epsilon]=0$ and $Var(\epsilon)=\sigma_{\epsilon}^2$, then prediction error of a learning method $\hat{f}(X)$ at a new input point $X=x_0$ is given by:

\begin{align*}
\rm{Err}(x_0)&=E[(Y-\hat{f}(x_0))^2|X=x_0]\\
&=\sigma^2_{\epsilon}+(E[\hat{f}(x_0)]-f(x_0))^2+ E(\hat{f}(x_0)-E\hat{f}(x_0))^2\\
&=\rm{Irreducible Error}+\rm{Bias}^2+Variance.
\end{align*}

## Caret Package

The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

- data splitting
- pre-processing
- feature selection
- model tuning using resampling
- variable importance estimation

as well as other functionality.

There are many different modeling functions in R. Some have different syntax for model training and/or prediction. The package started off as a way to provide a uniform interface the functions themselves, as well as a way to standardize common tasks (such parameter tuning and variable importance).


Read https://cran.r-project.org/web/packages/caret/vignettes/caret.html for a short intro and 
https://topepo.github.io/caret/ for a long introduction.

In this section we explore the caret package to compare performance of the different regression approaches based on a real data example.



