---
title: "Multivariate Regression"
author: "Nicolas St√§dler"
date: "3/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results="hide")
```


```{r}
library(caret)
library(glmnet)
library(tidyverse)
```

## General Notation
We will typicall denote an input variable by the symbol $X$. If $X$ is a vector, its components can be accessed
by subscripts $X_j$. Quantative outputs will be denoted by $Y$, and qualitative outputs by $G$ (for group). We use uppercase letters such as $X$, $Y$ or $G$ when referring to the generic aspects of a variable. Observed values are written in lowercase; hence $i$th observed value of $X$ is written as $x_i$ (where $x_i$ is again a scalar or vector). Matrices are represented by bold uppercase letters; for example, a set of $N$ input p-vectors $x_i$, $i=1,\ldots,N$ would be represented by the $N\times p$ matrix $\bf{X}$. All vectors are assumed to be column vectors, the $i$th row of $\bf{X}$ is $x_i^T$.

For the moment we can loosely state the learning task as follows: given the value of an input vector $X$, make a good prediction of the output $Y$, denoted by $\hat Y$ (pronounced "y-hat"). $\hat Y$ is the outcome of a learning rule $\hat{f}(X)$.

We need data to construct learning rules. We thus suppose we have available a set of measurments $(x_i,y_i)$, $i=1,\ldots,N$, known as the training data, with which to construct $\hat{f}(X)$. 


## Multivariate Regression and Least Squares

We will discuss in this course several learning rules. In this section we consider Multivariate Regression. Given a vector of inputs $X=(X_1,X_2,\ldots,X_p)$, we predict the output $Y$ via the model:

\[ \hat{Y}=\sum_{j=1}^{p}X_j\hat\beta_j.\]

The term $\beta_0$ is the intercept. If we include the constant variable 1 in $X$, include $\hat\beta_0$ in the vector of coefficients $\hat\beta$, then we can write 

\[\hat{Y}=X^T\hat\beta.\]

How do we fit the linear model to a set of training data? In Multivariate Regression we typically use least squares. In this approach, we pick the coefficient $\beta$ to minimize the residual sum of squares

\begin{align*}
\rm{RSS}(\beta)&=\sum_{i=1}^{N}(y_i-x_i^T\beta)^2\\
&=(\bf y - \bf X \beta)^T (\bf y - \bf X \beta)\\
&=\|\bf y - \bf X \beta\|^2.
\end{align*}

If the matrix $\bf X^T \bf X$ is nonsingular, then the solution is given by 

\[\hat\beta=(\bf X^T \bf X)^{-1}\bf X^T \bf y.\]

Thus, the learning rule at the new input point $X_0$ is

\begin{align*}
\hat{Y}&=\hat{f}(X_0)\\
&=X_0^T\hat\beta\\
&=X_0^T(\bf X^T \bf X)^{-1}\bf X^T \bf y.\\
\end{align*}

## Statistical Learning

A key goal of a statistical learning is prediction. Therefore the assessment of how the method generalizes beyond 
the observed data is extremely important in practice. 

When buliding a good learning rule $\hat{f}(X)$ we need to keep in mind the following 2 fundamental steps in statistical learning:

1. Training step: In this step we explore based on training data a series of learning rules $\hat{f}_k(X)$, $k=1,...,K$. We compare the prediction performance of each rule. We choose the rule with the least prediction error.

2. Testing step: In this step we test the performance of the best learning rule $\hat{f}(X)$ identified in the Trainig step based on independent test data.

To perform the Training step we need to estimate the prediction error for the different learning methods. There are several approaches to do so. Examples include the use of information criteria (AIC, BIC or Mallows' Cp) or the use of re-sampling approaches (cross-validation and the bootstrap).

## Subset selection in regression

We illustrate the Training and Testing steps based on linear regression.

First we simulate training and test data. We generate $p=15$ covariates $X_{i1},\ldots,X_{ip}$ as i.i.d $N(0,1)$. We assume the outcome $Y$ depends only on the first covariate and follows $Y_i\sim \beta_1 X_{i1}+\epsilon_i$, $\beta_1=2$, $\epsilon_i\sim N(0,1)$. We assume the size of the training and test data is $n:=n_{\rm train}=n_{\rm test}=20$.

```{r}
set.seed(1)
n <- 20
p <- 15
# simulate covariates
x_train <- matrix(rnorm(n*p),n,p)
x_test <- matrix(rnorm(n*p),n,p)

# simulate outcome
beta0 <- 0
beta <- c(2,0,0,0,2,rep(0,p-5))
y_train <- beta0+x_train%*%beta+rnorm(n,sd=1)
y_test <- beta0+x_test%*%beta+rnorm(n,sd=1)

# train and test data as data.frames
data_train <- data.frame(cbind(y_train,x_train))
colnames(data_train) <- c("y",paste0("x",1:ncol(x_train)))
data_test <- data.frame(cbind(y_test,x_test))
colnames(data_test) <- c("y",paste0("x",1:ncol(x_train)))
```

In this example we build a series of $K=4$ candidate learning rules.  

1. Linear regression including all $p=15$ covariates
2. Linear regression including covariates $X_{10},\ldots,X_{15}$ 
3. Linear regression including covariates $X_{1},\ldots,X_{5}$ covariates
4. Linear regression including only $X_1$.

If we introduce the so-called set of active variables $S_k=\{j\in (1,\ldots,p); \beta_{j}\neq 0\}$ the 4 learning rules are given by the equations 

\begin{align*}
\hat{f}_k(X_0)&=X_0^T\hat\beta_{S_k}\\
&=X_0^T(\bf X_{S_k}^T \bf X_{S_k})^{-1}\bf X_{S_k}^T \bf y,
\end{align*}
where $S_{1}=\{1,\ldots,p\}$, $S_{2}=\{10, 11, 12, 13, 14, 15\}$, $S_{3}=\{1, 2, 3, 4, 5\}$ and $S_{4}=\{1\}$.

```{r, show=TRUE}
fit1 <- lm(y~.,data=data_train)
fit2 <- lm(y~x10+x11+x12+x13+x14+x15,data=data_train)
fit3 <- lm(y~x1+x2+x3+x4+x5,data=data_train)
fit4 <- lm(y~x1,data=data_train)
```

We now compare the different learning rules based on AIC, BIC and leave-one-out cross-validation (LOOCV).

```{r, echo=FALSE}
res <- data.frame(
  LearningRule=c("LR1","LR2","LR3","LR4")
  )
res$AIC <- res$BIC <- res$LOOCV <- res$RSStrain <- res$RSStest <- NA
res$AIC <- c(AIC(fit1),AIC(fit2),AIC(fit3),AIC(fit4))
res$BIC <- c(BIC(fit1),BIC(fit2),BIC(fit3),BIC(fit4))

library(caret)
res$LOOCV[1] <- train(formula(fit1), data = data_train, 
             method = "lm",
             trControl = trainControl(method = "LOOCV"))$results$RMSE
res$LOOCV[2] <- train(formula(fit2), data = data_train, 
             method = "lm",
             trControl = trainControl(method = "LOOCV"))$results$RMSE
res$LOOCV[3] <- train(formula(fit3), data = data_train, 
             method = "lm",
             trControl = trainControl(method = "LOOCV"))$results$RMSE
res$LOOCV[4] <- train(formula(fit4), data = data_train, 
             method = "lm",
             trControl = trainControl(method = "LOOCV"))$results$RMSE

res$RSStrain[1] <- mean((data_train$y-predict(fit1))^2)
res$RSStrain[2] <- mean((data_train$y-predict(fit2))^2)
res$RSStrain[3] <- mean((data_train$y-predict(fit3))^2)
res$RSStrain[4] <- mean((data_train$y-predict(fit4))^2)

res$RSStest[1] <- mean((data_test$y-predict(fit1,newdata=data_test))^2)
res$RSStest[2] <- mean((data_test$y-predict(fit2,newdata=data_test))^2)
res$RSStest[3] <- mean((data_test$y-predict(fit3,newdata=data_test))^2)
res$RSStest[4] <- mean((data_test$y-predict(fit4,newdata=data_test))^2)


knitr::kable(res,digits = 2)
```

We note that learning rule 4 is achieves the smallest prediction error on test data which coincides with 
the outcome from LOOCV. Note the 

## Forward stepwise selection 

In the previous section we compared learning methods based on 4 different subsets. For $p$ potential covariates there are $2^p$ possible subsets. Hence the number of potential learning methods grows exponentially with $p$. When an exhaustive search is not possible, we can seek a good path through them. One of the most common search methods is forward stepwise selection. This starts with the intercept, and then sequentially adds into the model the covariate that most improves the fit. In that way we create a series of learning methods and we can choose the best one as described above.

```{r, echo=FALSE}
tc <- trainControl(method = "cv", number = 10)
fit.forward <- train(y ~., data = data_train,
                     method = "leapForward", 
                     tuneGrid=data.frame(nvmax=1:15),
                     trControl = tc,
                     trace = FALSE
                    )
# nvmax
fit.forward$bestTune
# accuracy
fit.forward$results
# summary of the model
summary(fit.forward$finalModel)
# final model coefficients
coef(fit.forward$finalModel,2)
```

## Ridge Regression

Subset selection and forward regression outlined above all work by either including or
excluding covariates, i.e. contrain specific regression coefficients to be zero.
An alternative is Ridge Regression, which regularises the problem by shrinking
regression coefficients towards zero. This discourages complex models because
models that overfit tend to have larger coefficients. Shrinkage is achieved through
minimisation the penalised negative residual sum of squares, with
a penalty on the magnitude of the coefficients,

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=\rm{argmin}-\frac{1}{n}\rm{RSS}(\beta)+\lambda\|\beta\|^2
\end{align*}

The minimization problem can be solved in closed from. We obtain

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=\rm{argmin}-\frac{1}{n}\rm{RSS}(\beta)+\lambda\|\beta\|^2\\
&=(\bf X^T \bf X+\lambda \bf I)^{-1}\bf X^T \bf y.
\end{align*}

$\lambda$ the amount of shrinkage. Note that for "no penalization", $\lambda=0$, The Ridge Regression coincides with 
least squares. Increasing $\lambda$ has the effect of shinking the regression coefficients to zero.

```{r}
fit.ridge.glmnet <-glmnet(x=x_train,y=y_train,alpha=0) 
plot(fit.ridge.glmnet,xvar="lambda",label=TRUE)

```


In the context of statistical learning we would proceed as follows

1. Choose a grid $0<\lambda_1<\lambda_2<\ldots\lambda_K$
2. Define a series of learning rule

\begin{align*}
Y_{\rm new}&=\hat{f}_k(X_{\rm new})\\
&=X_{\rm new}\hat \hat{\beta}^{\rm Ridge}_{\lambda_k}
\end{align*}

3. Use Cross-validation to find the optimal learning rule

```{r}
lambda.max <- 10
lambda.min <- 10^{-6}*lambda.max
lambda.grid <- 10^seq(log10(lambda.min),log10(lambda.max),length=100)
fit.ridge<-train(y ~., 
                 data = data_train,
                 method = "glmnet",
                 preProcess = c("center", "scale"),
                 tuneGrid = expand.grid(alpha = 0,lambda=lambda.grid),
                 trControl = tc
) 

#
plot(fit.ridge)
# Best lambda
fit.ridge$bestTune$lambda
# Model coefficients
coef(fit.ridge$finalModel,fit.ridge$bestTune$lambda)
# Make predictions
yhat <- fit.ridge %>% predict(test.data)
```

```{r}
models <- list(forward= fit.forward,ridge = fit.ridge)
resamples(models) %>% summary( metric = "RMSE")
```


We discuss now some important properties of Ridge Regression.

### Shrinkage property

Although Ridge Regression works in high-dimenions, $p>>n$, a disadvantage is that it does not perform variable selection and therefore interpretation of the model is more challenging. 

Least squares regression becomes unstalbe (high variance) with highly correlated covariates. Ridge regression counter acts this by shrinking low-variance components more than high-variance components.


### Bayesian interpretation

Finally Ridge Regression beviewed as the maximum a posetrior (MAP) estimate of hierarchical bayesian model: 
\[Y_i|X_i,\beta\sim N(X_i^T\beta,\sigma^2),\; i=1,\ldots,n\]
with prior distribution on the regression coefficients
\[\beta_j \sim N(0,\tau^2),\; j=1,\ldots,p\]


## The Lasso

A ridge solution can be hard to interpret because it is not sparse (none of the $\beta_j$'s is set exactly to $0$). 

The Lasso works similar to Ridge but shrinkage is achieved by penalizing the L1 norm of the regression coefficients instead of the Euclidean (L2) norm

\begin{align*}
\hat{\beta}^{\rm Lasso}_{\lambda}&=\rm{argmin}-\frac{1}{n}\rm{RSS}(\beta)+\lambda\|\beta\|.
\end{align*}

The following Figure shows the Lasso solution for a grid of lambda values. Interestingly, we note that the Lasso
shrinks some coefficients to exactly zero. This is an important feature of the L1 penalty.

```{r}
fit.lasso.glmnet <-glmnet(x=x_train,y=y_train,alpha=1) 
plot(fit.lasso.glmnet,xvar="lambda",label=TRUE)
```

To identify the optimal model we proceed as in Ridge Regression.

```{r}
mysd <- function(y) sqrt(sum((y-mean(y))^2)/length(y))
sx <- scale(x,scale=apply(x, 2, mysd))
sx <- as.matrix(sx, ncol=20, nrow=100)
sy <- as.vector(scale(y, scale=mysd(y)))
max(abs(colSums(sx*sy)))/100
## [1] 0.1758808
fitGLM <- glmnet(sx,sy)
max(fitGLM$lambda)
## [1] 0.1758808

fit.lasso.glmnet <-glmnet(x=x_train,y=y_train,
                          alpha=1,
                          standardize=FALSE,
                          intercept=FALSE) 
max(fit.lasso.glmnet$lambda)
max(abs(t(x_train)%*%y_train))/nrow(x_train)
```


```{r}
lambda.max <- max(abs(t(x_train)%*%y_train))/nrow(x_train)
lambda.min <- 10^{-6}*lambda.max
lambda.grid <- 10^seq(log10(lambda.min),log10(lambda.max),length=100)
fit.lasso<-train(y ~., 
                 data = data_train,
                 method = "glmnet",
                 tuneGrid = expand.grid(alpha = 1,lambda=lambda.grid),
                 trControl = tc
) 

#
plot(fit.lasso)
# Best lambda
fit.lasso$bestTune$lambda
# Model coefficients
coef(fit.lasso$finalModel,fit.lasso$bestTune$lambda)
# Make predictions
yhat <- fit.lasso %>% predict(test.data)
```

We discuss now some important properties of the Lasso.

### Variable Selection
The Lasso does not only shrink coefficients to zero but also performs variable selection and therefore 
leads to more interpretabel models. To understand the difference between L1 and L2 penalization the following Geometric interpretation is helpful:

The lasso performs L1 shrinkage, so that there are "corners'' in the constraint, which in two dimensions corresponds to a diamond. If the sum of squares "hits'' one of these corners, then the coefficient corresponding to the axis is shrunk to zero.

As p increases, the multidimensional diamond has an increasing number of corners, and so it is highly likely that some coefficients will be set equal to zero. Hence, the lasso performs shrinkage and (effectively) subset selection.

For the Lasso we can define the selecte variables

$$S^{\rm Lasso}_{\lambda}=\{j\in (1,\ldots,p); \hat\beta^{\rm Lasso}_{\lambda,j}\neq 0\}$$

An interesting question is whether the Lasso does a good job in variable selection. In particular, does $S^{\rm Lasso}_{\lambda}$ agree with the true set of active variables $S_0=\{j\in (1,\ldots,p); \beta_{j}\neq 0\}$, does Lasso typically under or over select variables, i.e. $S^{\rm Lasso}_{\lambda}\subset S_0$ or $S_0 \subset S^{\rm Lasso}_{\lambda}$? These questions were an active field of mathematical statistics. We briefly discuss two results of this research.


### Numerical Optimization and Soft Thresholding

In contrast to Ridge Regression, there is no closed form solution to the Lasso optimization problem. 



## Details on the Prediction Error

\[Y=f(X)+\epsilon\]

where $E[\epsilon]=0$ and $Var(\epsilon)=\sigma_{\epsilon}^2$, then prediction error of a learning method $\hat{f}(X)$ at a new input point $X=x_0$ is given by:

\begin{align*}
\rm{Err}(x_0)&=E[(Y-\hat{f}(x_0))^2|X=x_0]\\
&=\sigma^2_{\epsilon}+(E[\hat{f}(x_0)]-f(x_0))^2+ E(\hat{f}(x_0)-E\hat{f}(x_0))^2\\
&=\rm{Irreducible Error}+\rm{Bias}^2+Variance.
\end{align*}

## Caret Package

The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

- data splitting
- pre-processing
- feature selection
- model tuning using resampling
- variable importance estimation

as well as other functionality.

There are many different modeling functions in R. Some have different syntax for model training and/or prediction. The package started off as a way to provide a uniform interface the functions themselves, as well as a way to standardize common tasks (such parameter tuning and variable importance).


Read https://cran.r-project.org/web/packages/caret/vignettes/caret.html for a short intro and 
https://topepo.github.io/caret/ for a long introduction.


## Exercises

1. Take covariates from dataset XYZ and simulate data with 5 active covariates. Run forward regression, 
ridge regression and lasso regression

2. Calculate the Ridge and the Lasso solution for orthogonal covariates.

3. 
