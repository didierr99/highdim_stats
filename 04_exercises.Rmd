# Exercises

```{r, include=FALSE, warning=FALSE}
library(knitr)
library(caret)
library(glmnet)
library(tidyverse)
library(MASS)
library(lars)
library(gridExtra)
```

## Ridge and Lasso for the orthonormal design

Calculate the Ridge and the Lasso solution for an orthonormal design matrix

## Bayesian interpretation of Ridge Regression

1. Write down the log-likelihood of the linear regression model. 
2. Find the expression for the maximum likelihood estimator.
3. Assuming a posteriory distribution $\beta_1,\ldots,\beta_p$ iid $\sim N(0,\tau^2)$, derive the posterior distribution of $\beta$ and show that the maximum a posteriori estimator (MAP) coincides with the Ridge estimator.

Note: 
$Y_i=X_{i}^T\beta +\epsilon_i,$ where $\epsilon_1,\epsilon_n$ iid $N(0,\sigma^2)$ and $\bf{X}$ is a fixed
$n\times p$ design matrix.

## Prostate Cancer data

Get the prostate cancer dataset from the webpage https://web.stanford.edu/~hastie/ElemStatLearn/datasets. 
Run OLS, Best Subset selection, Ridge Regression and the Lasso. Print the coefficients as a table and calculate for each method the Test error. Compare the results with Table 3.3. of The Elements of Statistical Learning.

## Elastic Net and cross-validation

Get the Riboflavin dataset from the hdi R package. Run the elastic net with $\alpha=0, 0.25, 0.5, 0.75$ and $1$ and create a trace plot. Run cross-validation and decide which $alpha$ leads to best performance (note: use the foldid argument in glment). Plot the final regression coefficients.

## P-values for High-dimensional Regression

Get the Riboflavin dataset from the hdi R package.

## Splines and  SAheart data

```{r}
library(splines)
dat <-  read.table("http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data",sep=",",head=T,row.names=1)

# Computes the logistic regression model using natural splines (note famhist is included as a factor): 
form = "chd ~ ns(sbp,df=4) + ns(tobacco,df=4) + ns(ldl,df=4) + famhist + ns(obesity,df=4) + ns(alcohol,df=4) + ns(age,df=4)"
form = formula(form)

m = glm( form, data=dat, family=binomial )
print( summary(m), digits=3 )

# Duplicates the numbers from Table 5.1:
# 
drop1( m, scope=form, test="Chisq" )

```


```{r, include=FALSE, eval=FALSE}
#https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Phoneme%20Recognition.ipynb
#https://waxworksmath.com/Authors/G_M/Hastie/Code/Chapter5/dup_fig_5_5.R
#https://waxworksmath.com/Authors/G_M/Hastie/WriteUp/Weatherwax_Epstein_Hastie_Solution_Manual.pdf
library(splines)
dat <- read.csv("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/phoneme.data")
dat2 <- dat[dat$g%in%c("aa","ao"),]

dtrain <- dat2[grepl("^train",dat2$speaker),-c(1,259)]
xtrain <- as.matrix(dtrain[,-257])
dtest <- dat2[grepl("^test",dat2$speaker),-c(1,259)]
xtest <- as.matrix(dtest[,-257])
dtrain$y <- ifelse(dtrain$g=="ao",1,0)
dtest$y <- ifelse(dtest$g=="ao",1,0)
dtrain <- dtrain[,-257]
dtest <- dtest[,-257]

# logistic regression
fit <- glm(y~.,data=dtrain,family=binomial)
mean(as.numeric((predict(fit,type="response")>0.5))!=dtrain$y)
mean(as.numeric((predict(fit,type="response",newdata=dtest)>0.5))!=dtest$y)

coef <-  coefficients(fit)
plot( coef, ylim=c(-0.4,+0.4), type="l", xlab="Frequency", ylab="Logistic Regression Coefficients" )

# logistic regression based on filtered features
hmat <- ns(x=1:256,df=12)
xstar <- xtrain%*%hmat
fit <- glm(dtrain$y~xstar,family="binomial")
coef.smooth <- as.numeric(hmat%*%coef(fit)[-1])

plot( coef[-1], 
      ylim=c(-0.4,+0.4), 
      type="l", 
      xlab="Frequency", 
      ylab="Logistic Regression Coefficients" )
lines(coef.smooth,col="red")
abline(h=0)

```

## Another Example

We simulate data for training and testing.

```{r}
set.seed(1)
n <- 20
p <- 50
# simulate covariates
xtrain <- matrix(rnorm(n*p),n,p)
xtest <- matrix(rnorm(n*p),n,p)
# simulate outcome
beta0 <- 0
beta <- c(2,0,0,0,2,rep(0,p-5))
ytrain <- beta0+xtrain%*%beta+rnorm(n,sd=0.5)
ytest <- beta0+xtest%*%beta+rnorm(n,sd=0.5)
# train and test data as data.frames
dtrain <- data.frame(cbind(ytrain,xtrain))
colnames(dtrain) <- c("y",paste0("x",1:ncol(xtrain)))
dtest <- data.frame(cbind(ytest,xtest))
colnames(dtest) <- c("y",paste0("x",1:ncol(xtrain)))
```

Linear regression with only x1.

```{r}
fit <- lm(y~x1,data=dtrain)
t.gp1 <- dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
t.gp2 <- dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
grid.arrange(t.gp1,t.gp2,ncol=2)
```


Linear regression with 10 covariates.
```{r}
fm <- as.formula(paste0("y~",paste0("x",1:10,collapse="+")))
fit <- lm(fm,data=dtrain)
t.gp1 <- dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
t.gp2 <- dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
grid.arrange(t.gp1,t.gp2,ncol=2)
```

Linear regression with 19 covariates
```{r}
fm <- as.formula(paste0("y~",paste0("x",1:19,collapse="+")))
fit <- lm(fm,data=dtrain)
t.gp1 <- dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
t.gp2 <- dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
grid.arrange(t.gp1,t.gp2,ncol=2)
```

Leap Forward Regression.


```{r, echo=FALSE}
tc <- trainControl(method = "cv", number = 10)
fit.forward <- train(y ~., data = dtrain,
                     method = "leapForward", 
                     tuneGrid=data.frame(nvmax=1:15),
                     trControl = tc,
                     trace = FALSE
                    )
# nvmax
fit.forward$bestTune
# accuracy
fit.forward$results
# summary of the model
summary(fit.forward$finalModel)
# final model coefficients
coef(fit.forward$finalModel,2)
```


Now we perform Ridge regession

```{r}
lambda.max <- 10
lambda.min <- 10^{-6}*lambda.max
lambda.grid <- 10^seq(log10(lambda.min),log10(lambda.max),length=100)
fit.ridge<-train(y ~., 
                 data = dtrain,
                 method = "glmnet",
                 tuneGrid = expand.grid(alpha = 0,lambda=lambda.grid),
                 trControl = tc
) 

# Cv plot
plot(fit.ridge)
# Best lambda
fit.ridge$bestTune$lambda
# Model coefficients
coef(fit.ridge$finalModel,fit.ridge$bestTune$lambda)%>%head
# Make predictions
fit.ridge %>% predict(dtest)%>%head
```


```{r}
lambda.max <- max(abs(t(xtrain)%*%ytrain))/nrow(xtrain)
lambda.min <- 10^{-6}*lambda.max
lambda.grid <- 10^seq(log10(lambda.min),log10(lambda.max),length=100)
fit.lasso<-train(y ~., 
                 data = dtrain,
                 method = "glmnet",
                 tuneGrid = expand.grid(alpha = 1,lambda=lambda.grid),
                 trControl = tc
) 

# Cv plot
plot(fit.lasso)
# Best lambda
fit.lasso$bestTune$lambda
# Model coefficients
coef(fit.lasso$finalModel,fit.lasso$bestTune$lambda)%>%head
# Make predictions
fit.lasso %>% predict(dtest)%>%head
```


```{r}
models <- list(forward= fit.forward,ridge = fit.ridge,lasso=fit.lasso)
resamples(models) %>% summary( metric = "RMSE")
```

That is how to calculate lambda_max
```{r}
# with scaling
mysd <- function(y) sqrt(sum((y-mean(y))^2)/length(y))
sx <- scale(xtrain,scale=apply(xtrain, 2, mysd))
sx <- as.matrix(sx, ncol=20, nrow=100)
sy <- as.vector(scale(ytrain, scale=mysd(ytrain)))
max(abs(colSums(sx*sy)))/nrow(xtrain)
fitglmnet <- glmnet(sx,sy)
max(fitglmnet$lambda)

# no scaling
fit.lasso <-glmnet(x=xtrain,y=ytrain,
                   alpha=1,
                   standardize=FALSE,
                   intercept=FALSE) 
max(fit.lasso$lambda)
max(abs(t(xtrain)%*%ytrain))/nrow(xtrain)
```


## Example based on simulated data

Take covariates from dataset XYZ and simulate data with 5 active covariates. Run forward regression, 
ridge regression and lasso regression