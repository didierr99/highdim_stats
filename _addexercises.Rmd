# Additional exercises

```{r, include=FALSE, warning=FALSE}
library(knitr)
library(glmnet)
library(tidyverse)
```

## P-values for High-dimensional Regression

1. Split the Riboflavin data set into two halves ("Data splitting"). 
2. On the first half: run `glmnet` and select the top genes ("Screening").
3. On the second half: take the selected top genes as covariates, run ordinary least squares and report the p-values ("P-value calculation").
4. Write a function combining steps 1-3 and re-run these steps 100 times. For a single gene, plot a histogram of the p-values. For each gene calculate the 10th percentile of the p-values ("P-value aggregation").
5. Use the function `hdi` with `method="multi.split"` to calculate p-values.


We start by splitting the data set into two halves.

```{r}
library(hdi)
library(multcomp)
data(riboflavin)
str(riboflavin)
x <- riboflavin$x
colnames(x) <- make.names(colnames(x))
y <- riboflavin$y

# split data into two halves
set.seed(1)
n <- length(y)
in_sample <- sample(seq(n),size=ceiling(n/2)) 
x_in <- x[in_sample,]
y_in <- y[in_sample]
x_out <- x[-in_sample,]
y_out <- y[-in_sample]
```

We perform variable screening based on the first half. 
```{r}
# screening
fit.cv <- cv.glmnet(x_in, y_in) 
bhat <- as.matrix(coef(fit.cv,s="lambda.min"))[-1,]
hatact <- names(bhat[bhat!=0])
```

Next we calculated p-values based on the second half.

```{r}
# testing
t.dat <- data.frame(cbind(x_out[,hatact,drop=FALSE]))
t.dat$y <- y_out
fit.lm <- lm(y~.,data=t.dat)
fit.glht <- glht(fit.lm)
pvals <- summary(fit.glht, test = adjusted("bonferroni"))$test$pvalues
pvals
```
We write a function in order to re-run the previous steps many times.

```{r}
hdpvalue_singlesplit <- function(x,y){
  
  # splitting
  n <- length(y)
  in_sample <- sample(seq(n),size=ceiling(n/2)) 
  x_in <- x[in_sample,]
  y_in <- y[in_sample]
  x_out <- x[-in_sample,]
  y_out <- y[-in_sample]
  
  # screening
  fit.cv <- cv.glmnet(scale(x_in), y_in,standardize=FALSE) 
  bhat <- as.matrix(coef(fit.cv,s="lambda.1se"))[-1,]
  bhato <- sort(abs(bhat[bhat!=0]),decreasing=TRUE)
  if(length(bhato)<length(y_out)-1){
    hatact <- names(bhato)
  }else{
    hatact <- names(bhato)[1:(length(y_out)-2)]
  }

  # testing
  t.dat <- data.frame(cbind(x_out[,hatact[-1]]))
  t.dat$y <- y_out
  fit.lm <- lm(y~.,data=t.dat)
  fit.glht <- glht(fit.lm)
  pvals <- summary(glht(fit.lm), test = adjusted("bonferroni"))$test$pvalues[-1]
  
  
  # output
  pvalues <- rep(1,length=ncol(x))
  names(pvalues) <- colnames(x)
  pvalues[names(pvals)] <- pvals
  
  return(pvalues)
}

pvals_ms <- replicate(100,hdpvalue_singlesplit(x=x,y=y))
hist(pvals_ms["YXLD_at",],breaks=20)
pval.agg <- sort(apply(pvals_ms,1,quantile,probs=0.1),decreasing=FALSE)
head(pval.agg)
```
The multi-splitting approach is implemented in the function `hdi` function. (Note the function uses clever why to aggregate the p-values.)

```{r eval=FALSE, include=FALSE}
library(hdi)
x <- riboflavin[,-1]
y <- riboflavin[,1]
## Multi-split p-values
set.seed(12)
fit.multi <- hdi(x, y, method = "multi-split", B = 100)
head(sort(fit.multi$pval.corr,decreasing = FALSE))
```
## Overfitting example

We illustrate overfitting based on simulated training and test data.

```{r message=FALSE}
set.seed(1)
n <- 20
p <- 50
# simulate covariates
xtrain <- matrix(rnorm(n*p),n,p)
xtest <- matrix(rnorm(n*p),n,p)
# simulate outcome
beta0 <- 0
beta <- c(2,0,0,0,2,rep(0,p-5))
ytrain <- beta0+xtrain%*%beta+rnorm(n,sd=0.5)
ytest <- beta0+xtest%*%beta+rnorm(n,sd=0.5)
# train and test data as data.frames
dtrain <- data.frame(cbind(ytrain,xtrain))
colnames(dtrain) <- c("y",paste0("x",1:ncol(xtrain)))
dtest <- data.frame(cbind(ytest,xtest))
colnames(dtest) <- c("y",paste0("x",1:ncol(xtrain)))
```

Linear regression with only x1.

```{r}
fit <- lm(y~x1,data=dtrain)
t.gp1 <- dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
t.gp2 <- dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
grid.arrange(t.gp1,t.gp2,ncol=2)
```

Linear regression with 10 covariates.
```{r}
fm <- as.formula(paste0("y~",paste0("x",1:10,collapse="+")))
fit <- lm(fm,data=dtrain)
t.gp1 <- dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
t.gp2 <- dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
grid.arrange(t.gp1,t.gp2,ncol=2)
```

Linear regression with 19 covariates
```{r}
fm <- as.formula(paste0("y~",paste0("x",1:19,collapse="+")))
fit <- lm(fm,data=dtrain)
t.gp1 <- dtrain%>%
  ggplot(.,aes(x=x1))+
  geom_point(aes(y=y))+
  geom_line(aes(y=predict(fit)))
t.gp2 <- dtest%>%
  ggplot(aes(x=x1,y=y))+
  geom_point()+
  geom_line(aes(x=x1,y=predict(fit,newdata = dtest)),col="red")
grid.arrange(t.gp1,t.gp2,ncol=2)
```

We perform Leap Forward Regression.


```{r warning=FALSE}
tc <- trainControl(method = "cv", number = 10)
fit.forward <- train(y ~., data = dtrain,
                     method = "leapForward", 
                     tuneGrid=data.frame(nvmax=1:15),
                     trControl = tc,
                     trace = FALSE
)
# nvmax
fit.forward$bestTune

# accuracy
fit.forward$results

# summary of the model
summary(fit.forward$finalModel)

# final model coefficients
coef(fit.forward$finalModel,2)%>%head
```


We perform Ridge regession

```{r}
lambda.max <- 10
lambda.min <- 10^{-6}*lambda.max
lambda.grid <- 10^seq(log10(lambda.min),log10(lambda.max),length=100)
fit.ridge<-train(y ~., 
                 data = dtrain,
                 method = "glmnet",
                 tuneGrid = expand.grid(alpha = 0,lambda=lambda.grid),
                 trControl = tc
) 

# CV plot
plot(fit.ridge)
# Best lambda
fit.ridge$bestTune$lambda
# Model coefficients
coef(fit.ridge$finalModel,fit.ridge$bestTune$lambda)%>%head
# Make predictions
fit.ridge %>% predict(dtest)%>%head
```

We perform Lasso regression.
```{r}
lambda.max <- max(abs(t(xtrain)%*%ytrain))/nrow(xtrain)
lambda.min <- 10^{-6}*lambda.max
lambda.grid <- 10^seq(log10(lambda.min),log10(lambda.max),length=100)
fit.lasso<-train(y ~., 
                 data = dtrain,
                 method = "glmnet",
                 tuneGrid = expand.grid(alpha = 1,lambda=lambda.grid),
                 trControl = tc
) 

# CV plot
plot(fit.lasso)
# Best lambda
fit.lasso$bestTune$lambda
# Model coefficients
coef(fit.lasso$finalModel,fit.lasso$bestTune$lambda)%>%head
# Make predictions
fit.lasso %>% predict(dtest)%>%head
```

Comparison between the 3 approaches
```{r}
models <- list(forward= fit.forward,ridge = fit.ridge,lasso=fit.lasso)
resamples(models) %>% 
  summary( metric = "RMSE")
```

That is how lambda_max is calculated.
```{r}
# with scaling
mysd <- function(y) sqrt(sum((y-mean(y))^2)/length(y))
sx <- scale(xtrain,scale=apply(xtrain, 2, mysd))
sx <- as.matrix(sx, ncol=20, nrow=100)
sy <- as.vector(scale(ytrain, scale=mysd(ytrain)))
max(abs(colSums(sx*sy)))/nrow(xtrain)
fitglmnet <- glmnet(sx,sy)
max(fitglmnet$lambda)

# no scaling
fit.lasso <-glmnet(x=xtrain,y=ytrain,
                   alpha=1,
                   standardize=FALSE,
                   intercept=FALSE) 
max(fit.lasso$lambda)
max(abs(t(xtrain)%*%ytrain))/nrow(xtrain)
```



```{r, eval=FALSE, include=FALSE}
set.seed(1)
spam <-  read.table("http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/spam.data")
colnames(spam)[58] <- "spam"
spam$spam <- factor(spam$spam)
# b) Fit a random Forest (library(randomForest)) with the default settings. 

library(randomForest)
set.seed(123)
system.time(rf.spam <- randomForest(spam ~ ., data = spam)) # about 17 sec
plot(rf.spam)



# c) Plot the error rate vs. the number of fitted trees. How many trees are necessary? Refit the model with the chosen number of trees. How long does it take now?

plot(rf.spam)# Choose no more than 100 trees.
set.seed(123)
system.time(rf.spam <- randomForest(spam ~ ., data = spam, ntree = 100)) # about 3 sec
rf.spam

set.seed(123)
idx <- sample(1:nrow(spam), 2601)
dTrain <- spam[idx,]
dTest <- spam[-idx,]
rf.train <- randomForest(spam ~ ., data = dTrain, ntree = 100)
rf.train ## OOB error: 5.04%
pred.rf <- predict(rf.train, newdata = dTest)
mean(pred.rf != dTest$spam) ## Test error: 5.4%

```

