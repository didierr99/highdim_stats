# Multiple linear regression

```{r include=FALSE}
library(gridExtra)
library(knitr)
library(tidyverse)
library(caret)
```

In this chapter we will review multiple linear regression and in particular the Ordinary Least Squares (OLS) estimator. We will further investigate the challenges which appear in the high-dimensional setting where the number of covariates is large compared to the number of observations, i.e. $p>>n$.

## Remarks on notation

We will typically denote the covariates by the symbol $X$. If $X$ is a vector, its components can be accessed
by subscripts $X_j$. The response variable will be denoted by $Y$. We use uppercase letters such as $X$, $Y$ when referring to the generic aspects of a variable. Observed values are written in lowercase; hence the $i$th observed value of $X$ is written as $x_i$ (where $x_i$ is again a scalar or vector). Matrices are represented by bold uppercase letters; for example, a set of $N$ input p-vectors $x_i$, $i=1,\ldots,N$ would be represented by the $N\times p$ matrix $\bf{X}$. All vectors are assumed to be column vectors, the $i$th row of $\bf{X}$ is $x_i^T$.

## Ordinary Least Squares

Given a vector of inputs $X=(X_1,X_2,\ldots,X_p)$, in multiple regression we predict the output $Y$ via the linear model:

\[ \hat{Y}=\hat\beta_0+\sum_{j=1}^{p}X_j\hat\beta_j.\]

The term $\beta_0$ is the intercept. If we include the constant variable 1 in $X$, include $\hat\beta_0$ in the vector of coefficients $\hat\beta$, then we can write 

\[\hat{Y}=X^T\hat\beta.\]

How do we fit the linear model to a set of training data (i.e. how do we obtain the estimator $\hat \beta$)? We typically use *ordinary least squares* (OLS) where we pick the coefficient $\beta$ to minimize the residual sum of squares

\begin{align*}
\rm{RSS}(\beta)&=\sum_{i=1}^{N}(y_i-x_i^T\beta)^2\\
&=(\bf y - \bf X \beta)^T (\bf y - \bf X \beta)\\
&=\|\bf y - \bf X \beta\|^2_2.
\end{align*}

If the matrix $\bf X^T \bf X$ is nonsingular, then the solution is given by 

\[\hat\beta=(\bf X^T \bf X)^{-1}\bf X^T \bf y.\]

Thus, the prediction at the new input point $X_{\rm new}$ is

\begin{align*}
\hat{Y}&=\hat{f}(X_{\rm new})\\
&=X_{\rm new}^T\hat\beta\\
&=X_{\rm new}^T(\bf X^T \bf X)^{-1}\bf X^T \bf y.\\
\end{align*}

Figures \@ref(fig:olsgeom2) and \@ref(fig:olsgeom1) show two geometric representations of the OLS estimator. In Figure \@ref(fig:olsgeom2) the $N$ data points $(y_i,x_{i1},\ldots,x_{ip})$ randomly spread around a $p$-dimensional hyperplane in a $p+1$-dimensional space; the random spread only occurs parallel to the y-axis. Figure \@ref(fig:olsgeom1) shows a different representation where the vector $\bf y$ is a single point in the $N$-dimensional space ${\bf R}^N$; the fitted $\hat {\bf y}$ is the orthogonal projection onto the $p$-dimensional subspace of ${\bf R}^N$ spanned by the vectors ${\bf x}_1,\ldots,{\bf x}_p$.


```{r olsgeom2,echo=FALSE,out.width="80%",fig.cap="N data points spreading around the p-dimensional OLS hyperplane."}
knitr::include_graphics("ols_geom2.JPG")
```


```{r olsgeom1,echo=FALSE,out.width="80%",fig.cap="OLS fit as the orthogonal projection **y** onto subspace spanned by covariates."}
knitr::include_graphics("ols_geom1.JPG")
```

## Overfitting 

Overfitting refers to the phenomenon of modelling the noise rather than the signal. In case the true model is parsimonious (few covariates driving the response $Y$) and data on many covariates are available, it is likely that a linear combination of all covariates yields a higher likelihood than a combination of the few that are actually related to the response. As only the few covariates related to the response contain the signal, the model involving all covariates then cannot
but explain more than the signal alone: it also models the error. Hence, it overfits the data. 

We illustrate overfitting with a simulation exercise.
We simulate $n=10$ training data points and take $p=15$ and $X_{i1},\ldots,X_{ip}$ i.i.d. $N(0,1)$. We assume that the response depends only on the first covariate, i.e.  $Y_i=\beta_1 X_{i1}+\epsilon_i$, where $\beta_1=2$ and $\epsilon_i$ i.i.d. $N(0,0.5)$. 

```{r}
set.seed(1)
n <- 10
p <- 15
beta <- c(2,rep(0,p-1))

# simulate covariates
xtrain <- matrix(rnorm(n*p),n,p)
ytrain <- xtrain%*%beta+rnorm(n,sd=0.5)
dtrain <- data.frame(xtrain)
dtrain$y <- ytrain
```


We start by fitting a univariate linear regression model.

```{r}
fit1 <- lm(y~X1,data=dtrain)
```

We print the `summary` of the estimated model.

```{r}
summary(fit1)
```

Next, we plot the fitted values (red triangles) against $X_1$ and we add the reference line representing the “true” relationship. 

```{r, echo=FALSE}
t.d <-data.frame(x=xtrain[,1],y=ytrain,yhat=predict(fit1)) 
t.d%>%
  ggplot(.,aes(x=x))+
  geom_point(aes(y=y),size=2)+
  geom_point(aes(y=yhat),col="red",pch=2,size=3,stroke=1.5)+
  geom_abline(intercept=0,slope=2,col="blue",lty=2)+
  xlab("X1")+ylab("Y")
```

We re-fit the model including three "noise" covariates $X_2$, $X_3$ and $X_4$.

```{r}
fit4 <- lm(y~X1+X2+X3+X4,data=dtrain)
summary(fit4)
```

We note that the fitted values (red triangles) start to deviate from the truth (blue line).

```{r, echo=FALSE}
t.d <-data.frame(x=xtrain[,1],y=ytrain,yhat=predict(fit4)) 
t.d%>%
  ggplot(.,aes(x=x))+
  geom_point(aes(y=y),size=2)+
  geom_point(aes(y=yhat),col="red",pch=2,size=3,stroke=1.5)+
  geom_abline(intercept=0,slope=2,col="blue",lty=2)+
  xlab("X1")+ylab("Y")
```

Next, we fit a regression model with a total number of 8 covariates. 

```{r}
fit8 <- lm(y~X1+X2+X3+X4+X5+X6+X7+X8,data=dtrain)
```

The fitted values further deviate from the truth and at the same time they get closer to the actual data points (black dots). 

```{r, echo=FALSE}
t.d <-data.frame(x=xtrain[,1],y=ytrain,yhat=predict(fit8)) 
t.d%>%
  ggplot(.,aes(x=x))+
  geom_point(aes(y=y),size=2)+
  geom_point(aes(y=yhat),col="red",pch=2,size=3,stroke=1.5)+
  geom_abline(intercept=0,slope=2,col="blue",lty=2)+
  xlab("X1")+ylab("Y")
```

From the `summary` we observe that the regression coefficients are clearly *over-estimated* (note the true coefficients $\beta = (0,2, 0, \ldots, 0)$). The model "overfits" the data. 

```{r}
summary(fit8)
```

Finally we re-fit the model with all 15 covariates.

```{r}
# fit linear regression (all 15 covariates)
fit15 <- lm(y~.,data=dtrain)
```

Now the fitted values match perfectly the data. The model perfectly explains the response, including the noise.

```{r, echo=FALSE}
t.d <-data.frame(x=xtrain[,1],y=ytrain,yhat=predict(fit15)) 
t.d%>%
  ggplot(.,aes(x=x))+
  geom_point(aes(y=y),size=2)+
  geom_point(aes(y=yhat),col="red",pch=2,size=3,stroke=1.5)+
  geom_abline(intercept=0,slope=2,col="blue",lty=2)+
  xlab("X1")+ylab("Y")
```

We investigate the `summary`. The output indicates that something went wrong. We have a note saying that there are *no residual degrees of freedom*. Furthermore, many entries in the table of coefficients are not available and a message says that coefficients cannot be calculated because of *singularities*. 

```{r}
summary(fit15)
```

What has happened? In fact, the OLS estimator as introduced above is not well defined as the design matrix $\bf X$ is rank deficient (${\rm rank}({\bf X})=n< p$) and therefore the matrix ${\bf X}^T {\bf X}$ is singular (not invertible). We can check this by calculating the determinant.

```{r}
x <- model.matrix(fit15)
det(t(x)%*%x)
```

In this simulation exercise we illustrated that overfitting in high-dimensional settings becomes a real threat. In particular, if $p>n$ then it is possible to form a linear combination of the covariates that perfectly explains the response, including the noise. Further, we noted that large estimates of regression coefficients are an indication of overfitting. Finally, a useful rule of thumb is that the number of predictors $p$ should be less than $n/10$ in order to avoid overfitting. We refer to this as the "10:1 rule". 


## Collinearity

Recall that *collinearity* in regression analysis refers to the event of two (or multiple) covariates being strongly linearly related. The case of two (or multiple) covariates being perfectly linearly dependent is referred to as *super-collinearity*. For illustration we generate artificial data.
```{r }
set.seed(1315)
n <- 20
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- x1+rnorm(n,sd=0.25)
x4 <- x1
dat <- data.frame(x1,x2,x3,x4)
dat$y <- 2*dat$x1+rnorm(n)
```

The figures show pairs of covariates with no-, high- and super-collinearity.

```{r echo=FALSE, fig.height=4, fig.width=12}
gp1 <- dat%>%
  ggplot(.,aes(x=x1,y=x2))+
  geom_point()+
  ggtitle("no collinearity")

gp2 <- dat%>%
  ggplot(.,aes(x=x1,y=x3))+
  geom_point()+
  ggtitle("collinearity")

gp3 <- dat%>%
  ggplot(.,aes(x=x1,y=x4))+
  geom_point()+
  ggtitle("super collinearity")

grid.arrange(gp1,gp2,gp3,ncol=3)
```

An important implication of collinearity is that the estimated regression coefficients are less precise than if covariates were uncorrelated with one another. Intuitively this can be explained based on Figure \@ref(fig:collinearity) which shows the response Y and two highly correlated covariates x1 and x2. The depicted hyperplanes for the true ("modell") and estimated ("geschätzt") coefficients are deviating from each other. Further we see that the exact position of the estimated hyperplane is stable alongside of the "hedge" of data points (but unstable in the orthogonal direction).

```{r collinearity,echo=FALSE,out.width="80%",fig.cap="OLS regression in the context of 2 highly correlated covariates."}
knitr::include_graphics("collinearity.JPG")
```

We can illustrate this numerically by fitting the following two models to the simulated data.

```{r}
modela <- lm(y~x1+x2,data=dat)
modelb <- lm(y~x1+x3,data=dat)
```

We obtain the variances of the estimated regression coefficients using the function `vcov`. The variances are larger for model b where x1 and x3 are highly correlated compared to model a with uncorrelated covariates.

```{r}
# variance of model a
diag(vcov(modela))

#variance of model b
diag(vcov(modelb))
```

The chance of collinearity increases with a larger number covariates. Ultimately, in a high-dimensional setting where $p>n$ we always have super-collinearity: the rank of the design matrix cannot exceed $n$ which implies that the columns of $\bf X$ are linearly dependent. 

## Prediction error

So far we investigated the fitted model visually and by inspecting the `summary` of the regression analysis. Another important aspect is to assess how the fitted model generalizes beyond the "observed" training data. We do this by distinguishing between model fitting and model testing, that is fit the model on training data and we assess the prediction error on separate test data.

```{r traintest,echo=FALSE,out.width="80%",fig.cap="Training and Test data"}
knitr::include_graphics("traintest.JPG")
```

To illustrate this we simulate dummy test data.

```{r include=FALSE}
set.seed(2)
```


```{r}
# simulate test data
xtest <- matrix(rnorm(n*p),n,p)
ytest <- xtest%*%beta+rnorm(n,sd=0.5)
dtest <- data.frame(xtest)
dtest$y <- ytest
```

Next, we take the previously fitted models (`fit4` and `fit8`) and make predictions on the test data.

```{r warning=FALSE}
pred4 <- predict(fit4,newdata = dtest)
pred8 <- predict(fit8,newdata = dtest)
```

The following plot shows the test data points (in black) together with the predictions obtained from models `fit4` (in orange) and `fit8` (in green). 

```{r, echo=FALSE}
dtest$pred4 <- pred4
dtest$pred8 <- pred8
dtest%>%
  ggplot(.,aes(x=X1))+
  geom_point(aes(y=y),size=3,col="black")+
  geom_point(aes(y=pred4),col="orange",pch=3,size=2,stroke=1.5)+
  geom_point(aes(y=pred8),col="green",pch=4,size=2,stroke=1.5)+
  geom_abline(intercept=0,slope=2,col="blue",lty=2)+
  xlab("X1")+ylab("Y")+
  theme_bw()
```

The predictions from `fit8` (model with 8 covariates) lie way off from the truth! 
We quantify the prediction error with the root-mean-square error (RMSE).

$$ {\bf RMSE}= \sqrt{\frac{\sum_{i=1}^N(y_i-\hat y_i)^2}{N}}$$

The RMSE measures how far off we should expect the prediction of our model to be.
In our dummy data example we can calculate the prediction error using the function `RMSE`.

```{r}
RMSE(pred4,ytest)
RMSE(pred8,ytest)
```

The RMSE=`r round(RMSE(pred4,ytest),2)` for model `fit4` is close to the inherent error $\sigma=0.5$. The RMSE=`r round(RMSE(pred8,ytest),2)` for model `fit8` indicates that a predictions deviate on average `r round(RMSE(pred8,ytest),2)` units from the observed values.

