<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Multiple Linear Regression |  Prediction and Feature Assessment</title>
  <meta name="description" content="Script for Analysis of High-Dimensional Data" />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Multiple Linear Regression |  Prediction and Feature Assessment" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/logo2.jpg" />
  <meta property="og:description" content="Script for Analysis of High-Dimensional Data" />
  <meta name="github-repo" content="staedlern/highdim_stats" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Multiple Linear Regression |  Prediction and Feature Assessment" />
  
  <meta name="twitter:description" content="Script for Analysis of High-Dimensional Data" />
  <meta name="twitter:image" content="/logo2.jpg" />

<meta name="author" content="Nicolas StÃ¤dler" />


<meta name="date" content="2022-09-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="regularization.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Analysis of High-Dimensional Data</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#notation"><i class="fa fa-check"></i><b>2.1</b> Notation<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>2.2</b> Ordinary Least Squares<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#overfitting"><i class="fa fa-check"></i><b>2.3</b> Overfitting<span></span></a></li>
<li class="chapter" data-level="2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#generalization-error"><i class="fa fa-check"></i><b>2.4</b> Generalization Error<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>3</b> Regularization<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="regularization.html"><a href="regularization.html#model-selection"><i class="fa fa-check"></i><b>3.1</b> Model Selection<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="regularization.html"><a href="regularization.html#subset--and-stepwise-regression"><i class="fa fa-check"></i><b>3.2</b> Subset- and Stepwise Regression<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>3.3</b> Ridge Regression<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="regularization.html"><a href="regularization.html#choice-of-penalty-parameter"><i class="fa fa-check"></i><b>3.3.1</b> Choice of penalty parameter<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="regularization.html"><a href="regularization.html#shrinkage-property"><i class="fa fa-check"></i><b>3.3.2</b> Shrinkage property<span></span></a></li>
<li class="chapter" data-level="3.3.3" data-path="regularization.html"><a href="regularization.html#effective-degrees-of-freedom"><i class="fa fa-check"></i><b>3.3.3</b> Effective degrees of freedom<span></span></a></li>
<li class="chapter" data-level="3.3.4" data-path="regularization.html"><a href="regularization.html#bayesian-interpretation"><i class="fa fa-check"></i><b>3.3.4</b> Bayesian interpretation<span></span></a></li>
<li class="chapter" data-level="3.3.5" data-path="regularization.html"><a href="regularization.html#splines"><i class="fa fa-check"></i><b>3.3.5</b> Splines<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="regularization.html"><a href="regularization.html#lasso-regression"><i class="fa fa-check"></i><b>3.4</b> Lasso Regression<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="regularization.html"><a href="regularization.html#numerical-optimization-and-soft-thresholding"><i class="fa fa-check"></i><b>3.4.1</b> Numerical optimization and soft thresholding<span></span></a></li>
<li class="chapter" data-level="3.4.2" data-path="regularization.html"><a href="regularization.html#variable-selection"><i class="fa fa-check"></i><b>3.4.2</b> Variable selection<span></span></a></li>
<li class="chapter" data-level="3.4.3" data-path="regularization.html"><a href="regularization.html#elasticnet-regression"><i class="fa fa-check"></i><b>3.4.3</b> Elasticnet Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="regularization.html"><a href="regularization.html#diabetes-example"><i class="fa fa-check"></i><b>3.5</b> Diabetes example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.1</b> Logistic Regression<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#regularized-logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Regularized Logistic Regression<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="classification.html"><a href="classification.html#classification-trees-and-machine-learning"><i class="fa fa-check"></i><b>4.3</b> Classification Trees and Machine Learning<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>5</b> Survival Analysis<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="survival-analysis.html"><a href="survival-analysis.html#survival-endpoints-and-cox-regression"><i class="fa fa-check"></i><b>5.1</b> Survival Endpoints and Cox Regression<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="survival-analysis.html"><a href="survival-analysis.html#regularized-cox-regression"><i class="fa fa-check"></i><b>5.2</b> Regularized Cox Regression<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="survival-analysis.html"><a href="survival-analysis.html#brier-score"><i class="fa fa-check"></i><b>5.3</b> Brier Score<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="high-dimensional-feature-assessment.html"><a href="high-dimensional-feature-assessment.html"><i class="fa fa-check"></i><b>6</b> High-Dimensional Feature Assessment<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="high-dimensional-feature-assessment.html"><a href="high-dimensional-feature-assessment.html#gene-wise-two-sample-comparison"><i class="fa fa-check"></i><b>6.1</b> Gene-wise Two-sample Comparison<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="high-dimensional-feature-assessment.html"><a href="high-dimensional-feature-assessment.html#multiple-testing"><i class="fa fa-check"></i><b>6.2</b> Multiple Testing<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="high-dimensional-feature-assessment.html"><a href="high-dimensional-feature-assessment.html#p-value-adjustment"><i class="fa fa-check"></i><b>6.3</b> P-value Adjustment<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="high-dimensional-feature-assessment.html"><a href="high-dimensional-feature-assessment.html#volcano-plot"><i class="fa fa-check"></i><b>6.4</b> Volcano Plot<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="high-dimensional-feature-assessment.html"><a href="high-dimensional-feature-assessment.html#variance-shrinkage-and-empirical-bayes"><i class="fa fa-check"></i><b>6.5</b> Variance Shrinkage and Empirical Bayes<span></span></a></li>
</ul></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><p><img src="logo2.jpg" style="width:3.5in" /><br />
Prediction and Feature Assessment</p></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-linear-regression" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Multiple Linear Regression<a href="multiple-linear-regression.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter we will review multiple linear regression and in particular the Ordinary Least Squares (OLS) estimator. We will further investigate the challenges which appear in the high-dimensional setting where the number of covariates is large compared to the number of observations, i.e.Â <span class="math inline">\(p&gt;&gt;n\)</span>.</p>
<div id="notation" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Notation<a href="multiple-linear-regression.html#notation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will typically denote the covariates by the symbol <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> is a vector, its components can be accessed
by subscripts <span class="math inline">\(X_j\)</span>. The response variable will be denoted by <span class="math inline">\(Y\)</span>. We use uppercase letters such as <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> when referring to the generic aspects of a variable. Observed values are written in lowercase; hence the <span class="math inline">\(i\)</span>th observed value of <span class="math inline">\(X\)</span> is written as <span class="math inline">\(x_i\)</span> (where <span class="math inline">\(x_i\)</span> is again a scalar or vector). Matrices are represented by bold uppercase letters; for example, a set of <span class="math inline">\(n\)</span> input p-vectors <span class="math inline">\(x_i\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span> would be represented by the <span class="math inline">\(n\times p\)</span> matrix <span class="math inline">\(\bf{X}\)</span>. All vectors are assumed to be column vectors, the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\bf{X}\)</span> is <span class="math inline">\(x_i^T\)</span>.</p>
</div>
<div id="ordinary-least-squares" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Ordinary Least Squares<a href="multiple-linear-regression.html#ordinary-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given a vector of inputs <span class="math inline">\(X=(X_1,X_2,\ldots,X_p)\)</span>, in multiple regression we predict the output <span class="math inline">\(Y\)</span> via the linear model:</p>
<p><span class="math display">\[ \hat{Y}=\hat\beta_0+\sum_{j=1}^{p}X_j\hat\beta_j.\]</span></p>
<p>The term <span class="math inline">\(\beta_0\)</span> is the intercept. If we include the constant variable 1 in <span class="math inline">\(X\)</span>, include <span class="math inline">\(\hat\beta_0\)</span> in the vector of coefficients <span class="math inline">\(\hat\beta\)</span>, then we can write</p>
<p><span class="math display">\[\hat{Y}=X^T\hat\beta.\]</span></p>
<p>How do we fit the linear model to a set of training data (i.e.Â how do we obtain the estimator <span class="math inline">\(\hat \beta\)</span>)? We typically use <em>ordinary least squares</em> (OLS) where we pick the coefficient <span class="math inline">\(\beta\)</span> to minimize the residual sum of squares</p>
<p><span class="math display">\[\begin{eqnarray*}
\textrm{RSS}(\beta)&amp;=&amp;\sum_{i=1}^{n}(y_i-x_i^T\beta)^2\\
&amp;=&amp;(\textbf{y} - \textbf{X} \beta)^T (\textbf{y} - \textbf{X} \beta)\\
&amp;=&amp;\|\textbf{y} - \textbf{X} \beta\|^2_2.
\end{eqnarray*}\]</span></p>
<p>If the matrix <span class="math inline">\(\bf X^T \bf X\)</span> is nonsingular, then the solution is given by</p>
<p><span class="math display">\[\hat\beta=(\bf X^T \bf X)^{-1}\bf X^T \bf y.\]</span></p>
<p>Thus, the prediction at the new input point <span class="math inline">\(X_{\rm new}\)</span> is</p>
<p><span class="math display">\[\begin{align*}
\hat{Y}&amp;=\hat{f}(X_{\rm new})\\
&amp;=X_{\rm new}^T\hat\beta\\
&amp;=X_{\rm new}^T(\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T \textbf{y}.
\end{align*}\]</span></p>
<p>Figures <a href="multiple-linear-regression.html#fig:olsgeom2">2.1</a> and <a href="multiple-linear-regression.html#fig:olsgeom1">2.2</a> show two geometric representations of the OLS estimator. In Figure <a href="multiple-linear-regression.html#fig:olsgeom2">2.1</a> the <span class="math inline">\(n\)</span> data points <span class="math inline">\((y_i,x_{i1},\ldots,x_{ip})\)</span> randomly spread around a <span class="math inline">\(p\)</span>-dimensional hyperplane in a <span class="math inline">\(p+1\)</span>-dimensional space; the random spread only occurs parallel to the y-axis and the hyperplane is defined via <span class="math inline">\(\hat \beta\)</span>. Figure <a href="multiple-linear-regression.html#fig:olsgeom1">2.2</a> shows a different representation where the vector <span class="math inline">\(\bf y\)</span> is a single point in the <span class="math inline">\(n\)</span>-dimensional space <span class="math inline">\({\bf R}^n\)</span>; the fitted <span class="math inline">\(\hat {\bf y}\)</span> is the orthogonal projection onto the <span class="math inline">\(p\)</span>-dimensional subspace of <span class="math inline">\({\bf R}^n\)</span> spanned by the vectors <span class="math inline">\({\bf x}_1,\ldots,{\bf x}_p\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:olsgeom2"></span>
<img src="ols_geom2.JPG" alt="Data points spreading around the p-dimensional OLS hyperplane." width="80%" />
<p class="caption">
Figure 2.1: Data points spreading around the p-dimensional OLS hyperplane.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:olsgeom1"></span>
<img src="ols_geom1.JPG" alt="OLS fit $\hat{\textbf{y}}$ as the orthogonal projection of $\textbf{y}$ onto subspace spanned by covariates." width="80%" />
<p class="caption">
Figure 2.2: OLS fit <span class="math inline">\(\hat{\textbf{y}}\)</span> as the orthogonal projection of <span class="math inline">\(\textbf{y}\)</span> onto subspace spanned by covariates.
</p>
</div>
</div>
<div id="overfitting" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Overfitting<a href="multiple-linear-regression.html#overfitting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Overfitting refers to the phenomenon of modelling the noise rather than the signal. In case the true model is parsimonious (few covariates driving the response <span class="math inline">\(Y\)</span>) and data on many covariates are available, it is likely that a linear combination of all covariates yields a higher likelihood than a combination of the few that are actually related to the response. As only the few covariates related to the response contain the signal, the model involving all covariates then cannot
but explain more than the signal alone: it also models the error. Hence, it overfits the data.</p>
<p>We illustrate overfitting by generating artificial data.
We simulate <span class="math inline">\(n=10\)</span> training data points, take <span class="math inline">\(p=15\)</span> and <span class="math inline">\(X_{i1},\ldots,X_{ip}\)</span> i.i.d. <span class="math inline">\(N(0,1)\)</span>. We assume that the response depends only on the first covariate, i.e.Â <span class="math inline">\(Y_i=\beta_1 X_{i1}+\epsilon_i\)</span>, where <span class="math inline">\(\beta_1=2\)</span> and <span class="math inline">\(\epsilon_i\)</span> i.i.d. <span class="math inline">\(N(0,0.5^2)\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="multiple-linear-regression.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1-2"><a href="multiple-linear-regression.html#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb1-3"><a href="multiple-linear-regression.html#cb1-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb1-4"><a href="multiple-linear-regression.html#cb1-4" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="fu">rep</span>(<span class="dv">0</span>,p<span class="dv">-1</span>))</span>
<span id="cb1-5"><a href="multiple-linear-regression.html#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="multiple-linear-regression.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate covariates</span></span>
<span id="cb1-7"><a href="multiple-linear-regression.html#cb1-7" aria-hidden="true" tabindex="-1"></a>xtrain <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p),n,p)</span>
<span id="cb1-8"><a href="multiple-linear-regression.html#cb1-8" aria-hidden="true" tabindex="-1"></a>ytrain <span class="ot">&lt;-</span> xtrain<span class="sc">%*%</span>beta<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span><span class="fl">0.5</span>)</span>
<span id="cb1-9"><a href="multiple-linear-regression.html#cb1-9" aria-hidden="true" tabindex="-1"></a>dtrain <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(xtrain)</span>
<span id="cb1-10"><a href="multiple-linear-regression.html#cb1-10" aria-hidden="true" tabindex="-1"></a>dtrain<span class="sc">$</span>y <span class="ot">&lt;-</span> ytrain</span></code></pre></div>
<p>We fit a univariate linear regression model with X1 as covariate and print the <code>summary</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="multiple-linear-regression.html#cb2-1" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>X1,<span class="at">data=</span>dtrain)</span>
<span id="cb2-2"><a href="multiple-linear-regression.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ X1, data = dtrain)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.59574 -0.41567 -0.06222  0.18490  0.97592 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.1002     0.1785  -0.561     0.59    
## X1            1.8070     0.2373   7.614 6.22e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5558 on 8 degrees of freedom
## Multiple R-squared:  0.8787, Adjusted R-squared:  0.8636 
## F-statistic: 57.97 on 1 and 8 DF,  p-value: 6.223e-05</code></pre>
<p>The coefficient for X1 is close to the true value. The R squared value <span class="math inline">\(R^2=\)</span> 0.88 indicates that the model fits the data well. In order to explore what happens if we add noise covariates, we re-fit the model with an increasing number of covariates, i.e.Â <span class="math inline">\(p=4\)</span>, <span class="math inline">\(8\)</span> and <span class="math inline">\(15\)</span>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="multiple-linear-regression.html#cb4-1" aria-hidden="true" tabindex="-1"></a>fit4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>X1<span class="sc">+</span>X2<span class="sc">+</span>X3<span class="sc">+</span>X4,<span class="at">data=</span>dtrain)</span>
<span id="cb4-2"><a href="multiple-linear-regression.html#cb4-2" aria-hidden="true" tabindex="-1"></a>fit8 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>X1<span class="sc">+</span>X2<span class="sc">+</span>X3<span class="sc">+</span>X4<span class="sc">+</span>X5<span class="sc">+</span>X6<span class="sc">+</span>X7<span class="sc">+</span>X8,<span class="at">data=</span>dtrain)</span>
<span id="cb4-3"><a href="multiple-linear-regression.html#cb4-3" aria-hidden="true" tabindex="-1"></a>fit15 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>.,<span class="at">data=</span>dtrain) <span class="co">#all 15 covariates</span></span></code></pre></div>
<p>The next plot shows the data points (black circles) together with the fitted values (red crosses).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="highdimstats_files/figure-html/unnamed-chunk-8-1.png" alt="Observed and fitted values for models with increasing $p$." width="672" />
<p class="caption">
Figure 2.3: Observed and fitted values for models with increasing <span class="math inline">\(p\)</span>.
</p>
</div>
<p>With increasing <span class="math inline">\(p\)</span> the fitted values start to deviate from the true model (blue line) and they move closer towards the observed data points. Finally, with <span class="math inline">\(p=15\)</span>, the fitted values match perfectly the data, i.e.Â the model captures the noise and overfits the data. In line with these plots we note that the R squared values increase with <span class="math inline">\(p\)</span>.</p>
<table>
<caption><span id="tab:unnamed-chunk-9">Table 2.1: </span>R2 for models with increasing p.</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">R2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">p=1</td>
<td align="right">0.88</td>
</tr>
<tr class="even">
<td align="left">p=4</td>
<td align="right">0.90</td>
</tr>
<tr class="odd">
<td align="left">p=8</td>
<td align="right">0.98</td>
</tr>
<tr class="even">
<td align="left">p=15</td>
<td align="right">1.00</td>
</tr>
</tbody>
</table>
<p>The following figure shows the regression coefficients for the different models. The larger the <span class="math inline">\(p\)</span>, the bigger the discrepancy to the true coefficients.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="highdimstats_files/figure-html/unnamed-chunk-10-1.png" alt="Regression coefficients for models with increasing $p$." width="672" />
<p class="caption">
Figure 2.4: Regression coefficients for models with increasing <span class="math inline">\(p\)</span>.
</p>
</div>
<p>This becomes even more evident when calculating the mean squared error between the estimated and true coefficients.</p>
<table>
<caption><span id="tab:unnamed-chunk-11">Table 2.2: </span>MSE between estimated and true coefficients.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">mse</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">p=1</td>
<td align="right">0.02</td>
</tr>
<tr class="even">
<td align="left">p=4</td>
<td align="right">0.04</td>
</tr>
<tr class="odd">
<td align="left">p=8</td>
<td align="right">0.84</td>
</tr>
<tr class="even">
<td align="left">p=15</td>
<td align="right">1.63</td>
</tr>
</tbody>
</table>
<p>The <code>summary</code> of the full model with <span class="math inline">\(p=15\)</span> indicates that something went wrong.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="multiple-linear-regression.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit15)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = dtrain)
## 
## Residuals:
## ALL 10 residuals are 0: no residual degrees of freedom!
## 
## Coefficients: (6 not defined because of singularities)
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.01592        NaN     NaN      NaN
## X1          -0.50138        NaN     NaN      NaN
## X2           0.81492        NaN     NaN      NaN
## X3          -0.56052        NaN     NaN      NaN
## X4           0.72667        NaN     NaN      NaN
## X5           1.84831        NaN     NaN      NaN
## X6           0.05759        NaN     NaN      NaN
## X7          -1.21460        NaN     NaN      NaN
## X8          -1.30908        NaN     NaN      NaN
## X9          -1.39005        NaN     NaN      NaN
## X10               NA         NA      NA       NA
## X11               NA         NA      NA       NA
## X12               NA         NA      NA       NA
## X13               NA         NA      NA       NA
## X14               NA         NA      NA       NA
## X15               NA         NA      NA       NA
## 
## Residual standard error: NaN on 0 degrees of freedom
## Multiple R-squared:      1,  Adjusted R-squared:    NaN 
## F-statistic:   NaN on 9 and 0 DF,  p-value: NA</code></pre>
<p>There is a note saying âno residual degrees of freedomâ. Furthermore, many entries in the table of coefficients are not available and a note says that coefficients cannot be calculated because of <em>singularities</em>. What has happened? In fact, the OLS estimator as introduced above is not well defined. The design matrix <span class="math inline">\(\bf X\)</span> is rank deficient (<span class="math inline">\({\rm rank}({\bf X})=n&lt; p\)</span>) and therefore the matrix <span class="math inline">\({\bf X}^T {\bf X}\)</span> is singular (not invertible). We can check this by calculating the determinant.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="multiple-linear-regression.html#cb7-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(fit15)</span>
<span id="cb7-2"><a href="multiple-linear-regression.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">det</span>(<span class="fu">t</span>(x)<span class="sc">%*%</span>x)</span></code></pre></div>
<pre><code>## [1] -2.8449e-81</code></pre>
<p>In this simulation exercise we illustrated the problem of overfitting. We have seen that the models with large <span class="math inline">\(p\)</span> fit the data very well, but the estimated coefficients are far off from the truth. In practice we do not know the truth. How do we know when a model is overfitting and how do we decide what a âgoodâ model is? Shortly we will introduce the <a href="multiple-linear-regression.html#generalization-error">Generalization Error</a> which will shed light on this question.</p>
<!-- In particular, if $p>n$ then it is possible to form a linear combination of the covariates that perfectly explains the response, including the noise. Further, we noted that large estimates of regression coefficients are an indication of overfitting.  -->
<p>We end this section with the helpful <strong><em>10:1 rule:</em></strong></p>
<blockquote>
<p>In order to avoid overfitting the number of predictors (or covariates) <strong>p should be less than n/10</strong>. This rule can be extended to binary and time-to-event endpoints. For binary endpoints we replace <span class="math inline">\(n\)</span> with <span class="math inline">\(\min\{n_0,n_1\}\)</span> and for time-to-event with <span class="math inline">\(n_{\rm events}\)</span>.</p>
</blockquote>
</div>
<div id="generalization-error" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Generalization Error<a href="multiple-linear-regression.html#generalization-error" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The ultimative goal of a good model is to make good predictions for the future. That is we need to assess how the fitted model generalizes beyond the âobservedâ data. Conceptually, given new input data <span class="math inline">\(x_{\rm new}\)</span>, the model provides a prediction <span class="math inline">\(\hat{Y}=\hat{f}(x_{\rm new})\)</span>. The <em>Generalization Error</em> is the expected discrepancy between the prediction <span class="math inline">\(\hat{Y}=\hat{f}(x_{\rm new})\)</span> and the actual outcome <span class="math inline">\(Y_{\rm new}\)</span></p>
<p><span class="math display">\[{\rm Err}(x_{\rm new})=E[(Y_{\rm new}-\hat{f}(x_{\rm new}))^2].\]</span>
One can show that this error can be decomposed into three terms</p>
<p><span class="math display">\[\begin{eqnarray}
{\rm Err}(x_{\rm new})&amp;=&amp;\sigma_{\epsilon}^2+{\rm Bias}^2(\hat{f}(x_{\rm new})) + {\rm Var}(\hat{f}(x_{\rm new})),
\end{eqnarray}\]</span></p>
<p>where the first term is the irreducible error (or ânoiseâ), the second term describes the systematic bias from the truth and the third term is the variance of the predictive model. For linear regression the expected variance can be approximated by <span class="math inline">\(\sigma^2_{\epsilon} \frac{p}{N}\)</span>. Complex models (with large number of covariates) have typically a small bias but a large variance. Therefore the equation above is referred to as the bias-variance dilemma as it describes the conflict in trying simultaneously minimize both sources of error, bias and variance.<br />
<!-- the =(E[\hat{f}(x_{\rm new})]-f(x_{\rm new})\right)$ and ${\bf Var}(\hat{f}(x_{\rm new}))=E\left[(E[\hat{f}(x_{\rm new})]-\hat{f}(x_{\rm new}))^2\right]$. The first term represent the *irreducible error* (or noise), the second term is the  --></p>
<p>How do we calculate the Generalization Error in practice? The most simple approach is to separate the data into a training and testing set (see Figure <a href="multiple-linear-regression.html#fig:traintest">2.5</a>). The model is fitted (or âtrainedâ) on the training data and the Generalization Error is calculated on the test data and quantified using the root-mean-square error (RMSE)</p>
<p><span class="math display">\[ {\rm RMSE}= \sqrt{\frac{\sum_{i=1}^{n_{\rm test}}(y_{\rm test,i}-\hat y_{i})^2}{n_{\rm test}}}.\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:traintest"></span>
<img src="traintest.JPG" alt="Splitting the data into training set and test sets." width="80%" />
<p class="caption">
Figure 2.5: Splitting the data into training set and test sets.
</p>
</div>
<p>We illustrate this based on the dummy data. First we simulate test data.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="multiple-linear-regression.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate test data</span></span>
<span id="cb9-2"><a href="multiple-linear-regression.html#cb9-2" aria-hidden="true" tabindex="-1"></a>xtest <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p),n,p)</span>
<span id="cb9-3"><a href="multiple-linear-regression.html#cb9-3" aria-hidden="true" tabindex="-1"></a>ytest <span class="ot">&lt;-</span> xtest<span class="sc">%*%</span>beta<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span><span class="fl">0.5</span>)</span>
<span id="cb9-4"><a href="multiple-linear-regression.html#cb9-4" aria-hidden="true" tabindex="-1"></a>dtest <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(xtest)</span>
<span id="cb9-5"><a href="multiple-linear-regression.html#cb9-5" aria-hidden="true" tabindex="-1"></a>dtest<span class="sc">$</span>y <span class="ot">&lt;-</span> ytest</span></code></pre></div>
<p>Next, we take the fitted models and make predictions on the test data</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="multiple-linear-regression.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prediction</span></span>
<span id="cb10-2"><a href="multiple-linear-regression.html#cb10-2" aria-hidden="true" tabindex="-1"></a>pred1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit1,<span class="at">newdata =</span> dtest)</span>
<span id="cb10-3"><a href="multiple-linear-regression.html#cb10-3" aria-hidden="true" tabindex="-1"></a>pred4 <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit4,<span class="at">newdata =</span> dtest)</span>
<span id="cb10-4"><a href="multiple-linear-regression.html#cb10-4" aria-hidden="true" tabindex="-1"></a>pred8 <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit8,<span class="at">newdata =</span> dtest)</span>
<span id="cb10-5"><a href="multiple-linear-regression.html#cb10-5" aria-hidden="true" tabindex="-1"></a>pred15 <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit15,<span class="at">newdata =</span> dtest)</span></code></pre></div>
<!-- The following plot shows the test data points (in black) together with the predictions obtained from models `fit4` (in orange) and `fit8` (in green). -->
<!-- ```{r, echo=FALSE} -->
<!-- dtest$pred4 <- pred4 -->
<!-- dtest$pred8 <- pred8 -->
<!-- dtest%>% -->
<!--   ggplot(.,aes(x=X1))+ -->
<!--   geom_point(aes(y=y),size=3,col="black")+ -->
<!--   geom_point(aes(y=pred4),col="orange",pch=3,size=2,stroke=1.5)+ -->
<!--   geom_point(aes(y=pred8),col="green",pch=4,size=2,stroke=1.5)+ -->
<!--   geom_abline(intercept=0,slope=2,col="blue",lty=2)+ -->
<!--   xlab("X1")+ylab("Y")+ -->
<!--   theme_bw() -->
<!-- ``` -->
<!-- The predictions from `fit8` (model with 8 covariates) lie way off from the truth!  -->
<!-- We quantify the prediction error with the root-mean-square error (RMSE). -->
<!-- $$ {\bf RMSE}= \sqrt{\frac{\sum_{i=1}^N(y_i-\hat y_i)^2}{N}}$$ -->
<!-- The RMSE measures how far off we should expect the prediction of our model to be. -->
<!-- In our dummy data example we can calculate the prediction error using the function `RMSE`. -->
<!-- ```{r} -->
<!-- RMSE(pred4,ytest) -->
<!-- RMSE(pred8,ytest) -->
<!-- ``` -->
<p>and we calculate the RMSE.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="multiple-linear-regression.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># rmse</span></span>
<span id="cb11-2"><a href="multiple-linear-regression.html#cb11-2" aria-hidden="true" tabindex="-1"></a>rmse <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb11-3"><a href="multiple-linear-regression.html#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">RMSE</span>(pred1,ytest),<span class="fu">RMSE</span>(pred4,ytest),</span>
<span id="cb11-4"><a href="multiple-linear-regression.html#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">RMSE</span>(pred8,ytest),<span class="fu">RMSE</span>(pred15,ytest)</span>
<span id="cb11-5"><a href="multiple-linear-regression.html#cb11-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-6"><a href="multiple-linear-regression.html#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(rmse) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;p=&quot;</span>,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">15</span>))</span>
<span id="cb11-7"><a href="multiple-linear-regression.html#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(rmse) <span class="ot">&lt;-</span> <span class="st">&quot;RMSE&quot;</span></span>
<span id="cb11-8"><a href="multiple-linear-regression.html#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(rmse,<span class="at">digits=</span><span class="dv">2</span>,<span class="at">booktabs=</span><span class="cn">TRUE</span>,</span>
<span id="cb11-9"><a href="multiple-linear-regression.html#cb11-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">caption=</span><span class="st">&quot;RMSE for models with increasing p.&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-17">Table 2.3: </span>RMSE for models with increasing p.</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">p=1</th>
<th align="right">p=4</th>
<th align="right">p=8</th>
<th align="right">p=15</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">RMSE</td>
<td align="right">0.54</td>
<td align="right">0.63</td>
<td align="right">3.28</td>
<td align="right">3.7</td>
</tr>
</tbody>
</table>
<p>The models with <span class="math inline">\(p=1\)</span> and <span class="math inline">\(4\)</span> achieve a good error close to the âirreducibleâ <span class="math inline">\(\sigma_{\epsilon}\)</span>. On the other hand the predictions obtained with <span class="math inline">\(p=8\)</span> and <span class="math inline">\(15\)</span> are very poor (RMSEs are 6 to 8-fold larger).</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/staedlern/highdim_stats/edit/main/01-multregr.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["highdimstats.pdf", "highdimstats.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
