--- 
title: "High-dimensional statistics"
author: "Nicolas StÃ¤dler"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: staedlern/highdim_stats
description: "This book is for the CAS Module High-dimensional statistics"
---

# Prerequisites

This book is thought as accompanying material for the course on *High-dimensional statistics*. 
Much of the content is based on the book from @elements. The book has a focus on applications using **R** [@R-base].

What is high-dimensional statistics? The Statistics Department of the University of California, Berkeley summarizes it as follows:

*High-dimensional statistics focuses on data sets in which the number of features is of comparable size, or larger than the    number of observations.  Data sets of this type present a variety of new challenges, since classical theory and methodology can break down in surprising and unexpected ways.*

High-dimensional statistics is often paraphrased with the expression "$p>>n$" referring to a linear regression setting where the number of features $p$ is much larger than the sample size $n$. However, challenges due to high-dimensionality are already present when $p$ is comparable to $n$ and often the model complexity manifests itself in a more subtle manner.

High-dimensional data are omnipresent and the approaches which we will discuss have applications in many disciplines. In the lecture we will see examples in molecular biology, health care, speech recognition and financial time series. 

For this course we distinguish between two "tasks" of high-dimension statistics. The first task is *Prediction & Feature Selection* where we have many explanatory variables (features) $X_1,\ldots,X_p$ and one response variable $Y$. The goal is to predict the response based on features and to identify the features which contribute most to prediction. The topics which we will discuss for this first task are: multiple linear regression; challenges in high-dimensions due overfitting, collinearity and numerical singularity; prediction error and cross-validation; Subset-, Ridge- and Lasso-regression; Classification and logistic regression; and Glmnet and Boosting. The second task we refer to as *Feature Assessment*. Here the starting point is one (or few) explanatory variable $X$ and many response variables $Y_1, \ldots, Y_p$. $X$ often defines two or more groups (e.g. treatment groups or groups with specific characteristics) and the aim is to find variables which differ with respect to $X$. The topics which we will cover here are: hypothesis testing; multiple testing problem; Bonferroni inequality; P-value adjustment and variance shrinkage.

```{r twotasks,echo=FALSE,out.width="90%",fig.cap="Two tasks in high-dimensional statistics. Left figure (Prediction and Feature selection). The task is to predict the response based on many explanatory variables and to select the most important variables. Right figure (Feature assessment). The task is to identify features which differ with respect to $X$ (e.g. treatment groups A vs B)."}
knitr::include_graphics("twotasks.JPG")
```




```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warnings=FALSE)
```

```{r include=FALSE}
library(knitr)
library(tidyverse)
```

## Remarks on notation

We will typically denote an input variable by the symbol $X$. If $X$ is a vector, its components can be accessed
by subscripts $X_j$. Quantative outputs will be denoted by $Y$, and qualitative outputs by $G$ (for group). We use uppercase letters such as $X$, $Y$ or $G$ when referring to the generic aspects of a variable. Observed values are written in lowercase; hence $i$th observed value of $X$ is written as $x_i$ (where $x_i$ is again a scalar or vector). Matrices are represented by bold uppercase letters; for example, a set of $N$ input p-vectors $x_i$, $i=1,\ldots,N$ would be represented by the $N\times p$ matrix $\bf{X}$. All vectors are assumed to be column vectors, the $i$th row of $\bf{X}$ is $x_i^T$.

