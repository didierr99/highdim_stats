# Contrainted Least Squares

```{r, include=FALSE}
library(knitr)
library(caret)
library(glmnet)
library(tidyverse)
library(MASS)
library(lars)
library(gridExtra)
library(stats)
library(splines)
```

We have seen that OLS falls short in the high-dimensional context. It leads to overfitting and as a result in large estimates of regression coefficients. Augmentation of the estimation
procedure with a constraint on the regression coefficients is a simple remedy to large parameter estimates. As a consequence it decreases the probability of overfitting. In the following we will discuss methods which minimize $\rm{RSS}(\beta)$ under some contstraints on the parameter $\beta$.


## Subset selection

The most traditional approach to impose constraints is subset selection. In this approach we retain only a subset of the variables, and eliminate the rest from the model. OLS is used to estimate the coefficients of the inputs that are retained. More formally, given a subset $S\subset\{1,\ldots,p\}$ we solve the optimization problem


\begin{align*}
\hat{\beta}_{S}&=\rm{argmin}_{\beta_j=0\;\rm{for\; all}\;j\notin S}\rm{RSS}(\beta)\\
&=(\bf{X}_S^T \bf{X}_S)^{-1}\bf{X}_S^T \bf{y}.
\end{align*}

In practice we need to explore a sequence of subsets $S_1,\ldots,S_K$ and choose the optimal subset by either a re-sampling approach or by using an information criteria. There are a number of different strategies for choosing the sequence of subsets. Best subsets regression consists of looking at possible combinations of covariates. Rather than search though all possible subsets, we can seek a good path through them. Two popular approaches are backward stepwise selection which starts with the full model and sequentially deletes covariates, whereas forward stepwise selection starts with the intercept, and then sequentially adds into the model the covariate that most improves the fit. 

In R we can use stepAIC for subset selection. It uses AIC to select the optimal subset. For example to perform forward stepwise selection we proceed

```{r}
# Forward regression
fit0 <- lm(y~1,data=dtrain)
fit.fw <- stepAIC(fit0,direction="forward",
                  scope=list(lower=fit0,upper=paste("~", paste(colnames(dtrain[,-10]), collapse=" + "))
                             ),
                  trace = FALSE
                  )
```

The selection process can be summarized

```{r}
kable(as.data.frame(fit.fw$anova),digits=3)
```

The regression coefficients of the optimal model are

```{r}
kable(broom::tidy(fit.fw),digits=3)
```

## Ridge Regression

Subset selection and forward regression outlined above all work by either including or
excluding covariates, i.e. contrain specific regression coefficients to be zero.
An alternative is Ridge Regression, which regularises the problem by shrinking
regression coefficients towards zero. This discourages complex models because
models that overfit tend to have larger coefficients. We can formulate Ridge Regression as the constrained optimization problem

\begin{align*}
\hat{\beta}^{\rm Ridge}_{c}&=\rm{argmin}_{\|\beta\|^2\leq c}\rm{RSS}(\beta).
\end{align*}

The constrained estimation interpretation of the ridge regression estimator is illustrated in the figure. It shows the levels sets of the $\rm{RSS}(\beta)$ which are ellipsoids and centered around zero the circular ridge
parameter constraint parametrized by some $c > 0$. The ridge regression estimator
is then the point where the smallest level set hits the constraint. Exactly at that point the $\rm{RSS}(\beta)$ is
minimized over those $\beta$â€™s that live inside the constraint.

![](ridge_geometry.JPG)

Alternatively Ridge Regression can be cast as 
minimisation of the penalised residual sum of squares, with
a penalty on the magnitude of the coefficients

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=\rm{argmin}\; \rm{RSS}(\beta)+\lambda\|\beta\|^2.
\end{align*}

Both formulation are equivalent (one-to-one relationship between $c$ and $\lambda$). We will use more often the second "penalisation" formulation. The parameter $\lambda$ is the amount of penalisation. Note that for "no penalization", $\lambda=0$, the Ridge Regression coincides with OLS. Increasing $\lambda$ has the effect of shinking the regression coefficients to zero. 

```{r}
fit.ridge.glmnet <-glmnet(x=xtrain,y=ytrain,alpha=0) 
plot(fit.ridge.glmnet,xvar="lambda",label=TRUE)
```

The Ridge optimization problem has the closed form solution

\begin{align*}
\hat{\beta}^{\rm Ridge}_{\lambda}&=(\bf X^T \bf X+\lambda \bf I)^{-1}\bf X^T \bf y,
\end{align*}

which can be easily verified by taking the derivative of the loss function $\ell_{\rm Ridge}(\beta|\bf Y,\bf X)=\rm{RSS}(\beta)+\lambda\|\beta\|^2$ and equating it to zero. Note that for $\lambda>0$ the matrix $\bf X^T \bf X+\lambda \bf I$ has always full rank and therefore the Ridge Regression is well defined in the high-dimensional context (in contrast to OLS).


### Choice of penalty parameter

We proceed as explained in the section on statistical learning. We define a grid 
of penalty parameters $0<\lambda_1<\lambda_2<\ldots<\lambda_K<\infty$ and choose the optimal $\lambda_{\rm opt}$ by either a re-sampling approach or by using an information criteria. In glmnet we can use cross-validation

```{r}
cv.ridge.glmnet <-cv.glmnet(x=xtrain,y=ytrain,alpha=0) 
plot(cv.ridge.glmnet)
cv.ridge.glmnet$lambda.min
```

Although Ridge Regression involves all $p$ covariate the degrees of freedom $\rm{DF}$ is smaller then $p$ as we imposed constraints. One can show that the degrees of freedom for Ridge Regression equals 

\[\rm DF^{\rm Ridge}(\lambda)=\sum_{j=1}^{p}\frac{d_j^2}{d_j^2+\lambda},\]
where $d_1,\ldots,d_p$ are the singular values of $\bf X$.

```{r, echo=FALSE}
# get singular values
fit.svd <- svd(xtrain) #fit.svd$d

# ridge degree of freedom for lambdaopt
df_lambdaopt <- sum(fit.svd$d^2/(fit.svd$d^2+cv.ridge.glmnet$lambda.1se))
```

In our example the residual degrees of freedom is $n-\rm{DF}^{\rm Ridge}(\lambda_{\rm opt})$=`r round(nrow(xtrain)-df_lambdaopt,2)` which lies within the "rule of thumb" of minimum of $5-10$ residual degrees of freedom to avoid overfitting.

### Shrinkage property

We noted that another challenge in multivariate regression is collinearity. In particular, we have seen that OLS becomes unstable with highly correlated covariates. Ridge regression counter acts this by shrinking low-variance components more than high-variance components. 

We note that ${\bf \hat y}^{OLS}$ can be viewed has $\bf y$ orthogonally projected onto the hyperplane spanned the column space of $\bf X$. In term of principle components we get

\[{\bf\hat y}^{OLS}=\sum_{j=1}^{p}u_j u_j^T y.\]

Similar we can represent the Ridge projection as

\[{\bf\hat y}^{Ridge}=\sum_{j=1}^{p}u_j \frac{d_j^2}{d_j^2+\lambda}u_j^T y.\]

This shows that the largest shrinkage is in the direction of the last principle component which in return is the component with largest variance. This is illustrated in Figure XXX.

Although Ridge Regression works in high-dimensions, $p>>n$, a disadvantage is that it does not perform variable selection and therefore interpretation of the model is more challenging. 

### Bayesian interpretation

Ridge Regression beviewed as the maximum a posetrior (MAP) estimate of hierarchical bayesian model: 
\[Y_i|X_i,\beta\sim N(X_i^T\beta,\sigma^2),\; i=1,\ldots,n\]
with prior distribution on the regression coefficients
\[\beta_j \sim N(0,\tau^2),\; j=1,\ldots,p\]

### Towards non-parametric regression

Some times it is extremely unlikely that the true function $f(X)$ is actually linear in $X$. Consider the following example

```{r}
# define function
n <- 101
x <- seq(0, 1, length.out = n)
fx <- sin(2 * pi * x)

# generate noisy data
set.seed(1)
y <- fx + rnorm(n, sd = 0.5)

plot(x, y)             # data
lines(x, fx, lwd = 2)  # f(x)
legend("topright", legend = "f(x)", lty = 1, lwd = 2, bty = "n")
```

One approach to find a non-parametric approximation are so-called smoothing splines. The approach starts by approximating $f(x)$ as natural cubic spline with knots at the unique values of $x_i$, $i=1,\ldots,n$

\[f(x)\approx\sum_{j=1}^{n}N_j(x)\beta_j,\]

where $N_j(x)$'s are an n-dimensional set of basis functions for representing this family of natural splines. This is an over-parameterized problem as the number of basis function ("covariates") equals the number of observations. Smoothing splines impose generalized Ridge penalty on the spline coefficients $\beta_j$, i.e.

\[\hat{\beta}_{\lambda}=\rm{argmin}\;\|\bf Y- \bf N \beta\|^2+\lambda \beta^T\Omega\beta,\]

with design matrix $\bf N$ has $jth$ column $(N_j(x_1),\ldots,N_j(x_n))^T$. In practice we can fit 
smoothing spline using the function smooth.spline. The penalty term is specified by setting the degree of freedom or by cross-validation.

```{r, eval=FALSE,include=FALSE}
#https://www.hse.ru/data/2018/03/15/1164357459/5._Splines.html
#https://cswr.nrhstat.org/3-5-splines.html
# https://lbelzile.github.io/lineaRmodels/splines.html
#https://bobby.gramacy.com/surrogates/splines.html


my.knots <- sort(unique(x))[-c(1,101)]
bs_mat <- ns(x,knots=my.knots)
bmat2 <- splineDesign(knots=my.knots,x=x,outer.ok = TRUE)
fit.smsp <- smooth.spline(x, y, all.knots = TRUE,keep.stuff = TRUE)
fit.smsp$auxM

x <- seq(0, 1, by=0.001)
spl <- ns(x,df=6)
plot(spl[,1]~x, ylim=c(min(spl),max(spl)), type='l', lwd=2, col=1, 
     xlab="Cubic B-spline basis", ylab="")
for (j in 2:ncol(spl)) lines(spl[,j]~x, lwd=2, col=j)
```


```{r}
fit.smsp.df10 <- smooth.spline(x, y, df = 10)
fit.smsp.df30 <- smooth.spline(x, y, df = 30)
fit.smsp.cv <- smooth.spline(x, y)
plot(x, y)
lines(x, fx, lwd = 2)
lines(x, fit.smsp.df10$y, lty = 2, col = "blue",lwd=3)
lines(x, fit.smsp.df30$y, lty = 2, col = "green",lwd=3)
lines(x, fit.smsp.cv$y, lty = 3, col="red",lwd=3)
```
A smoothing spline with 30 degrees of freedom (in green) leads to overfitting. The spline with 10 degrees of freedom (in blue) is a good approximation of the truth and is close the spline obtained by cross-validation (in red). The degree of freedom of the cross-validation solution is

```{r}
fit.smsp.cv$df
```


## The Lasso

In Ridge Regression we minimize the $\rm RSS(\beta)$ given constrains on the L2-norm of the regression coefficients

\[\|\beta\|^2_2=\sum_{j=1}^p \beta^2_j \leq c.\] 

An other very popular approach in high-dimensional statistics is the Lasso. The Lasso works very similar. The only difference is that constraints are imposed on the so-called L1-norm of the coefficients

\[\|\beta\|_1=\sum_{j=1}^p |\beta_j| \leq c.\]

The change in the form of the constraints has important implications. The following figure illustrates the geometry of the Lasso optimization.

![](lasso_geometry.JPG)

Geometrically the Lasso contraint is a diamond with "corners" (Ridge contraint is a circle). If the sum of squares "hits'' one of these corners, then the coefficient corresponding to the axis is shrunk to zero. As $p$ increases, the multidimensional diamond has an increasing number of corners, and so it is highly likely that some coefficients will be set equal to zero. Hence, the Lasso performs not only shrinkage but it also sets some coefficients to zero and therefore it simultaneously performs variable selection. A disadvantage of the "diamond" geometry is that there is no closed form solution (Lasso optimisation problem is not differentiable at the corners of the diamond).

Similar to Ridge the Lasso can be formulated as a penalisation problem

\begin{align*}
\hat{\beta}^{\rm Lasso}_{\lambda}&=\rm{argmin}\;\rm{RSS}(\beta)+\lambda\|\beta\|_1.
\end{align*}

The following Figure shows the Lasso solution for a grid of lambda values. We see that the Lasso shrinks some coefficients to exactly zero.

```{r}
fit.lasso.glmnet <-glmnet(x=xtrain,y=ytrain,alpha=1) 
plot(fit.lasso.glmnet,xvar="lambda",label=TRUE)
```

To identify an optimal model we define a grid 
of penalty parameters $0<\lambda_1<\lambda_2<\ldots<\lambda_K<\infty$ and choose the optimal $\lambda_{\rm opt}$ by cross-validation.

```{r}
cv.lasso.glmnet <-cv.glmnet(x=xtrain,y=ytrain,alpha=1) 
plot(cv.lasso.glmnet)
cv.lasso.glmnet$lambda.min
```

The optimal Lasso regression coefficient can be optained as

```{r}
beta.lasso <- coef(fit.lasso.glmnet, s = cv.lasso.glmnet$lambda.min)
names(beta.lasso) <- colnames(xtrain)
beta.lasso
```

and we can identify the selected variables

$S^{\rm Lasso}_{\lambda}=\{j\in (1,\ldots,p); \hat\beta^{\rm Lasso}_{\lambda,j}\neq 0\}$

```{r}
Shat <- rownames(beta.lasso)[which(beta.lasso != 0)]
Shat
```

We discuss now some important properties of the Lasso.

### Numerical Optimization and Soft Thresholding

We mentioned in the beginning that in general the Lasso optimization problem has not a closed from solution. The reason is that the absolute value funtion $|\beta_j|$ is not differential at zero.

In the case of orthonormal design matrix $\bf X$, ie. $\bf X^T\bf X=\bf I$, we have

\begin{align*}
\rm{RSS}(\beta)&=({\bf y}-{\bf X}\beta)^T({\bf y}-{\bf X}\beta)\\
&=\sum_{j=1}^p(\beta_j-\hat\beta_j^{\rm OLS})^2
\end{align*}

and therefore the Lasso optimization reduces to $j=1,\ldots,p$ univariate problems

\[\rm{minimize}\; \left(\hat\beta_j^{\rm OLS}-\beta_j\right)^2+\lambda |\beta_j|.\]

In the exercises we will show that the solution is

\[\hat \beta_{\lambda,j}^{\rm Lasso}=\rm{sign}(\hat\beta_j^{\rm OLS})\left(|\hat\beta_j^{\rm OLS}|-0.5\lambda\right)_{+}.\]

This function is depicted in the next figure. It is also referred to as the soft-thresholing function.

```{r, echo=FALSE}
softthreshold <- function(x,lambda=1){
  sign(x)*pmax(abs(x)-0.5*lambda,0)
}
curve(softthreshold,xlim=c(-2,2),xlab="beta ols",ylab="beta lasso")
```

In general there is no closed-form solution for the Lasso. The optimization has to be performed numerically. An very efficient algorithm is implemented in glmnet and is referred to as Pathwise Coordinate Optimization. It is based on coordinate-wise optimization, where one regression coefficient is updated at a time using the soft-thresholding function. This is done iteratively until some convergence criteria is met.

### Variable Selection
The Lasso does not only shrink coefficients to zero but also performs variable selection and therefore 
leads to more interpretabel models. For the Lasso we can define the selecte variables

$$S^{\rm Lasso}_{\lambda}=\{j\in (1,\ldots,p); \hat\beta^{\rm Lasso}_{\lambda,j}\neq 0\}$$

An interesting question is whether the Lasso does a good job in variable selection. In particular, does $S^{\rm Lasso}_{\lambda}$ agree with the true set of active variables $S_0$, or, does the Lasso typically under or over selects covariates? These questions were an active field of mathematical statistics. 

### Elastic Net

The Lasso penalty
is somewhat indifferent to the choice among a set of strong but corre-
lated variables. The Ridge penalty, on the other hand, tends
to shrink the coefficients of correlated variables toward each other. The elastic net penalty is a
compromise, and has the form 

\[\lambda \big(\alpha \|\beta\|_2^2+(1-\alpha)\|\beta\|_1\big)\]

The second term encourages highly correlated features to be averaged, while
the first term encourages a sparse solution in the coefficients of these averaged features.

In glmnet the elastic net penality is implemented via the parameter $\alpha$.

## Diabetes Example

These data consist of
observations on 442 patients, with the response of interest being a quantitative
measure of disease progression one year after baseline. There are ten baseline
variablesâ€”age, sex, body-mass index, average blood pressure, and six blood
serum measurementsâ€”plus quadratic terms, giving a total of 64 features.The statisticians were asked to construct a model that predicted response y from covariates
x1, x2,... ,x10. Two hopes were evident here, that the model would produce accurate baseline
predictions of response for future patients, and also that the form of the model would suggest
which covariates were important factors in disease progression.

We with splitting into train - and test data.

```{r}
library(lars)
data("diabetes")
data <- as.data.frame(cbind(y=diabetes$y,diabetes$x2))
colnames(data) <- gsub(":",".",colnames(data))
train_ind <- sample(seq(nrow(data)),size=nrow(data)/2)
data_train <- data[train_ind,]
xtrain <- as.matrix(data_train[,-1])
ytrain <- data_train[,1]
data_test <- data[-train_ind,]
xtest <- as.matrix(data_test[,-1])
ytest <- data_test[,1]
```


We perform Forward Regression.
```{r}
# Forward regression
fit0 <- lm(y~1,data=data_train)
fit.fw <- stepAIC(fit0,direction="forward",
                  scope=list(lower=fit0,upper=paste("~", paste(colnames(data_train[,-1]), collapse=" + "))
                             ),
                  trace = FALSE
                  )
#summary(fit.fw)
```

The variable selection process is as follows

```{r}
kable(as.data.frame(fit.fw$anova),digits=2)
```

And the final regression coefficients are

```{r}
kable(broom::tidy(fit.fw),digits=2)
```

We continue with Ridge Regression

```{r}
# Ridge
set.seed(1515)
fit.ridge <- glmnet(xtrain,ytrain,alpha=0)
fit.ridge.cv <- cv.glmnet(xtrain,ytrain,alpha=0)
plot(fit.ridge,xvar="lambda")
plot(fit.ridge.cv)
#fit.ridge.cv$lambda.1se
```

Finally we run the Lasso

```{r}
# Lasso
set.seed(1515)
fit.lasso <- glmnet(xtrain,ytrain,alpha=1)
fit.lasso.cv <- cv.glmnet(xtrain,ytrain,alpha=1)
plot(fit.lasso,xvar="lambda")
plot(fit.lasso.cv)
#fit.lasso.cv$lambda.1se
```

We compare the 3 approaches based on their root-mean-square error (RMSE)

```{r}
# RMSE
pred.fw <- predict(fit.fw,newdata=data_test)
pred.ridge <- as.vector(predict(fit.ridge,newx=xtest,s=fit.ridge.cv$lambda.1se))
pred.lasso <- as.vector(predict(fit.lasso,newx=xtest,s=fit.lasso.cv$lambda.1se))
res.rmse <- data.frame(rmse=c(RMSE(pred.fw,ytest),RMSE(pred.ridge,ytest),RMSE(pred.lasso,ytest)),
                       method=c("fw","ridge","lasso"))
kable(res.rmse,digits = 2)
```

And we plot the final regression coefficients

```{r, echo=FALSE}
beta.fw <- coef(fit.fw)
beta.ridge <- coef(fit.ridge,s=fit.ridge.cv$lambda.1se)
beta.lasso <- coef(fit.lasso,s=fit.lasso.cv$lambda.1se)
res.coef <- data.frame(fw=0,ridge=as.numeric(beta.ridge),lasso=as.numeric(beta.lasso))
rownames(res.coef) <- rownames(beta.ridge)
res.coef[names(beta.fw),"fw"] <- beta.fw
res.coef$coef <- rownames(res.coef)
res.coef.l <- pivot_longer(res.coef,cols=c("fw","ridge","lasso"),names_to="method")

res.coef.l%>%
  dplyr::mutate(coef=factor(coef,levels = unique(coef)))%>%
  ggplot(.,aes(x=coef,y=value,fill=method))+
  geom_bar(stat="identity",position = position_dodge(width = .7),width=0.8)+
  theme(legend.position = "top",axis.text.x = element_text(angle = 90,vjust = 0.5, hjust=1))+
  xlab("")+ylab("beta")
```



```{r, eval=FALSE,echo=FALSE}
data("diabetes")
x <- diabetes$x #mean=0, L2 norm=1
xs <- scale(diabetes$x) #mean=0, var=1
x2 <- diabetes$x2
x2s <- scale(x2)
y <- diabetes$y-mean(diabetes$y)
ys <- scale(y)

# Lasso
fit <- glmnet(xs,ys,alpha=1,standardize = FALSE,intercept=FALSE)
plot(fit,xvar="norm",label=TRUE) # Fig 6.2 in SLS
fit.cv <- cv.glmnet(xs,ys,alpha=1,standardize = FALSE,intercept=FALSE)
plot(fit.cv) # Fig 6.5

```

```{r, eval=FALSE,echo=FALSE}
data("diabetes")
dat <- read.csv("data/diabetes_raw.csv")
colnames(dat) <- c("age","sex","bmi","map","tc","ldl","hdl","tch","ltg","glu","y")
# x <- apply(dat[,-11],2,FUN=function(x){
#   x <- x-mean(x)
#   x/sqrt(sum(x^2))
# })
x <- scale(dat[,-11])/(sqrt(nrow(dat)-1))
head(x[,"age"])
head(diabetes$x2[,"age"])
head(diabetes$x2[,"age^2"])
age2 <- dat$age^2
age2c <- age2-mean(age2)
age2sc <- age2c/sqrt(sum(age2c^2))
xage2 <- (x[,"age"])^2
xage2c <- xage2-mean(xage2)
xage2sc <- xage2c/sqrt(sum(xage2c^2))
head(xage2sc)

x2 <- scale(x^2)/sqrt((nrow(x)-1))
colnames(x2) <- paste0(colnames(x2),"^2")
xint <- scale(model.matrix(~0+.^2,data=data.frame(x)))/sqrt((nrow(x)-1))
head(xint[,"age:sex"])
head(diabetes$x2[,"age:sex"])

xfull <- cbind(x,x2,xint)[,colnames(diabetes$x2)]
```

```{r, eval=FALSE,echo=FALSE}
library(lars)
data("diabetes")
x2s <- scale(diabetes$x2)
colnames(x2s) <- gsub(":",".",colnames(x2s))
y <- diabetes$y-mean(diabetes$y)
dat <- as.data.frame(cbind(y,x2s))

# Forward regression
fit0 <- lm(y~1,data=dat)
fit.fw <- stepAIC(fit0,direction="forward",
                  scope=list(lower=fit0,upper=paste("~", paste(colnames(x2s), collapse=" + "))
                             ),
                  trace = FALSE
                  )
kable(as.data.frame(fit.fw$anova),digits=2)

#summary(fit.fw)
beta.fw <- coef(fit.fw)
kable(broom::tidy(fit.fw),digits=2)

# Ridge
set.seed(1515)
fit.ridge <- glmnet(x2s,y,alpha=0,intercept = FALSE,standardize = FALSE)
fit.ridge.cv <- cv.glmnet(x2s,y,alpha=0,intercept = FALSE,standardize = FALSE)
plot(fit.ridge.cv)
fit.ridge.cv$lambda.1se
beta.ridge <- coef(fit.ridge,s=fit.ridge.cv$lambda.1se)

# Lasso
set.seed(1515)
fit.lasso <- glmnet(x2s,y,alpha=1,intercept = FALSE,standardize = FALSE)
fit.lasso.cv <- cv.glmnet(x2s,y,alpha=1,intercept = FALSE,standardize = FALSE)
plot(fit.lasso.cv)
fit.lasso.cv$lambda.1se
beta.lasso <- coef(fit.lasso,s=fit.lasso.cv$lambda.1se)


res.coef.l%>%
  dplyr::filter(coef!="(Intercept)")%>%
  dplyr::mutate(coef=factor(coef,levels = unique(coef)))%>%
  ggplot(.,aes(x=coef,y=value,fill=method))+
  geom_bar(stat="identity",position = position_dodge(width = .7),width=0.8)+
  theme(legend.position = "top",axis.text.x = element_text(angle = 90,vjust = 0.5, hjust=1))+
  xlab("")+ylab("beta")
```

We finally show how we can use the caret package to perform Ridge and Lasso regression. 
With the caret package we can also easily compare the 3 methods.

```{r}
tc <- trainControl(method = "cv", number = 10)

## Ridge
lambda.grid <- fit.ridge.cv$lambda
fit.ridge.caret<-train(x=xtrain,
                       y=ytrain, 
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 0,lambda=lambda.grid),
                       trControl = tc
) 

# CV curve
plot(fit.ridge.caret)
# Best lambda
fit.ridge.caret$bestTune$lambda
# Model coefficients
coef(fit.ridge.caret$finalModel,fit.ridge.cv$lambda.1se)%>%head
# Make predictions
fit.ridge.caret %>% predict(xtest,s=fit.ridge.cv$lambda.1se)%>%head

## Lasso
lambda.grid <- fit.lasso.cv$lambda
fit.lasso.caret<-train(x=xtrain,
                       y=ytrain, 
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 1,lambda=lambda.grid),
                       trControl = tc
) 

# CV curve
plot(fit.lasso.caret)
# Best lambda
fit.lasso.caret$bestTune$lambda
# Model coefficients
coef(fit.lasso.caret$finalModel,fit.lasso.caret$bestTune$lambda)%>%head
# Make predictions
fit.lasso.caret %>% predict(xtest,s=fit.ridge.cv$lambda.1se)%>%head

## Compare Ridge and Lasso
models <- list(ridge= fit.ridge.caret,ridge = fit.lasso.caret)
resamples(models) %>% summary( metric = "RMSE")
```

